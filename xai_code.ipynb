{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e298a999",
   "metadata": {},
   "source": [
    "## step1\n",
    "한 이미지에서 csv,\n",
    "4개의 object추출하니까 4개의 좌표값 가져오기, \n",
    "그 좌표값에 해당하는 feature가져와서 \n",
    "\n",
    "test csv 다시 확장해서 -> 열 29개 추가하기.. 좋은데? 셔플을 하고 train_loader는 셔플하지말기\n",
    "\n",
    "binary 모델 가져온거에다가 학습하기..\n",
    "\n",
    "## step2\n",
    "binary shap 해보기 \n",
    "\n",
    "\n",
    "## step3\n",
    "full모델에서 attention 들고올 수 있는지 확인 \n",
    "nlp모델에서 attention 값을 가져온다..  \n",
    "\n",
    "## step4\n",
    "ui에서 유의미한걸 불러와야하는데 \n",
    "정답률이 높은거랑 낮은걸 불러와야함.. \n",
    "그거랑 실제로 생성한거랑 정답 비교할 수 있어야함.. csv에다가 report랑 성능도 넣는거 있어야할듯 \n",
    "\n",
    "## step5\n",
    "\n",
    "UI 구성 어떻게 할지?\n",
    "\n",
    "환자 ID정보 , 라벨 정보, csv에 있는 정보랑\n",
    "ob모델이랑 nlp모델 시각화해주고 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92094c2",
   "metadata": {},
   "source": [
    "# object detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9256700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c7d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()  # 현재 스크립트의 디렉토리\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, \"../\"))  # src 상위 디렉토리\n",
    "sys.path.append(parent_dir)\n",
    "from dataset.constants import ANATOMICAL_REGIONS# src.dataset.constants 였는데 바꿈\n",
    "from object_detector.custom_image_dataset_object_detector import CustomImageDataset\n",
    "from object_detector.object_detector import ObjectDetector\n",
    "from path_datasets_and_weights import path_full_dataset, path_runs_object_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c464bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e564b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s]: %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# define configurations for training run\n",
    "RUN = 14\n",
    "# comment can be useful to add additional information to run_config.txt file\n",
    "RUN_COMMENT = \"first_run\"\"\"\"Enter comment here.\"\"\"\n",
    "SEED = 41\n",
    "IMAGE_INPUT_SIZE = 512\n",
    "PERCENTAGE_OF_TRAIN_SET_TO_USE = 1.0\n",
    "PERCENTAGE_OF_VAL_SET_TO_USE = 0.2\n",
    "BATCH_SIZE = 16\n",
    "EFFECTIVE_BATCH_SIZE = 64\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "EVALUATE_EVERY_K_STEPS = 1000  # how often to evaluate the model on the validation set and log metrics to tensorboard (additionally, model will always be evaluated at end of epoch)\n",
    "PATIENCE_LR_SCHEDULER = 5  # number of evaluations to wait for val loss to reduce before lr is reduced\n",
    "THRESHOLD_LR_SCHEDULER = 1e-3\n",
    "FACTOR_LR_SCHEDULER = 0.5\n",
    "COOLDOWN_LR_SCHEDULER = 5\n",
    "\n",
    "# set the seed value for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "def get_title(region_set, region_indices, region_colors, class_detected_img):\n",
    "    # region_set always contains 6 region names (except for region_set_5)\n",
    "\n",
    "    # get a list of 6 boolean values that specify if that region was detected\n",
    "    class_detected = [class_detected_img[region_index] for region_index in region_indices]\n",
    "\n",
    "    # add color_code to region name (e.g. \"(r)\" for red)\n",
    "    # also add nd to the brackets if region was not detected (e.g. \"(r, nd)\" if red region was not detected)\n",
    "    region_set = [region + f\" ({color})\" if cls_detect else region + f\" ({color}, nd)\" for region, color, cls_detect in zip(region_set, region_colors, class_detected)]\n",
    "\n",
    "    # add a line break to the title, as to not make it too long\n",
    "    return \", \".join(region_set[:3]) + \"\\n\" + \", \".join(region_set[3:])\n",
    "\n",
    "\n",
    "def plot_box(box, ax, clr, linestyle, class_detected=True):\n",
    "    x0, y0, x1, y1 = box\n",
    "    h = y1 - y0\n",
    "    w = x1 - x0\n",
    "    ax.add_artist(\n",
    "        plt.Rectangle(\n",
    "            xy=(x0, y0),\n",
    "            height=h,\n",
    "            width=w,\n",
    "            fill=False,\n",
    "            color=clr,\n",
    "            linewidth=1,\n",
    "            linestyle=linestyle\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # add an annotation to the gt box, that the pred box does not exist (i.e. the corresponding class was not detected)\n",
    "    if not class_detected:\n",
    "        ax.annotate(\"not detected\", (x0, y0), color=clr, weight=\"bold\", fontsize=10)\n",
    "\n",
    "\n",
    "def plot_gt_and_pred_bboxes_to_tensorboard(writer, overall_steps_taken, images, detections, targets, class_detected, num_images_to_plot=2):\n",
    "    # pred_boxes is of shape [batch_size x 29 x 4] and contains the predicted region boxes with the highest score (i.e. top-1)\n",
    "    # they are sorted in the 2nd dimension, meaning the 1st of the 29 boxes corresponds to the 1st region/class,\n",
    "    # the 2nd to the 2nd class and so on\n",
    "    pred_boxes_batch = detections[\"top_region_boxes\"]\n",
    "\n",
    "    # targets is a list of dicts, with each dict containing the key \"boxes\" that contain the gt boxes of a single image\n",
    "    # gt_boxes is of shape [batch_size x 29 x 4]\n",
    "    gt_boxes_batch = torch.stack([t[\"boxes\"] for t in targets], dim=0)\n",
    "\n",
    "    # plot 6 regions at a time, as to not overload the image with boxes (except for region_set_5, which has 5 regions)\n",
    "    # the region_sets were chosen as to minimize overlap between the contained regions (i.e. better visibility)\n",
    "    region_set_1 = [\"right lung\", \"right costophrenic angle\", \"left lung\", \"left costophrenic angle\", \"cardiac silhouette\", \"spine\"]\n",
    "    region_set_2 = [\"right upper lung zone\", \"right mid lung zone\", \"right lower lung zone\", \"left upper lung zone\", \"left mid lung zone\", \"left lower lung zone\"]\n",
    "    region_set_3 = [\"right hilar structures\", \"right apical zone\", \"left hilar structures\", \"left apical zone\", \"right hemidiaphragm\", \"left hemidiaphragm\"]\n",
    "    region_set_4 = [\"trachea\", \"right clavicle\", \"left clavicle\", \"aortic arch\", \"abdomen\", \"right atrium\"]\n",
    "    region_set_5 = [\"mediastinum\", \"svc\", \"cavoatrial junction\", \"carina\", \"upper mediastinum\"]\n",
    "\n",
    "    regions_sets = [region_set_1, region_set_2, region_set_3, region_set_4, region_set_5]\n",
    "\n",
    "    for num_img in range(num_images_to_plot):\n",
    "        image = images[num_img].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "        gt_boxes_img = gt_boxes_batch[num_img]\n",
    "        pred_boxes_img = pred_boxes_batch[num_img]\n",
    "        class_detected_img = class_detected[num_img].tolist()\n",
    "\n",
    "        for num_region_set, region_set in enumerate(regions_sets):\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = plt.gca()\n",
    "\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "\n",
    "            region_indices = [ANATOMICAL_REGIONS[region] for region in region_set]\n",
    "            region_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n",
    "\n",
    "            if num_region_set == 4:\n",
    "                region_colors.pop()\n",
    "\n",
    "            for region_index, color in zip(region_indices, region_colors):\n",
    "                box_gt = gt_boxes_img[region_index].tolist()\n",
    "                box_pred = pred_boxes_img[region_index].tolist()\n",
    "                box_class_detected = class_detected_img[region_index]\n",
    "\n",
    "                plot_box(box_gt, ax, clr=color, linestyle=\"solid\", class_detected=box_class_detected)\n",
    "\n",
    "                # only plot predicted box if class was actually detected\n",
    "                if box_class_detected:\n",
    "                    plot_box(box_pred, ax, clr=color, linestyle=\"dashed\")\n",
    "\n",
    "            title = get_title(region_set, region_indices, region_colors, class_detected_img)\n",
    "            ax.set_title(title)\n",
    "\n",
    "            writer.add_figure(f\"img_{num_img}_region_set_{num_region_set}\", fig, overall_steps_taken)\n",
    "\n",
    "\n",
    "def compute_box_area(box):\n",
    "    \"\"\"\n",
    "    Calculate the area of a box given the 4 corner values.\n",
    "\n",
    "    Args:\n",
    "        box (Tensor[batch_size x 29 x 4])\n",
    "\n",
    "    Returns:\n",
    "        area (Tensor[batch_size x 29])\n",
    "    \"\"\"\n",
    "    x0 = box[..., 0]\n",
    "    y0 = box[..., 1]\n",
    "    x1 = box[..., 2]\n",
    "    y1 = box[..., 3]\n",
    "\n",
    "    return (x1 - x0) * (y1 - y0)\n",
    "\n",
    "\n",
    "def compute_intersection_and_union_area_per_class(detections, targets, class_detected):\n",
    "    # pred_boxes is of shape [batch_size x 29 x 4] and contains the predicted region boxes with the highest score (i.e. top-1)\n",
    "    # they are sorted in the 2nd dimension, meaning the 1st of the 29 boxes corresponds to the 1st region/class,\n",
    "    # the 2nd to the 2nd class and so on\n",
    "    pred_boxes = detections[\"top_region_boxes\"]\n",
    "\n",
    "    # targets is a list of dicts, with each dict containing the key \"boxes\" that contain the gt boxes of a single image\n",
    "    # gt_boxes is of shape [batch_size x 29 x 4]\n",
    "    gt_boxes = torch.stack([t[\"boxes\"] for t in targets], dim=0)\n",
    "\n",
    "    # below tensors are of shape [batch_size x 29]\n",
    "    x0_max = torch.maximum(pred_boxes[..., 0], gt_boxes[..., 0])\n",
    "    y0_max = torch.maximum(pred_boxes[..., 1], gt_boxes[..., 1])\n",
    "    x1_min = torch.minimum(pred_boxes[..., 2], gt_boxes[..., 2])\n",
    "    y1_min = torch.minimum(pred_boxes[..., 3], gt_boxes[..., 3])\n",
    "\n",
    "    # intersection_boxes is of shape [batch_size x 29 x 4]\n",
    "    intersection_boxes = torch.stack([x0_max, y0_max, x1_min, y1_min], dim=-1)\n",
    "\n",
    "    # below tensors are of shape [batch_size x 29]\n",
    "    intersection_area = compute_box_area(intersection_boxes)\n",
    "    pred_area = compute_box_area(pred_boxes)\n",
    "    gt_area = compute_box_area(gt_boxes)\n",
    "\n",
    "    # if x0_max >= x1_min or y0_max >= y1_min, then there is no intersection\n",
    "    valid_intersection = torch.logical_and(x0_max < x1_min, y0_max < y1_min)\n",
    "\n",
    "    # also there is no intersection if the class was not detected by object detector\n",
    "    valid_intersection = torch.logical_and(valid_intersection, class_detected)\n",
    "\n",
    "    # set all non-valid intersection areas to 0\n",
    "    intersection_area = torch.where(valid_intersection, intersection_area, torch.tensor(0, dtype=intersection_area.dtype, device=intersection_area.device))\n",
    "\n",
    "    union_area = (pred_area + gt_area) - intersection_area\n",
    "\n",
    "    # sum up the values along the batch dimension (the values will divided by each other later to get the averages)\n",
    "    intersection_area = torch.sum(intersection_area, dim=0)\n",
    "    union_area = torch.sum(union_area, dim=0)\n",
    "\n",
    "    return intersection_area, union_area\n",
    "\n",
    "\n",
    "def get_val_loss_and_other_metrics(model, val_dl, writer, overall_steps_taken):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (nn.Module): The input model to be evaluated.\n",
    "        val_dl (torch.utils.data.Dataloder): The val dataloader to evaluate on.\n",
    "        writer (tensorboardX.SummaryWriter.writer): Writer used to plot gt and predicted bboxes of first couple of image in val set\n",
    "        overall_steps_taken: for tensorboard\n",
    "\n",
    "    Returns:\n",
    "        val_loss (float): val loss for val set\n",
    "        avg_num_detected_classes_per_image (float): since it's possible that certain classes/regions of all 29 regions are not detected in an image,\n",
    "        this metric counts how many classes are detected on average for an image. Ideally, this number should be 29.0\n",
    "        avg_detections_per_class (list[float]): this metric counts how many times a class was detected in an image on average. E.g. if the value is 1.0,\n",
    "        then the class was detected in all images of the val set\n",
    "        avg_iou_per_class (list[float]): average IoU per class computed over all images in val set\n",
    "    \"\"\"\n",
    "    # PyTorch implementation only return losses in train mode, and only detections in eval mode\n",
    "    # see https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn/65347721#65347721\n",
    "    # my model is modified to return losses, detections and class_detected in eval mode\n",
    "    # see forward method of object detector class for more information\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "\n",
    "    num_images = 0\n",
    "\n",
    "    # tensor for accumulating the number of times a class is detected over all images (will be divided by num_images at the end of get average)\n",
    "    sum_class_detected = torch.zeros(29, device=device)\n",
    "\n",
    "    # tensor for accumulating the intersection area of each class (will be divided by union area of each class at the end of get the IoU for each class)\n",
    "    sum_intersection_area_per_class = torch.zeros(29, device=device)\n",
    "\n",
    "    # tensor for accumulating the union area of each class (will divide the intersection area of each class at the end of get the IoU for each class)\n",
    "    sum_union_area_per_class = torch.zeros(29, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, batch in tqdm(enumerate(val_dl)):\n",
    "            # \"targets\" maps to a list of dicts, where each dict has the keys \"boxes\" and \"labels\" and corresponds to a single image\n",
    "            # \"boxes\" maps to a tensor of shape [29 x 4] and \"labels\" maps to a tensor of shape [29]\n",
    "            # note that the \"labels\" tensor is always sorted, i.e. it is of the form [1, 2, 3, ..., 29] (starting at 1, since 0 is background)\n",
    "            images, targets = batch.values()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            num_images += batch_size\n",
    "\n",
    "            images = images.to(device, non_blocking=True)  # shape (batch_size x 1 x 512 x 512)\n",
    "            targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # detections is a dict with keys \"top_region_boxes\" and \"top_scores\"\n",
    "            # \"top_region_boxes\" maps to a tensor of shape [batch_size x 29 x 4]\n",
    "            # \"top_scores\" maps to a tensor of shape [batch_size x 29]\n",
    "\n",
    "            # class_detected is a tensor of shape [batch_size x 29]\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                loss_dict, detections, class_detected = model(images, targets)\n",
    "\n",
    "            # sum up all 4 losses\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += loss.item() * batch_size\n",
    "\n",
    "            # sum up detections for each class\n",
    "            sum_class_detected += torch.sum(class_detected, dim=0)\n",
    "\n",
    "            # compute intersection and union area for each class and add them to the sum\n",
    "            intersection_area_per_class, union_area_per_class = compute_intersection_and_union_area_per_class(detections, targets, class_detected)\n",
    "            sum_intersection_area_per_class += intersection_area_per_class\n",
    "            sum_union_area_per_class += union_area_per_class\n",
    "\n",
    "            if batch_num == 0:\n",
    "                plot_gt_and_pred_bboxes_to_tensorboard(writer, overall_steps_taken, images, detections, targets, class_detected, num_images_to_plot=2)\n",
    "\n",
    "    val_loss /= len(val_dl)\n",
    "    avg_num_detected_classes_per_image = torch.sum(sum_class_detected / num_images).item()\n",
    "    avg_detections_per_class = (sum_class_detected / num_images).tolist()\n",
    "    avg_iou_per_class = (sum_intersection_area_per_class / sum_union_area_per_class).tolist()\n",
    "\n",
    "    return val_loss, avg_num_detected_classes_per_image, avg_detections_per_class, avg_iou_per_class\n",
    "\n",
    "\n",
    "def log_stats_to_console(\n",
    "    train_loss,\n",
    "    val_loss,\n",
    "    epoch,\n",
    "):\n",
    "    log.info(f\"Epoch: {epoch}:\")\n",
    "    log.info(f\"\\tTrain loss: {train_loss:.3f}\")\n",
    "    log.info(f\"\\tVal loss: {val_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    lr_scheduler,\n",
    "    epochs,\n",
    "    weights_folder_path,\n",
    "    writer\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model on train set and evaluate on validation set.\n",
    "    Saves best model w.r.t. val loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        The input model to be trained.\n",
    "    train_dl: torch.utils.data.Dataloder\n",
    "        The train dataloader to train on.\n",
    "    val_dl: torch.utils.data.Dataloder\n",
    "        The val dataloader to validate on.\n",
    "    optimizer: Optimizer\n",
    "        The model's optimizer.\n",
    "    lr_scheduler: torch.optim.lr_scheduler\n",
    "        The learning rate scheduler to use.\n",
    "    epochs: int\n",
    "        Number of epochs to train for.\n",
    "    patience: int\n",
    "        Number of epochs to wait for val loss to decrease.\n",
    "        If patience is exceeded, then training is stopped early.\n",
    "    weights_folder_path: str\n",
    "        Path to folder where best weights will be saved.\n",
    "    writer: torch.utils.tensorboard.SummaryWriter\n",
    "        Writer for logging values to tensorboard.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None, but saves model with the overall lowest val loss at the end of every epoch.\n",
    "    \"\"\"\n",
    "    lowest_val_loss = np.inf\n",
    "\n",
    "    # the best_model_state is the one where the val loss is the lowest overall\n",
    "    best_model_state = None\n",
    "\n",
    "    overall_steps_taken = 0  # for logging to tensorboard\n",
    "\n",
    "    # for gradient accumulation\n",
    "    ACCUMULATION_STEPS = EFFECTIVE_BATCH_SIZE // BATCH_SIZE\n",
    "    \n",
    "    collected_features = []\n",
    "#     for epoch in range(epochs):\n",
    "#         log.info(f\"Training epoch {epoch}!\")\n",
    "\n",
    "#         train_loss = 0.0\n",
    "#         steps_taken = 0\n",
    "#         missing_count = 0 \n",
    "    progress_bar = tqdm(enumerate(train_dl), desc=\"Processing Batches\", total=len(train_dl), unit=\"batch\")\n",
    "    for num_batch, batch in progress_bar:\n",
    "        # batch is a dict with keys \"images\" and \"targets\"\n",
    "        images, targets = batch.values()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        images = images.to(device, non_blocking=True)  # shape (batch_size x 1 x 512 x 512)\n",
    "        targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            losses, top_region_features, class_detected = model(images, targets)\n",
    "\n",
    "                # sum up all 4 losses\n",
    "                #loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        collected_features.append(top_region_features.detach().cpu().numpy())\n",
    "    #log.info(\"Finished training!\")\n",
    "    #log.info(f\"Lowest overall val loss: {lowest_val_loss:.3f} at epoch {best_epoch}\")\n",
    "    return collected_features\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, Tensor]]):\n",
    "    # each dict in batch (which is a list) is for a single image and has the keys \"image\", \"boxes\", \"labels\"\n",
    "\n",
    "    # discard images from batch where __getitem__ from custom_image_dataset failed (i.e. returned None)\n",
    "    # otherwise, whole training loop will stop (even if only 1 image fails to open)\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    batch = list(filter(lambda x: len(x[\"boxes\"]) > 0 and len(x[\"labels\"]) > 0, batch))# youna add\n",
    "\n",
    "    image_shape = batch[0][\"image\"].size()\n",
    "    # allocate an empty images_batch tensor that will store all images of the batch\n",
    "    images_batch = torch.empty(size=(len(batch), *image_shape))\n",
    "\n",
    "    for i, sample in enumerate(batch):\n",
    "        # remove image tensors from batch and store them in dedicated images_batch tensor\n",
    "        images_batch[i] = sample.pop(\"image\")\n",
    "\n",
    "    # since batch (which is a list) now only contains dicts with keys \"boxes\" and \"labels\", rename it as targets\n",
    "    targets = batch\n",
    "\n",
    "    # create a new batch variable to store images_batch and targets\n",
    "    batch_new = {}\n",
    "    batch_new[\"images\"] = images_batch\n",
    "    batch_new[\"targets\"] = targets\n",
    "\n",
    "    return batch_new\n",
    "\n",
    "\n",
    "def get_data_loaders(train_dataset, val_dataset):\n",
    "    def seed_worker(worker_id):\n",
    "        \"\"\"To preserve reproducibility for the randomly shuffled train loader.\"\"\"\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, \n",
    "                              worker_init_fn=seed_worker, generator=g, pin_memory=True)#origin:num_workers=NUM_WORKERS\n",
    "    val_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def get_transforms(dataset: str):\n",
    "    # see compute_mean_std_dataset.py in src/dataset\n",
    "    mean = 0.471\n",
    "    std = 0.302\n",
    "\n",
    "    # use albumentations for Compose and transforms\n",
    "    # augmentations are applied with prob=0.5\n",
    "    # since Affine translates and rotates the image, we also have to do the same with the bounding boxes, hence the bbox_params arugment\n",
    "    train_transforms = A.Compose(\n",
    "        [\n",
    "            # we want the long edge of the image to be resized to IMAGE_INPUT_SIZE, and the short edge of the image to be padded to IMAGE_INPUT_SIZE on both sides,\n",
    "            # such that the aspect ratio of the images are kept, while getting images of uniform size (IMAGE_INPUT_SIZE x IMAGE_INPUT_SIZE)\n",
    "            # LongestMaxSize: resizes the longer edge to IMAGE_INPUT_SIZE while maintaining the aspect ratio\n",
    "            # INTER_AREA works best for shrinking images\n",
    "            A.LongestMaxSize(max_size=IMAGE_INPUT_SIZE, interpolation=cv2.INTER_AREA),\n",
    "            A.ColorJitter(hue=0.0),\n",
    "            A.GaussNoise(),\n",
    "            # randomly (by default prob=0.5) translate and rotate image\n",
    "            # mode and cval specify that black pixels are used to fill in newly created pixels\n",
    "            # translate between -2% and 2% of the image height/width, rotate between -2 and 2 degrees\n",
    "            A.Affine(mode=cv2.BORDER_CONSTANT, cval=0, translate_percent=(-0.02, 0.02), rotate=(-2, 2)),\n",
    "            # PadIfNeeded: pads both sides of the shorter edge with 0's (black pixels)\n",
    "            A.PadIfNeeded(min_height=IMAGE_INPUT_SIZE, min_width=IMAGE_INPUT_SIZE, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=['class_labels'])\n",
    "    )\n",
    "\n",
    "    # don't apply data augmentations to val and test set\n",
    "    val_test_transforms = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=IMAGE_INPUT_SIZE, interpolation=cv2.INTER_AREA),\n",
    "            A.PadIfNeeded(min_height=IMAGE_INPUT_SIZE, min_width=IMAGE_INPUT_SIZE, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ], bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=['class_labels'])\n",
    "    )\n",
    "\n",
    "    if dataset == \"train\":\n",
    "        return train_transforms\n",
    "    else:\n",
    "        return val_test_transforms\n",
    "\n",
    "\n",
    "def get_datasets_as_dfs(config_file_path):\n",
    "    usecols = [\"mimic_image_file_path\", \"bbox_coordinates\", \"bbox_labels\"]\n",
    "\n",
    "    # since bbox_coordinates and bbox_labels are stored as strings in the csv_file, we have to apply\n",
    "    # the literal_eval func to convert them to python lists\n",
    "    converters = {\"bbox_coordinates\": literal_eval, \"bbox_labels\": literal_eval}\n",
    "\n",
    "    datasets_as_dfs = {dataset: os.path.join(path_full_dataset, dataset) + \".csv\" for dataset in [\"train\", \"valid\"]}\n",
    "    datasets_as_dfs = {dataset: pd.read_csv(csv_file_path, usecols=usecols, converters=converters) for dataset, csv_file_path in datasets_as_dfs.items()}\n",
    "   \n",
    "    \n",
    "    for dataset_name, df in datasets_as_dfs.items():\n",
    "        before_len = len(df)\n",
    "        \n",
    "        # bbox_coordinates와 bbox_labels가 유효한 값인지 확인하여 필터링\n",
    "        df = df.dropna(subset=[\"bbox_coordinates\", \"bbox_labels\"]).copy()\n",
    "        def is_valid_target(row):\n",
    "            return isinstance(row[\"bbox_coordinates\"], list) and len(row[\"bbox_coordinates\"]) > 0 and \\\n",
    "                   isinstance(row[\"bbox_labels\"], list) and len(row[\"bbox_labels\"]) > 0\n",
    "\n",
    "        df = df[df.apply(is_valid_target, axis=1)].copy()  # 복사하여 안전하게 필터링\n",
    "        after_len = len(df)\n",
    "        datasets_as_dfs[dataset_name] = df\n",
    "\n",
    "        log.info(f\"{dataset_name.capitalize()}: Filtered {before_len - after_len} samples without targets.\")\n",
    "\n",
    "    \n",
    "    total_num_samples_train = len(datasets_as_dfs[\"train\"])\n",
    "    total_num_samples_val = len(datasets_as_dfs[\"valid\"])\n",
    "\n",
    "    # compute new number of samples for both train and val\n",
    "    new_num_samples_train = int(PERCENTAGE_OF_TRAIN_SET_TO_USE * total_num_samples_train)\n",
    "    new_num_samples_val = int(PERCENTAGE_OF_VAL_SET_TO_USE * total_num_samples_val)\n",
    "\n",
    "    log.info(f\"Train: {new_num_samples_train} images\")\n",
    "    log.info(f\"Val: {new_num_samples_val} images\")\n",
    "\n",
    "    with open(config_file_path, \"a\") as f:\n",
    "        f.write(f\"\\tTRAIN NUM IMAGES: {new_num_samples_train}\\n\")\n",
    "        f.write(f\"\\tVAL NUM IMAGES: {new_num_samples_val}\\n\")\n",
    "    datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # limit the datasets to those new numbers\n",
    "    #datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"][:new_num_samples_train]\n",
    "    #datasets_as_dfs[\"valid\"] = datasets_as_dfs[\"valid\"][:new_num_samples_val]\n",
    "    #print(datasets_as_dfs[\"train\"].head())\n",
    "    return datasets_as_dfs\n",
    "\n",
    "\n",
    "def create_run_folder():\n",
    "    \"\"\"\n",
    "    Run folder will contain a folder for saving the trained weights, a folder for the tensorboard files\n",
    "    as well as a config file that specifies the overall parameters used for training.\n",
    "    \"\"\"\n",
    "    run_folder_path = os.path.join(path_runs_object_detector, f\"run_{RUN}\")\n",
    "    weights_folder_path = os.path.join(run_folder_path, \"weights\")\n",
    "    tensorboard_folder_path = os.path.join(run_folder_path, \"tensorboard\")\n",
    "\n",
    "    # if os.path.exists(run_folder_path): \n",
    "    #     log.error(f\"Folder to save run {RUN} already exists at {run_folder_path}.\")\n",
    "    #     log.error(\"Delete the folder or change the run number.\")\n",
    "    #     return None\n",
    "\n",
    "    os.makedirs(run_folder_path,exist_ok = True)#원래 makedirs\n",
    "    os.makedirs(weights_folder_path,exist_ok = True)\n",
    "    os.makedirs(tensorboard_folder_path,exist_ok = True)\n",
    "\n",
    "    log.info(f\"Run {RUN} folder created at {run_folder_path}.\")\n",
    "\n",
    "    config_file_path = os.path.join(run_folder_path, \"run_config.txt\")\n",
    "    config_parameters = {\n",
    "        \"COMMENT\": RUN_COMMENT,\n",
    "        \"SEED\": SEED,\n",
    "        \"IMAGE_INPUT_SIZE\": IMAGE_INPUT_SIZE,\n",
    "        \"PERCENTAGE_OF_TRAIN_SET_TO_USE\": PERCENTAGE_OF_TRAIN_SET_TO_USE,\n",
    "        \"PERCENTAGE_OF_VAL_SET_TO_USE\": PERCENTAGE_OF_VAL_SET_TO_USE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EFFECTIVE_BATCH_SIZE\": EFFECTIVE_BATCH_SIZE,\n",
    "        \"NUM_WORKERS\": NUM_WORKERS,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"LR\": LR,\n",
    "        \"EVALUATE_EVERY_K_STEPS\": EVALUATE_EVERY_K_STEPS,\n",
    "        \"PATIENCE_LR_SCHEDULER\": PATIENCE_LR_SCHEDULER,\n",
    "        \"THRESHOLD_LR_SCHEDULER\": THRESHOLD_LR_SCHEDULER,\n",
    "        \"FACTOR_LR_SCHEDULER\": FACTOR_LR_SCHEDULER,\n",
    "        \"COOLDOWN_LR_SCHEDULER\": COOLDOWN_LR_SCHEDULER\n",
    "    }\n",
    "\n",
    "    with open(config_file_path, \"w\") as f:\n",
    "        f.write(f\"RUN {RUN}:\\n\")\n",
    "        for param_name, param_value in config_parameters.items():\n",
    "            f.write(f\"\\t{param_name}: {param_value}\\n\")\n",
    "\n",
    "    return weights_folder_path, tensorboard_folder_path, config_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a89dc8-2374-4814-82de-5be644622da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(config_file_path):\n",
    "    usecols = [\n",
    "        \"mimic_image_file_path\",\n",
    "        \"bbox_coordinates\",\n",
    "        \"bbox_labels\",\n",
    "        \"bbox_phrases\",\n",
    "        \"bbox_phrase_exists\",\n",
    "        \"bbox_is_abnormal\",\n",
    "    ]\n",
    "\n",
    "    # all of the columns below are stored as strings in the csv_file\n",
    "    # however, as they are actually lists, we apply the literal_eval func to convert them to lists\n",
    "    converters = {\n",
    "        \"bbox_coordinates\": literal_eval,\n",
    "        \"bbox_labels\": literal_eval,\n",
    "        \"bbox_phrases\": literal_eval,\n",
    "        \"bbox_phrase_exists\": literal_eval,\n",
    "        \"bbox_is_abnormal\": literal_eval,\n",
    "    }\n",
    "\n",
    "    datasets_as_dfs = {}\n",
    "    datasets_as_dfs[\"train\"] = pd.read_csv(os.path.join(path_full_dataset, \"train.csv\"), usecols=usecols, converters=converters)\n",
    "\n",
    "    # val dataset has additional \"reference_report\" column\n",
    "    usecols.append(\"reference_report\")\n",
    "    datasets_as_dfs[\"valid\"] = pd.read_csv(os.path.join(path_full_dataset, \"valid.csv\"), usecols=usecols, converters=converters)\n",
    "\n",
    "    total_num_samples_train = len(datasets_as_dfs[\"train\"])\n",
    "    total_num_samples_val = len(datasets_as_dfs[\"valid\"])\n",
    "\n",
    "    # compute new number of samples for both train and val\n",
    "    new_num_samples_train = int(PERCENTAGE_OF_TRAIN_SET_TO_USE * total_num_samples_train)\n",
    "    new_num_samples_val = int(PERCENTAGE_OF_VAL_SET_TO_USE * total_num_samples_val)\n",
    "\n",
    "    log.info(f\"Train: {new_num_samples_train} images\")\n",
    "    log.info(f\"Val: {new_num_samples_val} images\")\n",
    "\n",
    "    with open(config_file_path, \"a\") as f:\n",
    "        f.write(f\"\\tTRAIN NUM IMAGES: {new_num_samples_train}\\n\")\n",
    "        f.write(f\"\\tVAL NUM IMAGES: {new_num_samples_val}\\n\")\n",
    "\n",
    "    # limit the datasets to those new numbers\n",
    "    datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"][:new_num_samples_train]\n",
    "    datasets_as_dfs[\"valid\"] = datasets_as_dfs[\"valid\"][:new_num_samples_val]\n",
    "\n",
    "    raw_train_dataset = Dataset.from_pandas(datasets_as_dfs[\"train\"])\n",
    "    raw_val_dataset = Dataset.from_pandas(datasets_as_dfs[\"valid\"])\n",
    "\n",
    "    return raw_train_dataset, raw_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c5e9fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# DataFrame의 string으로 저장된 리스트를 실제 리스트로 변환\n",
    "columns_to_convert = ['bbox_coordinates', 'bbox_labels', 'bbox_phrases', 'bbox_phrase_exists', 'bbox_is_abnormal']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_test[column] = df_test[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a4180a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b60ddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: Run 14 folder created at D:/CMU/baseline_code/rgrg/runs/object_detector\\run_14.\n"
     ]
    }
   ],
   "source": [
    "weights_folder_path, tensorboard_folder_path, config_file_path = create_run_folder()\n",
    "\n",
    "datasets_as_dfs = get_datasets_as_dfs(config_file_path) # 여기서 shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e9681ab-309d-4769-a281-148a785f001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: Train: 128844 images\n",
      "[INFO]: Val: 6442 images\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset, raw_val_dataset = get_datasets(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5013fdc9-5800-4989-870a-12caaf9fcdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s50985099/6ad03ed1-97ee17ee-9cf8b320-f7011003-cd93b42d.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s51967283/943486a3-b3fa9ff7-50f5a769-7a62fcbb-f39b6da4.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54935705/6ad819bb-bae74eb9-7b663e90-b8deabd7-57f8054a.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54980801/a75a1fbe-802065ad-717eb7c1-e2ce3552-646276a6.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s57861150/5aa15ba6-55f5e96e-39cea686-7c3b28b2-b8c97a88.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s58206436/54affd39-8bf24209-232bac8a-df6c277a-398ee8a5.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s58636672/c7526473-7b7214ee-a5d58d12-29d1f67f-9f4edf00.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s59988438/925b9496-a956d7b2-05185e52-bb33313b-c06ee522.jpg']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset['mimic_image_file_path'][:2]+raw_train_dataset['mimic_image_file_path'][3:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a99546-12a1-4f4e-950b-a57be8670345",
   "metadata": {},
   "source": [
    "예외 처리 : 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54577367/cfb03587-782edf6c-1bf392e1-98196cd5-365d69e8.jpg', 0,1,2번째임. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c959b-1a10-4375-98c6-c0c2846a8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1,2 번째 \n",
    "\n",
    "'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54577367/cfb03587-782edf6c-1bf392e1-98196cd5-365d69e8.jpg',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7be8868b-038b-445d-ad11-1f41feb37f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54935705/6ad819bb-bae74eb9-7b663e90-b8deabd7-57f8054a.jpg'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['mimic_image_file_path'].iloc[2]# 하 세번째 꺼임.. 없는거만 예외 처리 할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4657c62-78b0-488d-98cc-326057930799",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_as_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42c434bc-3dfb-4987-87e9-400cb09d6aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    mimic_image_file_path  \\\n",
      "62943   D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
      "42768   D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
      "101021  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
      "13727   D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
      "48705   D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
      "\n",
      "                                         bbox_coordinates       bbox_labels  \n",
      "62943   [[225, 144, 512, 553], [406, 171, 512, 550], [...  [1, 2, 3, 6, 19]  \n",
      "42768   [[231, 109, 512, 615], [504, 163, 512, 615], [...        [1, 2, 19]  \n",
      "101021  [[313, 300, 512, 615], [450, 341, 512, 615], [...     [1, 2, 6, 19]  \n",
      "13727        [[231, 532, 512, 615], [477, 573, 512, 615]]            [1, 2]  \n",
      "48705                              [[450, 532, 512, 615]]               [1]  \n"
     ]
    }
   ],
   "source": [
    "# 조건에 해당하는 파일 경로 리스트\n",
    "target_files = raw_train_dataset['mimic_image_file_path'][:2]+raw_train_dataset['mimic_image_file_path'][3:9]\n",
    "\n",
    "# DataFrame에서 조건에 맞는 행 추출\n",
    "filtered_df = datasets_as_dfs['train'][datasets_as_dfs['train']['mimic_image_file_path'].isin(target_files)].copy()\n",
    "\n",
    "# 순서 보장 (target_files 순서대로)\n",
    "filtered_df['mimic_image_file_path'] = pd.Categorical(filtered_df['mimic_image_file_path'], categories=target_files, ordered=True)\n",
    "filtered_df = filtered_df.sort_values('mimic_image_file_path')\n",
    "\n",
    "# 결과 출력\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f6ae45f-13ec-4f20-ab6f-7ad8e8c77318",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('binary_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "190e0286-73e2-4102-adaa-8816101d4cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s50985099/6ad03ed1-97ee17ee-9cf8b320-f7011003-cd93b42d.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s51967283/943486a3-b3fa9ff7-50f5a769-7a62fcbb-f39b6da4.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54577367/cfb03587-782edf6c-1bf392e1-98196cd5-365d69e8.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54935705/6ad819bb-bae74eb9-7b663e90-b8deabd7-57f8054a.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s54980801/a75a1fbe-802065ad-717eb7c1-e2ce3552-646276a6.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s57861150/5aa15ba6-55f5e96e-39cea686-7c3b28b2-b8c97a88.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s58206436/54affd39-8bf24209-232bac8a-df6c277a-398ee8a5.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s58636672/c7526473-7b7214ee-a5d58d12-29d1f67f-9f4edf00.jpg',\n",
       " 'D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgrg\\\\files/p10/p10000980/s59988438/925b9496-a956d7b2-05185e52-bb33313b-c06ee522.jpg']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset['mimic_image_file_path'][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d392ad4-99ab-4987-93c2-9e7dfd0180a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mimic_image_file_path', 'bbox_coordinates', 'bbox_labels', 'bbox_phrases', 'bbox_phrase_exists', 'bbox_is_abnormal'])\n"
     ]
    }
   ],
   "source": [
    "# Dataset 데이터를 Dictionary로 변환\n",
    "dataset_dict = {\n",
    "    'mimic_image_file_path': raw_train_dataset['mimic_image_file_path'],\n",
    "    'bbox_coordinates': raw_train_dataset['bbox_coordinates'],\n",
    "    'bbox_labels': raw_train_dataset['bbox_labels'],\n",
    "    'bbox_phrases': raw_train_dataset['bbox_phrases'],\n",
    "    'bbox_phrase_exists': raw_train_dataset['bbox_phrase_exists'],\n",
    "    'bbox_is_abnormal': raw_train_dataset['bbox_is_abnormal'],\n",
    "}\n",
    "\n",
    "# 확인\n",
    "print(dataset_dict.keys())  # Dictionary의 키 확인\n",
    "import pickle\n",
    "\n",
    "# Pickle 파일로 저장\n",
    "with open(\"raw_train_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset_dict, f)\n",
    "\n",
    "# Pickle 파일 로드\n",
    "# with open(\"dataset_dict.pkl\", \"rb\") as f:\n",
    "#     loaded_dict = pickle.load(f)\n",
    "\n",
    "# print(loaded_dict['mimic_image_file_path'][:5])  # 일부 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a84633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a22c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e9523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(r\"D:\\CMU\\LSMA\\main_project\\full_dataset\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "66950f8d-f37f-4d3d-8b49-b0bcae3832ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f027b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation complete.\n"
     ]
    }
   ],
   "source": [
    "def validate_datasets(datasets_as_dfs):\n",
    "    for dataset_name, df in datasets_as_dfs.items():\n",
    "        # 1. bbox_coordinates와 bbox_labels가 모두 list인지 확인\n",
    "        invalid_format = df[\n",
    "            ~df[\"bbox_coordinates\"].apply(lambda x: isinstance(x, list)) |\n",
    "            ~df[\"bbox_labels\"].apply(lambda x: isinstance(x, list))\n",
    "        ]\n",
    "        if not invalid_format.empty:\n",
    "            print(f\"[{dataset_name}] Invalid format rows found:\")\n",
    "            print(invalid_format)\n",
    "\n",
    "        # 2. bbox_coordinates와 bbox_labels의 길이가 같은지 확인\n",
    "        invalid_lengths = df[\n",
    "            df.apply(lambda row: len(row[\"bbox_coordinates\"]) != len(row[\"bbox_labels\"]), axis=1)\n",
    "        ]\n",
    "        if not invalid_lengths.empty:\n",
    "            print(f\"[{dataset_name}] Mismatched lengths between bbox_coordinates and bbox_labels:\")\n",
    "            print(invalid_lengths)\n",
    "\n",
    "    print(\"Validation complete.\")\n",
    "validate_datasets(datasets_as_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d5af3-aa2f-4e2e-b91f-7a70d29076a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_as_dfs['train']을 주작하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a315bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: \n",
      "Starting training!\n",
      "\n",
      "Processing Batches: 100%|███████████████████████████████████████| 7015/7015 [6:49:15<00:00,  3.50s/batch]\n"
     ]
    }
   ],
   "source": [
    "train_transforms = get_transforms(\"train\")\n",
    "val_transforms = get_transforms(\"val\")\n",
    "\n",
    "train_dataset = CustomImageDataset(datasets_as_dfs[\"train\"], train_transforms)\n",
    "val_dataset = CustomImageDataset(datasets_as_dfs[\"valid\"], val_transforms)\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(train_dataset, val_dataset)\n",
    "\n",
    "model = ObjectDetector(return_feature_vectors=True)\n",
    "\n",
    "model.to(device, non_blocking=True)\n",
    "model.train()\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=FACTOR_LR_SCHEDULER, patience=PATIENCE_LR_SCHEDULER, threshold=THRESHOLD_LR_SCHEDULER, cooldown=COOLDOWN_LR_SCHEDULER)\n",
    "writer = SummaryWriter(log_dir=tensorboard_folder_path)\n",
    "log.info(\"\\nStarting training!\\n\")\n",
    "\n",
    "collected_features_train = train_model(\n",
    "    model=model,\n",
    "    train_dl=train_loader,\n",
    "    val_dl=val_loader,\n",
    "    optimizer=opt,\n",
    "    scaler=scaler,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS,\n",
    "    weights_folder_path=weights_folder_path,\n",
    "    writer=writer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41844b6a-b2e1-4d80-a120-6b8ca1498751",
   "metadata": {},
   "source": [
    "필터링 된거 뽑는거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0b954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(r\"D:\\CMU\\LSMA\\main_project\\full_dataset\\test-2.csv\")\n",
    "df_test = test_df[test_df['image_id']=='dcaead7c-bba30dd7-76fd5755-24ac78ea-2222e886'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ffa33c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset_df, transforms):\n",
    "        super().__init__()\n",
    "        self.dataset_df = dataset_df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # if something in __get__item fails, then return None\n",
    "        # collate_fn in dataloader filters out None values\n",
    "        try:\n",
    "            # mimic_image_file_path is the 1st column of the dataframes\n",
    "            image_path = self.dataset_df['mimic_image_file_path'].iloc[index]\n",
    "\n",
    "            # cv2.imread by default loads an image with 3 channels\n",
    "            # since we have grayscale images, we only have 1 channel and thus use cv2.IMREAD_UNCHANGED to read in the 1 channel\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # bbox_coordinates (List[List[int]]) is the 2nd column of the dataframes\n",
    "            bbox_coordinates = self.dataset_df['bbox_coordinates'].iloc[index]\n",
    "\n",
    "            # bbox_labels (List[int]) is the 3rd column of the dataframes\n",
    "            class_labels = self.dataset_df['bbox_labels'].iloc[index]\n",
    "            if isinstance(bbox_coordinates, str):\n",
    "                bbox_coordinates = ast.literal_eval(bbox_coordinates)\n",
    "\n",
    "            if isinstance(class_labels, str):\n",
    "                class_labels = ast.literal_eval(class_labels)\n",
    "            # apply transformations to image, bboxes and label\n",
    "            transformed = self.transforms(image=image, bboxes=bbox_coordinates, class_labels=class_labels)\n",
    "\n",
    "            transformed_image = transformed[\"image\"]\n",
    "            transformed_bboxes = transformed[\"bboxes\"]\n",
    "            transformed_bbox_labels = transformed[\"class_labels\"]\n",
    "\n",
    "            sample = {\n",
    "                \"image\": transformed_image,\n",
    "                \"boxes\": torch.tensor(transformed_bboxes, dtype=torch.float),\n",
    "                \"labels\": torch.tensor(transformed_bbox_labels, dtype=torch.int64),\n",
    "            }\n",
    "        except Exception:\n",
    "            print(f\"Error processing index {index}: {e}\")\n",
    "            return None\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7231eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: \n",
      "Starting training!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomImageDataset(df_test, train_transforms)\n",
    "\n",
    "\n",
    "test_loader, test1_loader = get_data_loaders(test_dataset, test_dataset)\n",
    "\n",
    "model = ObjectDetector(return_feature_vectors=True)\n",
    "\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "opt = AdamW(model.parameters(), lr=LR)\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=FACTOR_LR_SCHEDULER, patience=PATIENCE_LR_SCHEDULER, threshold=THRESHOLD_LR_SCHEDULER, cooldown=COOLDOWN_LR_SCHEDULER)\n",
    "writer = SummaryWriter(log_dir=tensorboard_folder_path)\n",
    "log.info(\"\\nStarting training!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1046835",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector_checkpoint = {\n",
    "    key.replace(\"object_detector.\", \"\"): value\n",
    "    for key, value in checkpoint['model'].items()\n",
    "    if \"binary\" not in key and \"language\" not in key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb4746fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_detector_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fad6eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectDetector(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rpn): CustomRegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(2048, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(2048, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): CustomRoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(8, 8), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=131072, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=30, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=120, bias=True)\n",
       "    )\n",
       "    (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "    (dim_reduction): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in object_detector_checkpoint.items():\n",
    "    if key in object_detector_state_dict:\n",
    "        object_detector_state_dict[key].copy_(value)\n",
    "    else:\n",
    "        print(f\"Skipping key not in model: {key}\")\n",
    "\n",
    "# 업데이트된 state_dict 로드\n",
    "model.load_state_dict(object_detector_state_dict, strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67da19b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[144, 315, 512, 512], [315, 360, 512, 512], [433, 315, 512, 512], [180, 144, 512, 512]]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['bbox_coordinates'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2926c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2137e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00,  5.84batch/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lowest_val_loss = np.inf\n",
    "\n",
    "# the best_model_state is the one where the val loss is the lowest overall\n",
    "best_model_state = None\n",
    "\n",
    "overall_steps_taken = 0  # for logging to tensorboard\n",
    "\n",
    "# for gradient accumulation\n",
    "ACCUMULATION_STEPS = EFFECTIVE_BATCH_SIZE // BATCH_SIZE\n",
    "\n",
    "object_test_f = []\n",
    "#     for epoch in range(epochs):\n",
    "#         log.info(f\"Training epoch {epoch}!\")\n",
    "\n",
    "#         train_loss = 0.0\n",
    "#         steps_taken = 0\n",
    "#         missing_count = 0 \n",
    "\n",
    "progress_bar = tqdm(enumerate(test_loader), desc=\"Processing Batches\", total=len(test_loader), unit=\"batch\")\n",
    "for num_batch, batch in progress_bar:\n",
    "    # batch is a dict with keys \"images\" and \"targets\"\n",
    "    images, targets = batch.values()\n",
    "\n",
    "    batch_size = images.size(0)\n",
    "    images = images.to(device, non_blocking=True)  # shape (batch_size x 1 x 512 x 512)\n",
    "    targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        losses,d, top_region_features, class_detected = model(images, targets)\n",
    "\n",
    "            # sum up all 4 losses\n",
    "            #loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    object_test_f.append(top_region_features.detach().cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6f0e8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 29, 1024])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_region_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14443cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "054d52a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top region features saved to object_feature_test.pkl\n"
     ]
    }
   ],
   "source": [
    "top_region_features_list = top_region_features.tolist()\n",
    "# 저장할 파일 경로\n",
    "save_path = \"object_feature_test.pkl\"\n",
    "\n",
    "# Pickle로 저장\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(top_region_features_list, f)\n",
    "\n",
    "print(f\"Top region features saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "568f7006-acee-4e5b-b4c3-af20b1a4ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|                                                       | 0/1 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mD:\\CMU\\baseline_code\\xai_mimic_report\\src\\object_detector\\custom_image_dataset_object_detector.py:24\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# cv2.imread by default loads an image with 3 channels\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# since we have grayscale images, we only have 1 channel and thus use cv2.IMREAD_UNCHANGED to read in the 1 channel\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_UNCHANGED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# bbox_coordinates (List[List[int]]) is the 2nd column of the dataframes\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert object to 'str' for 'filename'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m collected_features3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_folder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_folder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 60\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dl, val_dl, optimizer, scaler, lr_scheduler, epochs, weights_folder_path, writer)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#     for epoch in range(epochs):\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#         log.info(f\"Training epoch {epoch}!\")\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#         train_loss = 0.0\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#         steps_taken = 0\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#         missing_count = 0 \u001b[39;00m\n\u001b[0;32m     59\u001b[0m     progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dl), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dl), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num_batch, batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# batch is a dict with keys \"images\" and \"targets\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         images, targets \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m     64\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\CMU\\baseline_code\\xai_mimic_report\\src\\object_detector\\custom_image_dataset_object_detector.py:45\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     39\u001b[0m     sample \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: transformed_image,\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(transformed_bboxes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(transformed_bbox_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m     43\u001b[0m     }\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "collected_features3 = train_model(\n",
    "    model=model,\n",
    "    train_dl=test_loader,\n",
    "    val_dl=test_loader,\n",
    "    optimizer=opt,\n",
    "    scaler=scaler,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    epochs=EPOCHS,\n",
    "    weights_folder_path=weights_folder_path,\n",
    "    writer=writer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c240993-211a-4e77-b4f9-321d5720e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_feature_t = collected_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "079b0df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collected_features_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a309007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_as_dfs_2(config_file_path):\n",
    "    usecols = [\"mimic_image_file_path\", \"bbox_coordinates\", \"bbox_labels\"]\n",
    "\n",
    "    # since bbox_coordinates and bbox_labels are stored as strings in the csv_file, we have to apply\n",
    "    # the literal_eval func to convert them to python lists\n",
    "    converters = {\"bbox_coordinates\": literal_eval, \"bbox_labels\": literal_eval}\n",
    "\n",
    "    datasets_as_dfs = {dataset: os.path.join(path_full_dataset, dataset) + \".csv\" for dataset in [\"train\", \"valid\"]}\n",
    "    datasets_as_dfs = {dataset: pd.read_csv(csv_file_path, converters=converters) for dataset, csv_file_path in datasets_as_dfs.items()}\n",
    "   \n",
    "    \n",
    "    for dataset_name, df in datasets_as_dfs.items():\n",
    "        before_len = len(df)\n",
    "        \n",
    "        # bbox_coordinates와 bbox_labels가 유효한 값인지 확인하여 필터링\n",
    "        df = df.dropna(subset=[\"bbox_coordinates\", \"bbox_labels\"]).copy()\n",
    "        def is_valid_target(row):\n",
    "            return isinstance(row[\"bbox_coordinates\"], list) and len(row[\"bbox_coordinates\"]) > 0 and \\\n",
    "                   isinstance(row[\"bbox_labels\"], list) and len(row[\"bbox_labels\"]) > 0\n",
    "\n",
    "        df = df[df.apply(is_valid_target, axis=1)].copy()  # 복사하여 안전하게 필터링\n",
    "        after_len = len(df)\n",
    "        datasets_as_dfs[dataset_name] = df\n",
    "\n",
    "        log.info(f\"{dataset_name.capitalize()}: Filtered {before_len - after_len} samples without targets.\")\n",
    "\n",
    "    \n",
    "    total_num_samples_train = len(datasets_as_dfs[\"train\"])\n",
    "    total_num_samples_val = len(datasets_as_dfs[\"valid\"])\n",
    "\n",
    "    # compute new number of samples for both train and val\n",
    "    new_num_samples_train = int(PERCENTAGE_OF_TRAIN_SET_TO_USE * total_num_samples_train)\n",
    "    new_num_samples_val = int(PERCENTAGE_OF_VAL_SET_TO_USE * total_num_samples_val)\n",
    "\n",
    "    log.info(f\"Train: {new_num_samples_train} images\")\n",
    "    log.info(f\"Val: {new_num_samples_val} images\")\n",
    "\n",
    "    with open(config_file_path, \"a\") as f:\n",
    "        f.write(f\"\\tTRAIN NUM IMAGES: {new_num_samples_train}\\n\")\n",
    "        f.write(f\"\\tVAL NUM IMAGES: {new_num_samples_val}\\n\")\n",
    "    datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # limit the datasets to those new numbers\n",
    "    #datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"][:new_num_samples_train]\n",
    "    #datasets_as_dfs[\"valid\"] = datasets_as_dfs[\"valid\"][:new_num_samples_val]\n",
    "    #print(datasets_as_dfs[\"train\"].head())\n",
    "    return datasets_as_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d99533f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimic_image_file_path</th>\n",
       "      <th>bbox_coordinates</th>\n",
       "      <th>bbox_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[341, 40, 512, 426], [177, 177, 512, 426]]</td>\n",
       "      <td>[1, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[81, 532, 512, 615]]</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[95, 504, 512, 615], [313, 559, 512, 615], [4...</td>\n",
       "      <td>[1, 2, 6, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[18, 433, 512, 596], [252, 469, 512, 596], [3...</td>\n",
       "      <td>[1, 2, 6, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[497, 189, 512, 489]]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mimic_image_file_path  \\\n",
       "0  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "1  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "2  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "3  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "4  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "\n",
       "                                    bbox_coordinates    bbox_labels  \n",
       "0        [[341, 40, 512, 426], [177, 177, 512, 426]]        [1, 19]  \n",
       "1                              [[81, 532, 512, 615]]           [19]  \n",
       "2  [[95, 504, 512, 615], [313, 559, 512, 615], [4...  [1, 2, 6, 19]  \n",
       "3  [[18, 433, 512, 596], [252, 469, 512, 596], [3...  [1, 2, 6, 19]  \n",
       "4                             [[497, 189, 512, 489]]            [1]  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_as_dfs[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "078c6246-88e2-4137-bb36-5971d5ec165c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mimic_image_file_path</th>\n",
       "      <th>bbox_coordinates</th>\n",
       "      <th>bbox_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62943</th>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[225, 144, 512, 553], [406, 171, 512, 550], [...</td>\n",
       "      <td>[1, 2, 3, 6, 19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mimic_image_file_path  \\\n",
       "62943  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "\n",
       "                                        bbox_coordinates       bbox_labels  \n",
       "62943  [[225, 144, 512, 553], [406, 171, 512, 550], [...  [1, 2, 3, 6, 19]  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8c307cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112240\n",
      "7015\n"
     ]
    }
   ],
   "source": [
    "# 피처 프린트 해보고, len확인해보기, feature 확인\n",
    "print(len(datasets_as_dfs[\"train\"]))\n",
    "print(len(collected_features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ed13b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_f(batch: List[Dict[str, Tensor]], batch_index: int = 0):\n",
    "    # 필터링 기록을 저장할 리스트\n",
    "    filtered_indices = []\n",
    "\n",
    "    # 필터링 전 원래 배치 크기\n",
    "    original_batch_size = len(batch)\n",
    "    print(type(batch))\n",
    "    # __getitem__ 실패한 데이터 필터링\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if len(batch) < original_batch_size:\n",
    "        filtered_indices.append((batch_index, \"getitem_failed\"))\n",
    "\n",
    "    # boxes와 labels가 없는 데이터 필터링\n",
    "    before_second_filter_size = len(batch)\n",
    "    batch = list(filter(lambda x: len(x[\"boxes\"]) > 0 and len(x[\"labels\"]) > 0, batch))\n",
    "    if len(batch) < before_second_filter_size:\n",
    "        filtered_indices.append((batch_index, \"empty_boxes_or_labels\"))\n",
    "\n",
    "    image_shape = batch[0][\"image\"].size()\n",
    "    images_batch = torch.empty(size=(len(batch), *image_shape))\n",
    "\n",
    "    for i, sample in enumerate(batch):\n",
    "        images_batch[i] = sample.pop(\"image\")\n",
    "\n",
    "    targets = batch\n",
    "\n",
    "    batch_new = {}\n",
    "    batch_new[\"images\"] = images_batch\n",
    "    batch_new[\"targets\"] = targets\n",
    "\n",
    "    # 필터링된 데이터 기록 반환 (디버깅용)\n",
    "    return batch_new, filtered_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9144fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders_2(train_dataset):\n",
    "    def seed_worker(worker_id):\n",
    "        \"\"\"To preserve reproducibility for the randomly shuffled train loader.\"\"\"\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn_f, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, \n",
    "                              worker_init_fn=seed_worker, generator=g, pin_memory=True)#origin:num_workers=NUM_WORKERS\n",
    "    #val_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac7294bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = get_data_loaders_2(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d0d5b2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 는 15\n",
      "83 는 15\n",
      "102 는 15\n",
      "169 는 15\n",
      "196 는 15\n",
      "252 는 15\n",
      "326 는 15\n",
      "331 는 15\n",
      "332 는 15\n",
      "354 는 15\n",
      "452 는 15\n",
      "487 는 15\n",
      "499 는 15\n",
      "518 는 15\n",
      "574 는 15\n",
      "639 는 15\n",
      "742 는 15\n",
      "831 는 15\n",
      "945 는 15\n",
      "1122 는 15\n",
      "1128 는 15\n",
      "1130 는 15\n",
      "1136 는 15\n",
      "1204 는 15\n",
      "1235 는 15\n",
      "1243 는 15\n",
      "1302 는 15\n",
      "1351 는 14\n",
      "1429 는 15\n",
      "1486 는 15\n",
      "1527 는 15\n",
      "1547 는 15\n",
      "1574 는 15\n",
      "1578 는 15\n",
      "1758 는 15\n",
      "1779 는 15\n",
      "1798 는 15\n",
      "1881 는 15\n",
      "1884 는 15\n",
      "1938 는 15\n",
      "1945 는 15\n",
      "2007 는 15\n",
      "2080 는 15\n",
      "2127 는 15\n",
      "2212 는 15\n",
      "2218 는 15\n",
      "2232 는 15\n",
      "2251 는 15\n",
      "2266 는 15\n",
      "2325 는 15\n",
      "2331 는 15\n",
      "2355 는 15\n",
      "2364 는 15\n",
      "2375 는 15\n",
      "2391 는 15\n",
      "2429 는 15\n",
      "2521 는 15\n",
      "2611 는 15\n",
      "2762 는 15\n",
      "2771 는 15\n",
      "2778 는 15\n",
      "2794 는 15\n",
      "2862 는 15\n",
      "3003 는 15\n",
      "3006 는 15\n",
      "3027 는 15\n",
      "3035 는 15\n",
      "3049 는 15\n",
      "3076 는 15\n",
      "3098 는 15\n",
      "3216 는 15\n",
      "3274 는 15\n",
      "3430 는 15\n",
      "3433 는 15\n",
      "3450 는 15\n",
      "3453 는 15\n",
      "3460 는 15\n",
      "3461 는 15\n",
      "3536 는 15\n",
      "3538 는 15\n",
      "3643 는 15\n",
      "3735 는 15\n",
      "3757 는 15\n",
      "3762 는 15\n",
      "3817 는 15\n",
      "3904 는 15\n",
      "3978 는 15\n",
      "4028 는 15\n",
      "4099 는 15\n",
      "4147 는 15\n",
      "4250 는 15\n",
      "4292 는 15\n",
      "4316 는 15\n",
      "4357 는 15\n",
      "4410 는 15\n",
      "4481 는 15\n",
      "4488 는 15\n",
      "4653 는 15\n",
      "4663 는 15\n",
      "4741 는 15\n",
      "4742 는 15\n",
      "4750 는 15\n",
      "4810 는 15\n",
      "4862 는 15\n",
      "5023 는 15\n",
      "5157 는 15\n",
      "5175 는 15\n",
      "5214 는 15\n",
      "5256 는 15\n",
      "5275 는 15\n",
      "5407 는 15\n",
      "5429 는 15\n",
      "5464 는 15\n",
      "5497 는 15\n",
      "5592 는 15\n",
      "5611 는 15\n",
      "5626 는 15\n",
      "5634 는 15\n",
      "5711 는 15\n",
      "5726 는 15\n",
      "5727 는 15\n",
      "5749 는 15\n",
      "5774 는 15\n",
      "5798 는 15\n",
      "5821 는 15\n",
      "5848 는 15\n",
      "5914 는 15\n",
      "5961 는 15\n",
      "5963 는 15\n",
      "6140 는 15\n",
      "6201 는 15\n",
      "6244 는 15\n",
      "6245 는 15\n",
      "6446 는 15\n",
      "6464 는 15\n",
      "6474 는 15\n",
      "6756 는 15\n",
      "6787 는 15\n",
      "6836 는 15\n",
      "6898 는 15\n",
      "6926 는 15\n",
      "6972 는 15\n",
      "6994 는 15\n"
     ]
    }
   ],
   "source": [
    "# 데이터로더 루프에서 추적\n",
    "filtered_log = []\n",
    "\n",
    "for batch_index, batch in enumerate(train_loader2):\n",
    "    batch = batch[0]['targets']\n",
    "    #print(batch)\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    batch = list(filter(lambda x: len(x[\"boxes\"]) > 0 and len(x[\"labels\"]) > 0, batch))# youna add\n",
    "    if len(batch)!=16:\n",
    "        print(batch_index,'는',len(batch))\n",
    "    #batch_new, filtered_indices = collate_fn_f(batch[0], batch_index=batch_index)\n",
    "    #if filtered_indices:\n",
    "    #    filtered_log.extend(filtered_indices)\n",
    "    #break\n",
    "# 필터링된 데이터 출력\n",
    "#print(\"Filtered data log:\", filtered_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23162bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_as_dfs2랑 매칭하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bf0595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]: Train: Filtered 16604 samples without targets.\n",
      "[INFO]: Valid: Filtered 4029 samples without targets.\n",
      "[INFO]: Train: 112240 images\n",
      "[INFO]: Val: 5636 images\n"
     ]
    }
   ],
   "source": [
    "datasets_as_dfs2 = get_datasets_as_dfs_2(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e69d16a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112240"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets_as_dfs2['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a473caa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7015.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "112240/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "24556dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(112240-112079)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ef27fc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7015"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collected_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2107f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seclab\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\numpy\\lib\\npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 저장\n",
    "file_path = \"collected_features_train_zip.npy\"  # 저장할 파일 경로\n",
    "np.save(file_path, collected_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bc7fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining batches: 100%|██████████████████████████████████████| 7015/7015 [00:00<00:00, 342937.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112079\n"
     ]
    }
   ],
   "source": [
    "collected_feature_train_all = []\n",
    "for batch in tqdm(collected_features_train, desc=\"Combining batches\", unit=\"batch\"):\n",
    "    collected_feature_train_all.extend(batch)\n",
    "print(len(collected_feature_train_all))  \n",
    "# Step 3: 최종 결과 저장 (예: NumPy 파일로 저장)\n",
    "np.save(\"collected_feature_train_all.npy\", np.array(collected_feature_train_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40c0eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seclab\\AppData\\Local\\Temp\\ipykernel_37240\\1779270865.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys: dict_keys(['model', 'optimizer', 'scaler', 'current_epoch', 'overall_steps_taken', 'lowest_val_loss'])\n",
      "Optimizer keys: dict_keys(['state', 'param_groups'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Checkpoint 파일 로드\n",
    "checkpoint_path = r\"D:\\CMU\\baseline_code\\full_model_checkpoint_val_loss_19.793_overall_steps_155252.pt\"  # 실제 파일 경로\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# 파일 구조 확인\n",
    "print(\"Checkpoint keys:\", checkpoint.keys())  # 포함된 키 확인\n",
    "\n",
    "# 모델의 state_dict 확인\n",
    "if \"state_dict\" in checkpoint:\n",
    "    print(\"State_dict keys:\", checkpoint[\"state_dict\"].keys())\n",
    "\n",
    "# 기타 정보 확인\n",
    "if \"optimizer\" in checkpoint:\n",
    "    print(\"Optimizer keys:\", checkpoint[\"optimizer\"].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe890816",
   "metadata": {},
   "source": [
    "## binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5c48b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_binary = []\n",
    "\n",
    "for key in checkpoint['model'].keys():\n",
    "    a = 'binary_classifier_region_selection'\n",
    "    if a in key:\n",
    "        cp_binary.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4bd51a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['binary_classifier_region_selection.classifier.0.weight',\n",
       " 'binary_classifier_region_selection.classifier.0.bias',\n",
       " 'binary_classifier_region_selection.classifier.2.weight',\n",
       " 'binary_classifier_region_selection.classifier.2.bias',\n",
       " 'binary_classifier_region_selection.classifier.4.weight',\n",
       " 'binary_classifier_region_selection.classifier.4.bias',\n",
       " 'binary_classifier_region_selection.loss_fn.pos_weight']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "53ecbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state', 'param_groups'])\n",
      "dict_keys(['scale', 'growth_factor', 'backoff_factor', 'growth_interval', '_growth_tracker'])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint['optimizer'].keys())\n",
    "print(checkpoint['scaler'].keys())\n",
    "print(checkpoint['current_epoch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a2ec9a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : torch.Size([512, 1024]), b : torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "print(f\"W : {checkpoint['model']['binary_classifier_region_selection.classifier.0.weight'].shape}, b : {checkpoint['model']['binary_classifier_region_selection.classifier.0.bias'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "255f27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : tensor([[-0.0358, -0.0185,  0.0579,  ...,  0.0232, -0.0233,  0.0224],\n",
      "        [ 0.0400,  0.0119,  0.0498,  ...,  0.1066, -0.0391,  0.0423],\n",
      "        [ 0.0158,  0.0090,  0.0265,  ...,  0.0820, -0.0228,  0.0541],\n",
      "        ...,\n",
      "        [-0.0030, -0.0132,  0.0567,  ...,  0.0418, -0.0516,  0.0076],\n",
      "        [ 0.0053, -0.0460,  0.0769,  ..., -0.0593, -0.0729, -0.0254],\n",
      "        [ 0.0750,  0.0245,  0.1156,  ..., -0.0080, -0.0075,  0.0215]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"W : {checkpoint['model']['binary_classifier_region_selection.classifier.0.weight']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "729e0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifierRegionSelection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=1)\n",
    "        )\n",
    "\n",
    "        # since we have around 2.2x more regions without sentences than regions with sentences (see dataset/dataset_stats.txt generated from compute_stats_dataset.py),\n",
    "        # we set pos_weight=2.2 to put 2.2 more weight on the loss of regions with sentences\n",
    "        pos_weight = torch.tensor([2.2], device=device)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        top_region_features,  # tensor of shape [batch_size x 29 x 1024]\n",
    "        class_detected,  # boolean tensor of shape [batch_size x 29], indicates if the object detector has detected the region/class or not\n",
    "        return_loss,  # boolean value that is True if we need the loss (necessary for training and evaluation)\n",
    "        region_has_sentence=None  # boolean tensor of shape [batch_size x 29], indicates if a region has a sentence (True) or not (False) as the ground truth\n",
    "    ):\n",
    "        # logits of shape [batch_size x 29]\n",
    "        logits = self.classifier(top_region_features).squeeze(dim=-1)\n",
    "\n",
    "        # the loss is needed for training and evaluation\n",
    "        if return_loss:\n",
    "            # only compute loss for logits that correspond to a class that was detected\n",
    "            detected_logits = logits[class_detected]\n",
    "            detected_region_has_sentence = region_has_sentence[class_detected]\n",
    "\n",
    "            loss = self.loss_fn(detected_logits, detected_region_has_sentence.type(torch.float32))\n",
    "\n",
    "        if self.training:\n",
    "            return loss\n",
    "        else:\n",
    "            # compute selected_regions for evaluation and inference\n",
    "            # selected_regions are the predictions by the classifier, has shape [batch_size x 29] and is True for regions that should get a sentence\n",
    "            # selected_regions are needed to filter the top_region_features to those that the classifier selected for sentence generation,\n",
    "            # to evaluate the classifier during eval mode (comparing selected_regions to ground-truth region_has_sentence to compute recall, precision etc.)\n",
    "            # and to generally know which regions were selected for generation (which is needed for mapping each selected_region_feature to its corresponding class)\n",
    "            #\n",
    "            # use a threshold of -1 in logit-space (i.e. 0.269 in probability-space)\n",
    "            # if a logit > -1, then it means that class/region has boolean value True and a sentence should be generated for it\n",
    "            selected_regions = logits > -1\n",
    "\n",
    "            # set to False all regions that were not detected by object detector\n",
    "            # (since no detection -> no sentence generation possible)\n",
    "            selected_regions[~class_detected] = False\n",
    "\n",
    "            # selected_region_features are inputted into the decoder during evaluation and inference to generate the sentences\n",
    "            # selected_region_features is of shape [num_regions_selected_in_batch, 1024]\n",
    "            selected_region_features = top_region_features[selected_regions]\n",
    "\n",
    "            # if in eval mode\n",
    "            if return_loss:\n",
    "                return loss, selected_regions, selected_region_features\n",
    "            else:\n",
    "                # if in inference mode\n",
    "                return selected_regions, selected_region_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15c106c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 초기화\n",
    "model = BinaryClassifierRegionSelection().to(device)\n",
    "\n",
    "\n",
    "model.load_state_dict({\n",
    "    'classifier.0.weight': checkpoint['model']['binary_classifier_region_selection.classifier.0.weight'],\n",
    "    'classifier.0.bias': checkpoint['model']['binary_classifier_region_selection.classifier.0.bias'],\n",
    "    'classifier.2.weight': checkpoint['model']['binary_classifier_region_selection.classifier.2.weight'],\n",
    "    'classifier.2.bias': checkpoint['model']['binary_classifier_region_selection.classifier.2.bias'],\n",
    "    'classifier.4.weight': checkpoint['model']['binary_classifier_region_selection.classifier.4.weight'],\n",
    "    'classifier.4.bias': checkpoint['model']['binary_classifier_region_selection.classifier.4.bias'],\n",
    "    'loss_fn.pos_weight':checkpoint['model']['binary_classifier_region_selection.loss_fn.pos_weight']\n",
    "})\n",
    "\n",
    "\n",
    "#model.loss_fn.pos_weight = checkpoint['model']['binary_classifier_region_selection.loss_fn.pos_weight']\n",
    "# 학습 루프\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 8  # 예시 배치 크기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9fa4a5ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_s = []\n",
    "\n",
    "for i in datasets_as_dfs2['train']['bbox_phrase_exists'].iloc[:16]:\n",
    "    target_s.append(eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "02db88a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2fe8ff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112079"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collected_feature_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "914c595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: (16, 29, 1024)\n",
      "Sample 1: (16, 29, 1024)\n",
      "Sample 2: (16, 29, 1024)\n",
      "Sample 3: (16, 29, 1024)\n",
      "Sample 4: (16, 29, 1024)\n",
      "Sample 5: (16, 29, 1024)\n",
      "Sample 6: (16, 29, 1024)\n",
      "Sample 7: (16, 29, 1024)\n",
      "Sample 8: (16, 29, 1024)\n",
      "Sample 9: (16, 29, 1024)\n",
      "Sample 10: (16, 29, 1024)\n",
      "Sample 11: (16, 29, 1024)\n",
      "Sample 12: (16, 29, 1024)\n",
      "Sample 13: (16, 29, 1024)\n",
      "Sample 14: (16, 29, 1024)\n",
      "Sample 15: (16, 29, 1024)\n"
     ]
    }
   ],
   "source": [
    "# 4번째 샘플 제외하고 17번째까지 선택\n",
    "filtered_features = [feature for i, feature in enumerate(collected_features_train[:17]) if i != 4]\n",
    "\n",
    "# 선택한 샘플 출력\n",
    "for i, feature in enumerate(filtered_features):\n",
    "    print(f\"Sample {i}: {feature.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "22fb35bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 29, 1024])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_features_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "042db4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 29])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_s_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a097d8-04ee-4284-a5fb-67ae5c240fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 변환\n",
    "file_path = 'collected_features_train_zip.npy'\n",
    "collected_features_train = np.load(file_path, allow_pickle=True)\n",
    "# npy 파일 로드\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "307a2329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>study_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>mimic_image_file_path</th>\n",
       "      <th>bbox_coordinates</th>\n",
       "      <th>bbox_labels</th>\n",
       "      <th>bbox_phrases</th>\n",
       "      <th>bbox_phrase_exists</th>\n",
       "      <th>bbox_is_abnormal</th>\n",
       "      <th>reference_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>10019777</td>\n",
       "      <td>56013693</td>\n",
       "      <td>dcaead7c-bba30dd7-76fd5755-24ac78ea-2222e886</td>\n",
       "      <td>D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...</td>\n",
       "      <td>[[144, 315, 512, 512], [315, 360, 512, 512], [...</td>\n",
       "      <td>[1, 2, 6, 19]</td>\n",
       "      <td>[\"Lung volumes are low resulting crowding of t...</td>\n",
       "      <td>[True, False, True, True, True, False, True, F...</td>\n",
       "      <td>[True, False, True, True, True, False, False, ...</td>\n",
       "      <td>Lung volumes are low resulting crowding of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id  study_id                                      image_id  \\\n",
       "89    10019777  56013693  dcaead7c-bba30dd7-76fd5755-24ac78ea-2222e886   \n",
       "\n",
       "                                mimic_image_file_path  \\\n",
       "89  D:/CMU/LSMA/main_project/dataset/mimic_jpg_rgr...   \n",
       "\n",
       "                                     bbox_coordinates    bbox_labels  \\\n",
       "89  [[144, 315, 512, 512], [315, 360, 512, 512], [...  [1, 2, 6, 19]   \n",
       "\n",
       "                                         bbox_phrases  \\\n",
       "89  [\"Lung volumes are low resulting crowding of t...   \n",
       "\n",
       "                                   bbox_phrase_exists  \\\n",
       "89  [True, False, True, True, True, False, True, F...   \n",
       "\n",
       "                                     bbox_is_abnormal  \\\n",
       "89  [True, False, True, True, True, False, False, ...   \n",
       "\n",
       "                                     reference_report  \n",
       "89  Lung volumes are low resulting crowding of the...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "285d9b0f-b206-4132-8a60-356dba6c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_s = raw_train_dataset['bbox_phrase_exists'][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66df1de3-2c38-4a5d-a65a-bc7749720593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collected_features_train_ = torch.tensor(col_feature_t[0], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbe623b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Regions: tensor([[ True, False, False, False, False, False,  True, False, False, False,\n",
      "         False, False,  True, False,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True, False,  True,  True,  True, False,  True, False,\n",
      "         False, False, False, False,  True, False, False, False, False, False,\n",
      "         False, False, False, False,  True, False, False, False, False],\n",
      "        [ True,  True, False, False,  True, False,  True, False,  True,  True,\n",
      "         False, False,  True, False,  True, False, False, False, False, False,\n",
      "          True,  True, False, False,  True, False, False, False, False],\n",
      "        [ True,  True, False, False,  True, False,  True, False,  True,  True,\n",
      "         False, False,  True, False,  True, False, False,  True,  True,  True,\n",
      "          True,  True, False, False,  True, False, False, False, False],\n",
      "        [ True, False, False, False,  True, False,  True, False,  True, False,\n",
      "         False, False,  True, False,  True, False, False,  True,  True,  True,\n",
      "          True,  True, False, False,  True, False, False, False, False],\n",
      "        [False, False, False,  True,  True, False,  True, False,  True, False,\n",
      "         False,  True,  True, False,  True, False, False, False, False, False,\n",
      "          True,  True, False, False,  True, False, False, False, False],\n",
      "        [ True, False, False, False,  True, False, False, False,  True, False,\n",
      "         False, False,  True, False,  True, False, False, False, False, False,\n",
      "         False, False, False, False,  True, False, False, False, False],\n",
      "        [ True, False, False, False, False, False,  True, False,  True, False,\n",
      "         False, False, False, False,  True, False, False, False, False, False,\n",
      "          True,  True, False, False,  True, False, False, False, False]],\n",
      "       device='cuda:0')\n",
      "Selected Region Features: tensor([[-0.4053, -0.9604, -0.5327,  ...,  0.1161,  0.1752, -0.6196],\n",
      "        [-0.0462, -0.3372,  0.0448,  ..., -0.1379, -0.0701,  0.0037],\n",
      "        [ 0.1757, -0.2189,  0.1017,  ..., -0.0536,  0.0437,  0.0844],\n",
      "        ...,\n",
      "        [ 0.3787, -0.1602,  0.1466,  ..., -0.5166, -0.0727,  0.0519],\n",
      "        [-0.0460, -0.6421, -0.1597,  ..., -0.4724,  0.0040, -0.1580],\n",
      "        [ 0.1345,  0.1300,  0.1558,  ...,  0.1499,  0.1844, -0.0030]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_s_ = torch.tensor(target_s, dtype=torch.bool).to(device)\n",
    "\n",
    "# 추론\n",
    "with torch.no_grad():  # 추론 중에는 기울기 계산 비활성화\n",
    "    selected_regions, selected_region_features = model(\n",
    "        top_region_features=collected_features_train_,\n",
    "        class_detected=target_s_,\n",
    "        return_loss=False  # 추론 모드\n",
    "    )\n",
    "\n",
    "print(\"Selected Regions:\", selected_regions)\n",
    "print(\"Selected Region Features:\", selected_region_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9b799",
   "metadata": {},
   "source": [
    "SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f9c2dc8-17b2-4685-a2d4-2daa1edddb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 29, 1024])\n",
      "torch.Size([8, 29])\n"
     ]
    }
   ],
   "source": [
    "print(collected_features_train_.shape)\n",
    "print(target_s_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75f8264d-2a73-4b3e-9027-28487fb42a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False,  True, False,  True, False,  True, False,\n",
       "         False, False,  True, False,  True, False, False, False, False, False,\n",
       "         False,  True, False, False,  True, False, False, False, False],\n",
       "        [ True,  True,  True, False,  True,  True,  True, False,  True, False,\n",
       "         False, False, False, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False,  True, False, False, False, False],\n",
       "        [ True,  True, False, False,  True, False,  True, False,  True,  True,\n",
       "         False, False,  True, False,  True, False, False, False, False, False,\n",
       "          True,  True, False, False,  True, False, False, False, False],\n",
       "        [ True,  True, False, False,  True, False,  True, False,  True,  True,\n",
       "         False, False,  True, False,  True, False, False,  True,  True,  True,\n",
       "          True,  True, False, False,  True, False, False, False, False],\n",
       "        [ True, False, False, False,  True, False,  True, False,  True, False,\n",
       "         False, False,  True, False,  True, False, False,  True,  True,  True,\n",
       "          True,  True, False, False,  True, False, False, False, False],\n",
       "        [ True, False, False,  True,  True, False,  True, False,  True, False,\n",
       "         False,  True,  True, False,  True, False, False, False, False, False,\n",
       "          True,  True, False, False,  True, False, False, False, False],\n",
       "        [ True, False, False, False,  True, False,  True, False,  True, False,\n",
       "         False, False,  True, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False,  True, False, False, False, False],\n",
       "        [ True, False, False, False, False, False,  True, False,  True, False,\n",
       "         False, False, False, False,  True, False, False, False, False, False,\n",
       "          True,  True, False, False,  True, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42afc637-75b3-41f5-822d-fed0e801864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서를 NumPy 배열로 변환 후 저장\n",
    "np.save(\"target_s_array.npy\", target_s_.cpu().numpy())\n",
    "np.save(\"collected_features_train_array.npy\", collected_features_train_.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281aae3-6aa0-4337-92f2-8c198048278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy 배열 로드 후 다시 텐서로 변환\n",
    "target_s_ = np.load(\"target_s_array.npy\")\n",
    "target_s_ = torch.tensor(target_s_)\n",
    "collected_features_train_ = np.load(\"collected_features_train_array.npy\")\n",
    "collected_features_train_ = torch.tensor(collected_features_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b3b21d61-192b-405c-8829-8058ccc08c3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected_regions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# SHAP Explainer 생성\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshap_model_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# SHAP 값 계산\u001b[39;00m\n\u001b[0;32m     29\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X)  \u001b[38;5;66;03m# [Batch, Feature_dim]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:90\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, masker)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m \u001b[43mTFDeep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_phase_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m PyTorchDeep(model, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:84\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tf, tf_ops, tf_backprop, tf_execute, tf_gradients_impl\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop \u001b[38;5;28;01mas\u001b[39;00m tf_backprop\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute \u001b[38;5;28;01mas\u001b[39;00m tf_execute\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     87\u001b[0m         ops \u001b[38;5;28;01mas\u001b[39;00m tf_ops,\n\u001b[0;32m     88\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "model.eval()  # 평가 모드\n",
    "\n",
    "# Input 데이터 변환: [batch_size, 29, 1024] -> [batch_size, 29 * 1024]\n",
    "X = collected_features_train_.reshape(collected_features_train_.shape[0], -1).cpu().numpy()\n",
    "\n",
    "def shap_model_fn(x):\n",
    "    # SHAP에서 입력받은 x는 [1, 29 * 1024] 형태\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    x_tensor = x_tensor.view(x_tensor.shape[0], 29, 1024)  # 원래 모델 입력 형태로 변환\n",
    "\n",
    "    # `class_detected`의 shape을 SHAP에서 전달된 batch size에 맞게 변경\n",
    "    batch_size = x_tensor.shape[0]\n",
    "    class_detected_shap = target_s_[:batch_size, :]  # `target_s_`의 크기를 맞춤\n",
    "\n",
    "    # 모델 실행\n",
    "    selected_regions, _ = model(\n",
    "        top_region_features=x_tensor, \n",
    "        class_detected=class_detected_shap,\n",
    "        return_loss=False\n",
    "    )\n",
    "    return selected_regions.cpu().numpy()\n",
    "\n",
    "# SHAP Explainer 생성\n",
    "explainer = shap.DeepExplainer(model=shap_model_fn, data=X)\n",
    "\n",
    "# SHAP 값 계산\n",
    "shap_values = explainer.shap_values(X)  # [Batch, Feature_dim]\n",
    "\n",
    "# 각 데이터 포인트의 가장 중요한 특성 출력\n",
    "for i, sample_shap_values in enumerate(shap_values):\n",
    "    important_feature_idx = np.argmax(np.abs(sample_shap_values))  # SHAP 값 절대값 기준\n",
    "    print(f\"Sample {i}: Most important feature index: {important_feature_idx}, SHAP value: {sample_shap_values[important_feature_idx]}\")\n",
    "\n",
    "# 시각화\n",
    "shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0daa35a-c82e-4654-8fa9-4288f521f4f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m     _, selected_region_features \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     15\u001b[0m         top_region_features\u001b[38;5;241m=\u001b[39mx_tensor,\n\u001b[0;32m     16\u001b[0m         return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 필요 시만 설정\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected_region_features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 20\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_model_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m     24\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:90\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, masker)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m \u001b[43mTFDeep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_phase_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m PyTorchDeep(model, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:84\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tf, tf_ops, tf_backprop, tf_execute, tf_gradients_impl\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop \u001b[38;5;28;01mas\u001b[39;00m tf_backprop\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute \u001b[38;5;28;01mas\u001b[39;00m tf_execute\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     87\u001b[0m         ops \u001b[38;5;28;01mas\u001b[39;00m tf_ops,\n\u001b[0;32m     88\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "X = collected_features_train_.cpu().numpy()\n",
    "\n",
    "def shap_model_fn(x):\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32).to(next(model.parameters()).device)\n",
    "    x_tensor = x_tensor.view(x_tensor.shape[0], 29, 1024)\n",
    "    \n",
    "    # `class_detected` 기본값을 사용하도록 호출\n",
    "    _, selected_region_features = model(\n",
    "        top_region_features=x_tensor,\n",
    "        return_loss=False  # 필요 시만 설정\n",
    "    )\n",
    "    return selected_region_features.cpu().numpy()\n",
    "\n",
    "explainer = shap.DeepExplainer(shap_model_fn, torch.tensor(X, dtype=torch.float32).to(next(model.parameters()).device))\n",
    "\n",
    "shap_values = explainer.shap_values(torch.tensor(X, dtype=torch.float32).to(next(model.parameters()).device))\n",
    "\n",
    "shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67626fdc",
   "metadata": {},
   "source": [
    "## nlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9afb013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_nlp = []\n",
    "\n",
    "for key in checkpoint['model'].keys():\n",
    "    a = 'language_model'\n",
    "    if a in key:\n",
    "        cp_nlp.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2fddccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language_model.gpt_with_lm_head.transformer.wte.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.wpe.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.0.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.1.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.2.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.3.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.4.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.5.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.6.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.7.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.8.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.9.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.10.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.11.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.12.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.13.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.14.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.15.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.16.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.17.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.18.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.19.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.20.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.21.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.22.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.ln_1.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.ln_1.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.causal_mask',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.mask_out_value',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.c_attn.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.c_attn.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.uk.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.uk.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.uv.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.attn.uv.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.ln_2.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.ln_2.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.mlp.c_fc.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.mlp.c_fc.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.mlp.c_proj.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.h.23.mlp.c_proj.bias',\n",
       " 'language_model.gpt_with_lm_head.transformer.ln_f.weight',\n",
       " 'language_model.gpt_with_lm_head.transformer.ln_f.bias',\n",
       " 'language_model.gpt_with_lm_head.lm_head.weight',\n",
       " 'language_model.gpt.wte.weight',\n",
       " 'language_model.gpt.wpe.weight',\n",
       " 'language_model.gpt.h.0.ln_1.weight',\n",
       " 'language_model.gpt.h.0.ln_1.bias',\n",
       " 'language_model.gpt.h.0.attn.causal_mask',\n",
       " 'language_model.gpt.h.0.attn.mask_out_value',\n",
       " 'language_model.gpt.h.0.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.0.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.0.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.0.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.0.attn.uk.weight',\n",
       " 'language_model.gpt.h.0.attn.uk.bias',\n",
       " 'language_model.gpt.h.0.attn.uv.weight',\n",
       " 'language_model.gpt.h.0.attn.uv.bias',\n",
       " 'language_model.gpt.h.0.ln_2.weight',\n",
       " 'language_model.gpt.h.0.ln_2.bias',\n",
       " 'language_model.gpt.h.0.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.0.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.0.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.0.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.1.ln_1.weight',\n",
       " 'language_model.gpt.h.1.ln_1.bias',\n",
       " 'language_model.gpt.h.1.attn.causal_mask',\n",
       " 'language_model.gpt.h.1.attn.mask_out_value',\n",
       " 'language_model.gpt.h.1.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.1.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.1.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.1.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.1.attn.uk.weight',\n",
       " 'language_model.gpt.h.1.attn.uk.bias',\n",
       " 'language_model.gpt.h.1.attn.uv.weight',\n",
       " 'language_model.gpt.h.1.attn.uv.bias',\n",
       " 'language_model.gpt.h.1.ln_2.weight',\n",
       " 'language_model.gpt.h.1.ln_2.bias',\n",
       " 'language_model.gpt.h.1.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.1.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.1.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.1.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.2.ln_1.weight',\n",
       " 'language_model.gpt.h.2.ln_1.bias',\n",
       " 'language_model.gpt.h.2.attn.causal_mask',\n",
       " 'language_model.gpt.h.2.attn.mask_out_value',\n",
       " 'language_model.gpt.h.2.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.2.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.2.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.2.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.2.attn.uk.weight',\n",
       " 'language_model.gpt.h.2.attn.uk.bias',\n",
       " 'language_model.gpt.h.2.attn.uv.weight',\n",
       " 'language_model.gpt.h.2.attn.uv.bias',\n",
       " 'language_model.gpt.h.2.ln_2.weight',\n",
       " 'language_model.gpt.h.2.ln_2.bias',\n",
       " 'language_model.gpt.h.2.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.2.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.2.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.2.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.3.ln_1.weight',\n",
       " 'language_model.gpt.h.3.ln_1.bias',\n",
       " 'language_model.gpt.h.3.attn.causal_mask',\n",
       " 'language_model.gpt.h.3.attn.mask_out_value',\n",
       " 'language_model.gpt.h.3.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.3.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.3.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.3.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.3.attn.uk.weight',\n",
       " 'language_model.gpt.h.3.attn.uk.bias',\n",
       " 'language_model.gpt.h.3.attn.uv.weight',\n",
       " 'language_model.gpt.h.3.attn.uv.bias',\n",
       " 'language_model.gpt.h.3.ln_2.weight',\n",
       " 'language_model.gpt.h.3.ln_2.bias',\n",
       " 'language_model.gpt.h.3.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.3.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.3.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.3.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.4.ln_1.weight',\n",
       " 'language_model.gpt.h.4.ln_1.bias',\n",
       " 'language_model.gpt.h.4.attn.causal_mask',\n",
       " 'language_model.gpt.h.4.attn.mask_out_value',\n",
       " 'language_model.gpt.h.4.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.4.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.4.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.4.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.4.attn.uk.weight',\n",
       " 'language_model.gpt.h.4.attn.uk.bias',\n",
       " 'language_model.gpt.h.4.attn.uv.weight',\n",
       " 'language_model.gpt.h.4.attn.uv.bias',\n",
       " 'language_model.gpt.h.4.ln_2.weight',\n",
       " 'language_model.gpt.h.4.ln_2.bias',\n",
       " 'language_model.gpt.h.4.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.4.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.4.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.4.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.5.ln_1.weight',\n",
       " 'language_model.gpt.h.5.ln_1.bias',\n",
       " 'language_model.gpt.h.5.attn.causal_mask',\n",
       " 'language_model.gpt.h.5.attn.mask_out_value',\n",
       " 'language_model.gpt.h.5.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.5.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.5.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.5.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.5.attn.uk.weight',\n",
       " 'language_model.gpt.h.5.attn.uk.bias',\n",
       " 'language_model.gpt.h.5.attn.uv.weight',\n",
       " 'language_model.gpt.h.5.attn.uv.bias',\n",
       " 'language_model.gpt.h.5.ln_2.weight',\n",
       " 'language_model.gpt.h.5.ln_2.bias',\n",
       " 'language_model.gpt.h.5.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.5.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.5.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.5.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.6.ln_1.weight',\n",
       " 'language_model.gpt.h.6.ln_1.bias',\n",
       " 'language_model.gpt.h.6.attn.causal_mask',\n",
       " 'language_model.gpt.h.6.attn.mask_out_value',\n",
       " 'language_model.gpt.h.6.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.6.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.6.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.6.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.6.attn.uk.weight',\n",
       " 'language_model.gpt.h.6.attn.uk.bias',\n",
       " 'language_model.gpt.h.6.attn.uv.weight',\n",
       " 'language_model.gpt.h.6.attn.uv.bias',\n",
       " 'language_model.gpt.h.6.ln_2.weight',\n",
       " 'language_model.gpt.h.6.ln_2.bias',\n",
       " 'language_model.gpt.h.6.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.6.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.6.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.6.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.7.ln_1.weight',\n",
       " 'language_model.gpt.h.7.ln_1.bias',\n",
       " 'language_model.gpt.h.7.attn.causal_mask',\n",
       " 'language_model.gpt.h.7.attn.mask_out_value',\n",
       " 'language_model.gpt.h.7.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.7.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.7.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.7.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.7.attn.uk.weight',\n",
       " 'language_model.gpt.h.7.attn.uk.bias',\n",
       " 'language_model.gpt.h.7.attn.uv.weight',\n",
       " 'language_model.gpt.h.7.attn.uv.bias',\n",
       " 'language_model.gpt.h.7.ln_2.weight',\n",
       " 'language_model.gpt.h.7.ln_2.bias',\n",
       " 'language_model.gpt.h.7.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.7.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.7.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.7.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.8.ln_1.weight',\n",
       " 'language_model.gpt.h.8.ln_1.bias',\n",
       " 'language_model.gpt.h.8.attn.causal_mask',\n",
       " 'language_model.gpt.h.8.attn.mask_out_value',\n",
       " 'language_model.gpt.h.8.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.8.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.8.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.8.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.8.attn.uk.weight',\n",
       " 'language_model.gpt.h.8.attn.uk.bias',\n",
       " 'language_model.gpt.h.8.attn.uv.weight',\n",
       " 'language_model.gpt.h.8.attn.uv.bias',\n",
       " 'language_model.gpt.h.8.ln_2.weight',\n",
       " 'language_model.gpt.h.8.ln_2.bias',\n",
       " 'language_model.gpt.h.8.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.8.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.8.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.8.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.9.ln_1.weight',\n",
       " 'language_model.gpt.h.9.ln_1.bias',\n",
       " 'language_model.gpt.h.9.attn.causal_mask',\n",
       " 'language_model.gpt.h.9.attn.mask_out_value',\n",
       " 'language_model.gpt.h.9.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.9.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.9.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.9.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.9.attn.uk.weight',\n",
       " 'language_model.gpt.h.9.attn.uk.bias',\n",
       " 'language_model.gpt.h.9.attn.uv.weight',\n",
       " 'language_model.gpt.h.9.attn.uv.bias',\n",
       " 'language_model.gpt.h.9.ln_2.weight',\n",
       " 'language_model.gpt.h.9.ln_2.bias',\n",
       " 'language_model.gpt.h.9.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.9.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.9.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.9.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.10.ln_1.weight',\n",
       " 'language_model.gpt.h.10.ln_1.bias',\n",
       " 'language_model.gpt.h.10.attn.causal_mask',\n",
       " 'language_model.gpt.h.10.attn.mask_out_value',\n",
       " 'language_model.gpt.h.10.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.10.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.10.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.10.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.10.attn.uk.weight',\n",
       " 'language_model.gpt.h.10.attn.uk.bias',\n",
       " 'language_model.gpt.h.10.attn.uv.weight',\n",
       " 'language_model.gpt.h.10.attn.uv.bias',\n",
       " 'language_model.gpt.h.10.ln_2.weight',\n",
       " 'language_model.gpt.h.10.ln_2.bias',\n",
       " 'language_model.gpt.h.10.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.10.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.10.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.10.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.11.ln_1.weight',\n",
       " 'language_model.gpt.h.11.ln_1.bias',\n",
       " 'language_model.gpt.h.11.attn.causal_mask',\n",
       " 'language_model.gpt.h.11.attn.mask_out_value',\n",
       " 'language_model.gpt.h.11.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.11.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.11.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.11.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.11.attn.uk.weight',\n",
       " 'language_model.gpt.h.11.attn.uk.bias',\n",
       " 'language_model.gpt.h.11.attn.uv.weight',\n",
       " 'language_model.gpt.h.11.attn.uv.bias',\n",
       " 'language_model.gpt.h.11.ln_2.weight',\n",
       " 'language_model.gpt.h.11.ln_2.bias',\n",
       " 'language_model.gpt.h.11.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.11.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.11.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.11.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.12.ln_1.weight',\n",
       " 'language_model.gpt.h.12.ln_1.bias',\n",
       " 'language_model.gpt.h.12.attn.causal_mask',\n",
       " 'language_model.gpt.h.12.attn.mask_out_value',\n",
       " 'language_model.gpt.h.12.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.12.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.12.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.12.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.12.attn.uk.weight',\n",
       " 'language_model.gpt.h.12.attn.uk.bias',\n",
       " 'language_model.gpt.h.12.attn.uv.weight',\n",
       " 'language_model.gpt.h.12.attn.uv.bias',\n",
       " 'language_model.gpt.h.12.ln_2.weight',\n",
       " 'language_model.gpt.h.12.ln_2.bias',\n",
       " 'language_model.gpt.h.12.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.12.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.12.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.12.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.13.ln_1.weight',\n",
       " 'language_model.gpt.h.13.ln_1.bias',\n",
       " 'language_model.gpt.h.13.attn.causal_mask',\n",
       " 'language_model.gpt.h.13.attn.mask_out_value',\n",
       " 'language_model.gpt.h.13.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.13.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.13.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.13.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.13.attn.uk.weight',\n",
       " 'language_model.gpt.h.13.attn.uk.bias',\n",
       " 'language_model.gpt.h.13.attn.uv.weight',\n",
       " 'language_model.gpt.h.13.attn.uv.bias',\n",
       " 'language_model.gpt.h.13.ln_2.weight',\n",
       " 'language_model.gpt.h.13.ln_2.bias',\n",
       " 'language_model.gpt.h.13.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.13.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.13.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.13.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.14.ln_1.weight',\n",
       " 'language_model.gpt.h.14.ln_1.bias',\n",
       " 'language_model.gpt.h.14.attn.causal_mask',\n",
       " 'language_model.gpt.h.14.attn.mask_out_value',\n",
       " 'language_model.gpt.h.14.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.14.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.14.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.14.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.14.attn.uk.weight',\n",
       " 'language_model.gpt.h.14.attn.uk.bias',\n",
       " 'language_model.gpt.h.14.attn.uv.weight',\n",
       " 'language_model.gpt.h.14.attn.uv.bias',\n",
       " 'language_model.gpt.h.14.ln_2.weight',\n",
       " 'language_model.gpt.h.14.ln_2.bias',\n",
       " 'language_model.gpt.h.14.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.14.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.14.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.14.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.15.ln_1.weight',\n",
       " 'language_model.gpt.h.15.ln_1.bias',\n",
       " 'language_model.gpt.h.15.attn.causal_mask',\n",
       " 'language_model.gpt.h.15.attn.mask_out_value',\n",
       " 'language_model.gpt.h.15.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.15.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.15.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.15.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.15.attn.uk.weight',\n",
       " 'language_model.gpt.h.15.attn.uk.bias',\n",
       " 'language_model.gpt.h.15.attn.uv.weight',\n",
       " 'language_model.gpt.h.15.attn.uv.bias',\n",
       " 'language_model.gpt.h.15.ln_2.weight',\n",
       " 'language_model.gpt.h.15.ln_2.bias',\n",
       " 'language_model.gpt.h.15.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.15.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.15.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.15.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.16.ln_1.weight',\n",
       " 'language_model.gpt.h.16.ln_1.bias',\n",
       " 'language_model.gpt.h.16.attn.causal_mask',\n",
       " 'language_model.gpt.h.16.attn.mask_out_value',\n",
       " 'language_model.gpt.h.16.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.16.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.16.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.16.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.16.attn.uk.weight',\n",
       " 'language_model.gpt.h.16.attn.uk.bias',\n",
       " 'language_model.gpt.h.16.attn.uv.weight',\n",
       " 'language_model.gpt.h.16.attn.uv.bias',\n",
       " 'language_model.gpt.h.16.ln_2.weight',\n",
       " 'language_model.gpt.h.16.ln_2.bias',\n",
       " 'language_model.gpt.h.16.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.16.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.16.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.16.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.17.ln_1.weight',\n",
       " 'language_model.gpt.h.17.ln_1.bias',\n",
       " 'language_model.gpt.h.17.attn.causal_mask',\n",
       " 'language_model.gpt.h.17.attn.mask_out_value',\n",
       " 'language_model.gpt.h.17.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.17.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.17.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.17.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.17.attn.uk.weight',\n",
       " 'language_model.gpt.h.17.attn.uk.bias',\n",
       " 'language_model.gpt.h.17.attn.uv.weight',\n",
       " 'language_model.gpt.h.17.attn.uv.bias',\n",
       " 'language_model.gpt.h.17.ln_2.weight',\n",
       " 'language_model.gpt.h.17.ln_2.bias',\n",
       " 'language_model.gpt.h.17.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.17.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.17.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.17.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.18.ln_1.weight',\n",
       " 'language_model.gpt.h.18.ln_1.bias',\n",
       " 'language_model.gpt.h.18.attn.causal_mask',\n",
       " 'language_model.gpt.h.18.attn.mask_out_value',\n",
       " 'language_model.gpt.h.18.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.18.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.18.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.18.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.18.attn.uk.weight',\n",
       " 'language_model.gpt.h.18.attn.uk.bias',\n",
       " 'language_model.gpt.h.18.attn.uv.weight',\n",
       " 'language_model.gpt.h.18.attn.uv.bias',\n",
       " 'language_model.gpt.h.18.ln_2.weight',\n",
       " 'language_model.gpt.h.18.ln_2.bias',\n",
       " 'language_model.gpt.h.18.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.18.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.18.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.18.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.19.ln_1.weight',\n",
       " 'language_model.gpt.h.19.ln_1.bias',\n",
       " 'language_model.gpt.h.19.attn.causal_mask',\n",
       " 'language_model.gpt.h.19.attn.mask_out_value',\n",
       " 'language_model.gpt.h.19.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.19.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.19.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.19.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.19.attn.uk.weight',\n",
       " 'language_model.gpt.h.19.attn.uk.bias',\n",
       " 'language_model.gpt.h.19.attn.uv.weight',\n",
       " 'language_model.gpt.h.19.attn.uv.bias',\n",
       " 'language_model.gpt.h.19.ln_2.weight',\n",
       " 'language_model.gpt.h.19.ln_2.bias',\n",
       " 'language_model.gpt.h.19.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.19.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.19.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.19.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.20.ln_1.weight',\n",
       " 'language_model.gpt.h.20.ln_1.bias',\n",
       " 'language_model.gpt.h.20.attn.causal_mask',\n",
       " 'language_model.gpt.h.20.attn.mask_out_value',\n",
       " 'language_model.gpt.h.20.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.20.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.20.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.20.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.20.attn.uk.weight',\n",
       " 'language_model.gpt.h.20.attn.uk.bias',\n",
       " 'language_model.gpt.h.20.attn.uv.weight',\n",
       " 'language_model.gpt.h.20.attn.uv.bias',\n",
       " 'language_model.gpt.h.20.ln_2.weight',\n",
       " 'language_model.gpt.h.20.ln_2.bias',\n",
       " 'language_model.gpt.h.20.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.20.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.20.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.20.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.21.ln_1.weight',\n",
       " 'language_model.gpt.h.21.ln_1.bias',\n",
       " 'language_model.gpt.h.21.attn.causal_mask',\n",
       " 'language_model.gpt.h.21.attn.mask_out_value',\n",
       " 'language_model.gpt.h.21.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.21.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.21.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.21.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.21.attn.uk.weight',\n",
       " 'language_model.gpt.h.21.attn.uk.bias',\n",
       " 'language_model.gpt.h.21.attn.uv.weight',\n",
       " 'language_model.gpt.h.21.attn.uv.bias',\n",
       " 'language_model.gpt.h.21.ln_2.weight',\n",
       " 'language_model.gpt.h.21.ln_2.bias',\n",
       " 'language_model.gpt.h.21.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.21.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.21.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.21.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.22.ln_1.weight',\n",
       " 'language_model.gpt.h.22.ln_1.bias',\n",
       " 'language_model.gpt.h.22.attn.causal_mask',\n",
       " 'language_model.gpt.h.22.attn.mask_out_value',\n",
       " 'language_model.gpt.h.22.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.22.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.22.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.22.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.22.attn.uk.weight',\n",
       " 'language_model.gpt.h.22.attn.uk.bias',\n",
       " 'language_model.gpt.h.22.attn.uv.weight',\n",
       " 'language_model.gpt.h.22.attn.uv.bias',\n",
       " 'language_model.gpt.h.22.ln_2.weight',\n",
       " 'language_model.gpt.h.22.ln_2.bias',\n",
       " 'language_model.gpt.h.22.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.22.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.22.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.22.mlp.c_proj.bias',\n",
       " 'language_model.gpt.h.23.ln_1.weight',\n",
       " 'language_model.gpt.h.23.ln_1.bias',\n",
       " 'language_model.gpt.h.23.attn.causal_mask',\n",
       " 'language_model.gpt.h.23.attn.mask_out_value',\n",
       " 'language_model.gpt.h.23.attn.c_attn.weight',\n",
       " 'language_model.gpt.h.23.attn.c_attn.bias',\n",
       " 'language_model.gpt.h.23.attn.c_proj.weight',\n",
       " 'language_model.gpt.h.23.attn.c_proj.bias',\n",
       " 'language_model.gpt.h.23.attn.uk.weight',\n",
       " 'language_model.gpt.h.23.attn.uk.bias',\n",
       " 'language_model.gpt.h.23.attn.uv.weight',\n",
       " 'language_model.gpt.h.23.attn.uv.bias',\n",
       " 'language_model.gpt.h.23.ln_2.weight',\n",
       " 'language_model.gpt.h.23.ln_2.bias',\n",
       " 'language_model.gpt.h.23.mlp.c_fc.weight',\n",
       " 'language_model.gpt.h.23.mlp.c_fc.bias',\n",
       " 'language_model.gpt.h.23.mlp.c_proj.weight',\n",
       " 'language_model.gpt.h.23.mlp.c_proj.bias',\n",
       " 'language_model.gpt.ln_f.weight',\n",
       " 'language_model.gpt.ln_f.bias',\n",
       " 'language_model.lm_head.weight',\n",
       " 'language_model.wte.weight',\n",
       " 'language_model.wpe.weight',\n",
       " 'language_model.gpt2_blocks.0.0.weight',\n",
       " 'language_model.gpt2_blocks.0.0.bias',\n",
       " 'language_model.gpt2_blocks.0.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.0.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.0.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.0.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.0.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.0.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.0.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.0.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.0.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.0.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.0.2.weight',\n",
       " 'language_model.gpt2_blocks.0.2.bias',\n",
       " 'language_model.gpt2_blocks.0.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.0.3.c_fc.bias',\n",
       " 'language_model.gpt2_blocks.0.3.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.0.3.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.1.0.weight',\n",
       " 'language_model.gpt2_blocks.1.0.bias',\n",
       " 'language_model.gpt2_blocks.1.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.1.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.1.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.1.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.1.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.1.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.1.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.1.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.1.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.1.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.1.2.weight',\n",
       " 'language_model.gpt2_blocks.1.2.bias',\n",
       " 'language_model.gpt2_blocks.1.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.1.3.c_fc.bias',\n",
       " 'language_model.gpt2_blocks.1.3.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.1.3.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.2.0.weight',\n",
       " 'language_model.gpt2_blocks.2.0.bias',\n",
       " 'language_model.gpt2_blocks.2.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.2.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.2.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.2.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.2.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.2.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.2.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.2.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.2.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.2.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.2.2.weight',\n",
       " 'language_model.gpt2_blocks.2.2.bias',\n",
       " 'language_model.gpt2_blocks.2.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.2.3.c_fc.bias',\n",
       " 'language_model.gpt2_blocks.2.3.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.2.3.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.3.0.weight',\n",
       " 'language_model.gpt2_blocks.3.0.bias',\n",
       " 'language_model.gpt2_blocks.3.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.3.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.3.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.3.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.3.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.3.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.3.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.3.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.3.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.3.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.3.2.weight',\n",
       " 'language_model.gpt2_blocks.3.2.bias',\n",
       " 'language_model.gpt2_blocks.3.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.3.3.c_fc.bias',\n",
       " 'language_model.gpt2_blocks.3.3.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.3.3.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.4.0.weight',\n",
       " 'language_model.gpt2_blocks.4.0.bias',\n",
       " 'language_model.gpt2_blocks.4.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.4.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.4.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.4.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.4.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.4.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.4.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.4.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.4.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.4.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.4.2.weight',\n",
       " 'language_model.gpt2_blocks.4.2.bias',\n",
       " 'language_model.gpt2_blocks.4.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.4.3.c_fc.bias',\n",
       " 'language_model.gpt2_blocks.4.3.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.4.3.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.5.0.weight',\n",
       " 'language_model.gpt2_blocks.5.0.bias',\n",
       " 'language_model.gpt2_blocks.5.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.5.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.5.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.5.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.5.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.5.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.5.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.5.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.5.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.5.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.5.2.weight',\n",
       " 'language_model.gpt2_blocks.5.2.bias',\n",
       " 'language_model.gpt2_blocks.5.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.5.3.c_fc.bias',\n",
       " 'language_model.gpt2_blocks.5.3.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.5.3.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.6.0.weight',\n",
       " 'language_model.gpt2_blocks.6.0.bias',\n",
       " 'language_model.gpt2_blocks.6.1.causal_mask',\n",
       " 'language_model.gpt2_blocks.6.1.mask_out_value',\n",
       " 'language_model.gpt2_blocks.6.1.c_attn.weight',\n",
       " 'language_model.gpt2_blocks.6.1.c_attn.bias',\n",
       " 'language_model.gpt2_blocks.6.1.c_proj.weight',\n",
       " 'language_model.gpt2_blocks.6.1.c_proj.bias',\n",
       " 'language_model.gpt2_blocks.6.1.uk.weight',\n",
       " 'language_model.gpt2_blocks.6.1.uk.bias',\n",
       " 'language_model.gpt2_blocks.6.1.uv.weight',\n",
       " 'language_model.gpt2_blocks.6.1.uv.bias',\n",
       " 'language_model.gpt2_blocks.6.2.weight',\n",
       " 'language_model.gpt2_blocks.6.2.bias',\n",
       " 'language_model.gpt2_blocks.6.3.c_fc.weight',\n",
       " 'language_model.gpt2_blocks.6.3.c_fc.bias',\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "baf968d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchinfo import summary\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers.generation_beam_search import BeamSearchScorer\n",
    "\n",
    "\n",
    "class Conv1DWithTrainedWeights(nn.Module):\n",
    "    \"\"\"\n",
    "    Same functionality as Conv1D class of transformers.pytorch_utils but allows initialization with trained weights.\n",
    "\n",
    "    Conv1D has the same functionality as a linear layer.\n",
    "    It transforms the inputted hidden_states from shape [batch x sequence_len x hidden_dim] to [batch x sequence_len x 3*hidden_dim],\n",
    "    thus allowing the retrieval of the query, key and value matrices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trained_weight, trained_bias):\n",
    "        super(Conv1DWithTrainedWeights, self).__init__()\n",
    "        self.weight = nn.Parameter(trained_weight, requires_grad=False)  # of shape [hidden_dim x 3*hidden_dim] for c_attn, of shape [hidden_dim x hidden_dim] for c_proj\n",
    "        self.bias = nn.Parameter(trained_bias, requires_grad=False)  # of shape [3 * hidden_dim] for c_attn, of shape [hidden_dim] for c_proj\n",
    "\n",
    "    def forward(self, x):  # x has shape [batch x sequence_len x hidden_dim]\n",
    "        size_out = x.size()[:-1] + (self.weight.size(-1),)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x  # x has shape [batch x sequence_len x 3*hidden_dim] for c_attn, shape [batch x sequence_len x hidden_dim] for c_proj\n",
    "\n",
    "\n",
    "class GPT2PseudoAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_attn_weights_and_bias: tuple[torch.FloatTensor],  # pre-trained weights and bias for retrieving query, key, value matrices\n",
    "        c_proj_weights_and_bias: tuple[torch.FloatTensor],  # pre-trained weights and bias for projecting concatenated heads to original hidden dim\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.c_attn = Conv1DWithTrainedWeights(\n",
    "            trained_weight=c_attn_weights_and_bias[0],\n",
    "            trained_bias=c_attn_weights_and_bias[1],\n",
    "        )\n",
    "        self.c_proj = Conv1DWithTrainedWeights(\n",
    "            trained_weight=c_proj_weights_and_bias[0],\n",
    "            trained_bias=c_proj_weights_and_bias[1],\n",
    "        )\n",
    "\n",
    "        self.embed_dim = 1024\n",
    "        self.num_heads = 16\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.split_size = self.embed_dim\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(p=0.1)\n",
    "        self.resid_dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        # seq_len can maximally be 1024 tokens\n",
    "        max_positions = 1024\n",
    "\n",
    "        # create a causal mask for masking out attention weights in the masked self-attention operator (masking out weights of tokens that lie ahead of the attended token)\n",
    "        # first create a lower triangular matrix\n",
    "        lower_triangular_matrix = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8))\n",
    "        # then save lower_triangular_matrix (with additional dimensions for batch_size and num_heads) in a buffer\n",
    "        # (to make sure the causal mask does not get updated during backprop)\n",
    "        self.register_buffer(\"causal_mask\", lower_triangular_matrix.view(1, 1, max_positions, max_positions))\n",
    "\n",
    "        # value for masking out attention weights\n",
    "        self.register_buffer(\"mask_out_value\", torch.tensor(-1e4))\n",
    "\n",
    "        # matrices for getting key and value matrices for image hidden states\n",
    "        self.uk = nn.Linear(in_features=self.embed_dim, out_features=self.embed_dim)\n",
    "        self.uv = nn.Linear(in_features=self.embed_dim, out_features=self.embed_dim)\n",
    "\n",
    "    def _split_heads(self, tensor, num_heads, head_dim):\n",
    "        \"\"\"\n",
    "        Splits hidden_dim (i.e. 1024) into num_heads (i.e. 16) and head_dim (i.e. 64)\n",
    "        \"\"\"\n",
    "        new_shape = tensor.size()[:-1] + (num_heads, head_dim)\n",
    "        tensor = tensor.view(new_shape)\n",
    "        return tensor.permute(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    def _attn(self, query_word, key_image_word, value_image_word, attention_mask):\n",
    "        attn_weights = torch.matmul(query_word, key_image_word.transpose(-1, -2))  # shape [batch_size x num_heads x seq_len x 1+seq_len]\n",
    "\n",
    "        # scale attention weights\n",
    "        attn_weights = attn_weights / (value_image_word.size(-1) ** 0.5)\n",
    "\n",
    "        # create and apply the final causal mask to weights\n",
    "        query_length, key_length = query_word.size(-2), key_image_word.size(-2)\n",
    "\n",
    "        # note that this causal mask has a shape of seq_len x 1+seq_len (in the last 2 dims),\n",
    "        # with the first column of the mask only consisting of True boolean values\n",
    "        # meaning attention weights corresponding to images (which are stored in the first column) are not masked out!\n",
    "        causal_mask = self.causal_mask[:, :, key_length - query_length: key_length, :key_length].to(torch.bool)\n",
    "\n",
    "        # select the attention weights where the causal mask has True values, select -1e4 where the causal mask has False values\n",
    "        attn_weights = torch.where(causal_mask, attn_weights, self.mask_out_value.to(attn_weights.dtype))\n",
    "\n",
    "        # apply the attention mask of shape [batch_size, 1, 1, 1+seq_len] for masking out padding tokens\n",
    "        # there is an additional column of zeros for the attention weights corresponding to the image,\n",
    "        # such that these are not masked out\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # downcast (if necessary) back to V's dtype (if in mixed-precision) -- no-op otherwise\n",
    "        attn_weights = attn_weights.type(value_image_word.dtype)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_image_word)  # shape [batch_size x num_heads x seq_len x head_dim]\n",
    "\n",
    "        return attn_output,attn_weights\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, head_dim):\n",
    "        \"\"\"\n",
    "        Merges num_heads (i.e. 16) and head_dim (i.e. 64) into hidden_dim (i.e. 1024)\n",
    "        \"\"\"\n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * head_dim,)\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def _merge_heads(self, tensor, num_heads, head_dim):\n",
    "        \"\"\"\n",
    "        Merges num_heads and head_dim into hidden_dim\n",
    "        \"\"\"\n",
    "        # Debug input tensor shape\n",
    "        #print(f\"[DEBUG] Input tensor shape before merge_heads: {tensor.shape}\")\n",
    "        #print(f\"[DEBUG] num_heads: {num_heads}, head_dim: {head_dim}\")\n",
    "    \n",
    "        if tensor.size(1) != num_heads:\n",
    "            raise ValueError(f\"[ERROR] Number of heads mismatch: expected {num_heads}, got {tensor.size(1)}\")\n",
    "    \n",
    "        tensor = tensor.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len, num_heads, head_dim)\n",
    "        new_shape = tensor.size()[:-2] + (num_heads * head_dim,)  # (batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "        #print(f\"[DEBUG] Output tensor shape after merge_heads: {new_shape}\")\n",
    "        return tensor.view(new_shape)\n",
    "\n",
    "    def forward(self,\n",
    "                word_hidden_states,  # shape [batch_size x seq_len x hidden_dim]\n",
    "                image_hidden_states,  # shape [batch_size x hidden_dim]\n",
    "                attention_mask,  # shape [batch_size, 1, 1, 1+seq_len]\n",
    "                layer_past,\n",
    "                use_cache,output_attentions=True):\n",
    "\n",
    "        # query, key, value matrices each have shape [batch_size x seq_len x hidden_dim]\n",
    "        q_word, k_word, v_word = self.c_attn(word_hidden_states).split(self.split_size, dim=2)\n",
    "        \n",
    "        # if layer_past is None, we are either training the model or generating the first token in text generation mode\n",
    "        if layer_past is None:\n",
    "            # add an addition dimension to the image_hidden_states\n",
    "            #print(\"1 image hidden state : \",image_hidden_states.shape)\n",
    "            image_hidden_states = image_hidden_states[:, None, :]  # shape [batch_size x 1 x hidden_dim]\n",
    "\n",
    "            # get the key and value matrices for the image hidden states\n",
    "            k_image = self.uk(image_hidden_states)  # shape [batch_size x 1 x hidden_dim]\n",
    "            #print('k_image',k_image.shape)\n",
    "            #print(\"2 k image hidden state : \",k_image.shape)\n",
    "            v_image = self.uv(image_hidden_states)  # shape [batch_size x 1 x hidden_dim]\n",
    "            #print(\"3 v image hidden state : \",v_image.shape)\n",
    "            # if the batch_size is different, then we are in beam search generation mode (adjust k and v image matrices accordingly)\n",
    "            if k_image.size(0) != k_word.size(0):\n",
    "                num_beams = k_word.size(0) // k_image.size(0)\n",
    "                k_image = k_image.repeat_interleave(num_beams, dim=0)\n",
    "                v_image = v_image.repeat_interleave(num_beams, dim=0)\n",
    "            #print(f'image_hidden_states:{image_hidden_states}')\n",
    "            #print(f'kword:{k_word.shape}')\n",
    "            #print(f'k image:{k_image.shape}')\n",
    "            #print(\"4 v image hidden state : \",v_image.shape)\n",
    "            k_image_word = torch.cat((k_image, k_word), dim=1)  # shape [batch_size x 1+seq_len x hidden_dim]\n",
    "           \n",
    "            v_image_word = torch.cat((v_image, v_word), dim=1)  # shape [batch_size x 1+seq_len x hidden_dim]\n",
    "            #print(\"5 v word hidden state : \",v_word.shape)\n",
    "            #print(\"6 v image word hidden state : \",v_image_word.shape)\n",
    "            q_word = self._split_heads(q_word, self.num_heads, self.head_dim)  # shape [batch_size x num_heads x seq_len x head_dim]\n",
    "            k_image_word = self._split_heads(k_image_word, self.num_heads, self.head_dim)  # shape [batch_size x num_heads x 1+seq_len x head_dim]\n",
    "            v_image_word = self._split_heads(v_image_word, self.num_heads, self.head_dim)  # shape [batch_size x num_heads x 1+seq_len x head_dim]\n",
    "            #print(f\"7 q word : {q_word.shape}, k img word : {k_image_word.shape}, v img word : {v_image_word.shape}\")\n",
    "            if use_cache is True:\n",
    "                present = (k_image_word, v_image_word)\n",
    "            else:\n",
    "                present = None\n",
    "\n",
    "            attn_output,attn_weights = self._attn(q_word, k_image_word, v_image_word, attention_mask)  # shape [batch_size x num_heads x seq_len x head_dim]\n",
    "            #print(f\"8 attn_output : {attn_output.shape}, attn_weights : {attn_weights.shape}\")\n",
    "        else:\n",
    "            # if there is a layer_past (which stores key and value tensors of past tokens), then this means we are in text generation mode\n",
    "            q_word = self._split_heads(q_word, self.num_heads, self.head_dim)  # shape [batch_size x num_heads x 1 x head_dim]\n",
    "            k_word = self._split_heads(k_word, self.num_heads, self.head_dim)  # shape [batch_size x num_heads x 1 x head_dim]\n",
    "            v_word = self._split_heads(v_word, self.num_heads, self.head_dim)  # shape [batch_size x num_heads x 1 x head_dim]\n",
    "\n",
    "            past_key, past_value = layer_past\n",
    "            k = torch.cat((past_key, k_word), dim=-2)\n",
    "            v = torch.cat((past_value, v_word), dim=-2)\n",
    "\n",
    "            present = (k, v)\n",
    "\n",
    "            attn_output,attn_weights = self._attn(q_word, k, v, attention_mask)  # shape [batch_size x num_heads x seq_len x head_dim]\n",
    "\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)  # shape [batch_size x seq_len x hidden_dim]\n",
    "          # shape [batch_size x seq_len x hidden_dim]\n",
    "        attn_output = self.c_proj(attn_output)\n",
    "        attn_output = self.resid_dropout(attn_output)  # shape [batch_size x seq_len x hidden_dim]\n",
    "        if output_attentions:\n",
    "            #print(\"9 Attention shape: \",attn_weights.shape, attn_output.shape, self.num_heads, self.head_dim)\n",
    "            #attn_weights = self._merge_heads(attn_weights, self.num_heads, self.head_dim)\n",
    "            return attn_output,present,attn_weights\n",
    "        return attn_output, present\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT2 model with a language modeling head and pseudo self-attention.\n",
    "\n",
    "    Pseudo self-attention is based on the papar \"Encoder-Agnostic Adaptation for Conditional Language Generation\" (https://arxiv.org/abs/1908.06938).\n",
    "    It is a technique to condition a pretrained language model to arbitrary conditional input (in my case features of chest x-ray images).\n",
    "\n",
    "    The code is largely the same as the GPT2 implementation by Huggingface (https://github.com/huggingface/transformers/blob/d0acc9537829e7d067edbb791473bbceb2ecf056/src/transformers/models/gpt2/modeling_gpt2.py),\n",
    "    except for the custom GPT2PseudoAttention class replacing the GPT2Attention class.\n",
    "\n",
    "    Recommended reading to understand the GPT2 source code: https://amaarora.github.io/2020/02/18/annotatedGPT2.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.checkpoint = \"healx/gpt-2-pubmed-medium\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bos_token_id = 50256\n",
    "        self.eos_token_id = 50256\n",
    "        self.pad_token_id = 50256\n",
    "\n",
    "        # use GPT2 model with language modeling head, since we want to generate phrases\n",
    "        self.gpt_with_lm_head = GPT2LMHeadModel.from_pretrained(self.checkpoint)\n",
    "\n",
    "        # freeze all parameters of the model\n",
    "        for param in self.gpt_with_lm_head.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # replace normal attention layers by pseudo attention layers\n",
    "        self._replace_attention_by_pseudo_attention()\n",
    "\n",
    "        # divide model into GPT part and language modeling head part\n",
    "        self.gpt = self.gpt_with_lm_head.transformer\n",
    "        self.lm_head = self.gpt_with_lm_head.lm_head\n",
    "\n",
    "        # divide GPT part into word embedding layer, positional embedding layer, dropout layer, gpt2 blocks and final layernorm\n",
    "        gpt_children = list(self.gpt.children())\n",
    "        self.wte = gpt_children[0]  # word embedding layer\n",
    "        self.wpe = gpt_children[1]  # positional embedding layer\n",
    "        self.drop = gpt_children[2]  # dropout layer\n",
    "        self.gpt2_blocks = gpt_children[3]  # type: nn.ModuleList\n",
    "        self.final_layernorm = gpt_children[4]\n",
    "\n",
    "        # convert each individual gpt2_block into a nn.ModuleList\n",
    "        self.gpt2_blocks = nn.ModuleList(nn.ModuleList(gpt2_block.children()) for gpt2_block in self.gpt2_blocks)\n",
    "\n",
    "        # small neural network to transform embeddings coming from the image feature space into embeddings in the text feature space\n",
    "        self.feature_space_transformation_nn = nn.Sequential(\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024)\n",
    "        )\n",
    "\n",
    "    def _replace_attention_by_pseudo_attention(self):\n",
    "        GPT2PSA_list = []\n",
    "\n",
    "        for gpt2_block in self.gpt_with_lm_head.transformer.h:\n",
    "            # extract trained weights and biases\n",
    "            attn = gpt2_block.attn\n",
    "            c_attn_weights = attn.c_attn.weight.detach()\n",
    "            c_attn_bias = attn.c_attn.bias.detach()\n",
    "            c_proj_weights = attn.c_proj.weight.detach()\n",
    "            c_proj_bias = attn.c_proj.bias.detach()\n",
    "\n",
    "            # initialize GPT2PseudoAttention module\n",
    "            GPT2PSA = GPT2PseudoAttention(\n",
    "                c_attn_weights_and_bias=(c_attn_weights, c_attn_bias),\n",
    "                c_proj_weights_and_bias=(c_proj_weights, c_proj_bias),\n",
    "            )\n",
    "\n",
    "            GPT2PSA_list.append(GPT2PSA)\n",
    "\n",
    "        for i, GPT2PSA in enumerate(GPT2PSA_list):\n",
    "            self.gpt_with_lm_head.transformer.h[i].attn = GPT2PSA\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor,  # shape [batch_size x seq_len]\n",
    "                attention_mask: torch.FloatTensor,  # shape [batch_size x seq_len]\n",
    "                image_hidden_states: torch.FloatTensor,  # shape [batch_size x image_hidden_dim] (with image_hidden_dim = 1024)\n",
    "                return_loss: bool = False,\n",
    "                past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "                position_ids: Optional[torch.LongTensor] = None,\n",
    "                use_cache: Optional[bool] = False,\n",
    "                output_attentions: bool = True,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        If return_loss is True, returns the language modeling loss.\n",
    "        If return_loss is False (in which we are in text generation mode and use_cache will be True), returns the language modeling logits (of shape batch_size x seq_len x vocab_size)\n",
    "        as well as the so-called presents (which store the key and value tensors of the previous tokens, such that they don't have to be recomputed every time during text generation).\n",
    "\n",
    "        To compute the loss, the input_ids are used as labels.\n",
    "        To prevent padding tokens from counting towards the loss, the attention_mask is transformed to a boolean mask and inverted.\n",
    "        Then this inverted boolean mask is used to set all padding token ids to -100.\n",
    "        In the cross entropy loss, the ignore_index is set to -100, such that padding token ids are ignored as targets.\n",
    "\n",
    "        Furthermore, the label at the first position of the sequence is discarded and the labels are shifted accordingly (i.e. one to the left),\n",
    "        such that the language modeling logits align with the labels that they are trying to predict.\n",
    "        \"\"\"\n",
    "        # get a boolean copy of the attention_mask and invert it\n",
    "        mask_to_ignore_padding_tokens_for_loss_computation = ~(attention_mask.to(torch.bool))\n",
    "\n",
    "        # transform image_hidden_states from image feature space to text feature space\n",
    "        image_hidden_states = self.feature_space_transformation_nn(image_hidden_states)  # shape [batch_size x word_hidden_dim], with word_hidden_dim = 1024\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        # pass the token ids through the word embedding layer to get the word embeddings\n",
    "        inputs_embeds = self.wte(input_ids)  # shape [batch_size x seq_len x hidden_dim]\n",
    "\n",
    "        # position_ids is a tensor that specifies the position of each token in the input (necessary to create positional embeddings)\n",
    "        if position_ids is not None:\n",
    "            position_ids = position_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        if past_key_values is None:\n",
    "            past_length = 0\n",
    "            past_key_values = tuple([None] * len(self.gpt2_blocks))\n",
    "        else:\n",
    "            past_length = past_key_values[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=self.device)\n",
    "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])  # shape [1 x seq_len]\n",
    "\n",
    "        # pass the position ids through the positional embedding layer to get the positional embeddings\n",
    "        position_embeds = self.wte(position_ids)  # shape [1 x seq_len x hidden_dim]\n",
    "\n",
    "        # addition is broadcasted around batch_size dimension\n",
    "        word_hidden_states = inputs_embeds + position_embeds  # shape [batch_size x seq_len x hidden_dim]\n",
    "\n",
    "        word_hidden_states = self.drop(word_hidden_states)\n",
    "\n",
    "        output_shape = input_shape + (word_hidden_states.size(-1),)\n",
    "\n",
    "        # we change the attention_mask shape to [batch_size, 1, 1, seq_len], since the attention_mask is later applied to the last dimension of\n",
    "        # the attention weights that are of shape [batch_size x num_heads x seq_len x 1+seq_len]\n",
    "        attention_mask = attention_mask.view(batch_size, -1)\n",
    "        attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "        # since we have 1 additional column in the attention weights (i.e. 1+seq_len in the last dimension) due to the additional concatenated key matrix\n",
    "        # of the image hidden states (see forward method of GPT2PseudoAttention), we have to shift the attention mask \"one to the right\" and add a column of ones\n",
    "        # to the left such that the attention weights corresponding to the image are not masked out\n",
    "        attention_mask_size = attention_mask.size()\n",
    "        ones_column = torch.ones(attention_mask_size[:-1] + (1,), dtype=torch.int64, device=self.device)  # shape [batch_size, 1, 1, 1]\n",
    "        attention_mask = torch.cat((ones_column, attention_mask), dim=-1)  # shape [batch_size, 1, 1, 1+seq_len]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely\n",
    "        attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)  # dtype should be either torch.float32 or torch.float16\n",
    "        attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "        presents = () if use_cache else None\n",
    "        attentions = []\n",
    "        for gpt2_block, layer_past in zip(self.gpt2_blocks, past_key_values):\n",
    "            layer_norm_1 = gpt2_block[0]\n",
    "            pseudo_self_attention = gpt2_block[1]\n",
    "            layer_norm_2 = gpt2_block[2]\n",
    "            mlp = gpt2_block[3]\n",
    "\n",
    "            residual = word_hidden_states\n",
    "            word_hidden_states = layer_norm_1(word_hidden_states)\n",
    "\n",
    "            word_hidden_states, present, attention_weights = pseudo_self_attention(word_hidden_states, image_hidden_states, attention_mask, layer_past, use_cache)\n",
    "            attentions.append(attention_weights)#youna add\n",
    "            # residual connection\n",
    "            word_hidden_states = word_hidden_states + residual\n",
    "\n",
    "            residual = word_hidden_states\n",
    "            word_hidden_states = layer_norm_2(word_hidden_states)\n",
    "            word_hidden_states = mlp(word_hidden_states)\n",
    "\n",
    "            # residual connection\n",
    "            word_hidden_states = word_hidden_states + residual\n",
    "\n",
    "            if use_cache:\n",
    "                presents += (present,)\n",
    "\n",
    "        word_hidden_states = self.final_layernorm(word_hidden_states)\n",
    "\n",
    "        word_hidden_states = word_hidden_states.view(output_shape)\n",
    "\n",
    "        lm_logits = self.lm_head(word_hidden_states)  # shape [batch_size x seq_len x vocab_size], with vocab_size = 50257\n",
    "\n",
    "        if return_loss:\n",
    "            # use input_ids as ground_truth labels\n",
    "            labels = input_ids\n",
    "\n",
    "            # set padding tokens to -100, such that they are ignored and don't count towards the loss\n",
    "            labels[mask_to_ignore_padding_tokens_for_loss_computation] = -100\n",
    "\n",
    "            # shift the tokens, i.e. discard the last token in the sequence for the logits,\n",
    "            # and discard the first token in the sequence for the labels\n",
    "\n",
    "            # this way, the logits of the first token are \"aligned\" with the second token label,\n",
    "            # the logits of the second token are \"aligned\" with the third token label, and so on...\n",
    "            # since the previous token should predict the next token\n",
    "\n",
    "            # discard the last lm_logit corresponding to the last token\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous()  # shape [batch_size x seq_len-1 x vocab_size]\n",
    "\n",
    "            # discard the first token in the sequence\n",
    "            shift_labels = labels[:, 1:].contiguous()  # shape [batch_size x seq_len-1]\n",
    "\n",
    "            # flatten the tokens\n",
    "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))  # shape [batch_size*seq_len-1 x vocab_size]\n",
    "            shift_labels = shift_labels.view(-1)  # shape [batch_size * seq_len-1]\n",
    "\n",
    "            # padding tokens are ignored for loss computation, and loss is averaged over non-ignored targets\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "            return loss\n",
    "        if output_attentions:\n",
    "            return lm_logits, presents,attentions\n",
    "        if use_cache:\n",
    "            return lm_logits, presents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                 image_hidden_states: torch.FloatTensor,  # shape [batch_size x image_hidden_dim]\n",
    "                 max_length: int = None,\n",
    "                 num_beams: int = 1,\n",
    "                 num_beam_groups: int = 1,\n",
    "                 do_sample: bool = False,\n",
    "                 num_return_sequences: int = 1,\n",
    "                 early_stopping: bool = False\n",
    "                 ) -> torch.LongTensor:  # shape [batch_size x longest_generated_sequence_length]\n",
    "        \"\"\"\n",
    "        Generates output ids for a batch of image features.\n",
    "        These output ids can then be decoded by the tokenizer to get the generated sentences.\n",
    "        \"\"\"\n",
    "        batch_size = image_hidden_states.size(0)\n",
    "\n",
    "        # start with the bos_token_id for all image features in the batch.\n",
    "        input_ids = torch.full(size=(batch_size, 1), fill_value=self.bos_token_id, dtype=torch.int64, device=self.device)\n",
    "        model_kwargs = {\"attention_mask\": torch.ones(size=(batch_size, 1), dtype=torch.int64, device=self.device),\n",
    "                        \"use_cache\": True}\n",
    "\n",
    "        is_greedy_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is False\n",
    "        is_sample_gen_mode = (num_beams == 1) and (num_beam_groups == 1) and do_sample is True\n",
    "        is_beam_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is False\n",
    "        is_beam_sample_gen_mode = (num_beams > 1) and (num_beam_groups == 1) and do_sample is True\n",
    "        is_group_beam_gen_mode = (num_beams > 1) and (num_beam_groups > 1)\n",
    "\n",
    "        if num_beam_groups > num_beams:\n",
    "            raise ValueError(\"'num_beam_groups' has to be smaller or equal to 'num_beams'\")\n",
    "        if is_group_beam_gen_mode and do_sample is True:\n",
    "            raise ValueError(\n",
    "                \"Diverse beam search cannot be used in sampling mode. Make sure that 'do_sample' is set to 'False'.\"\n",
    "            )\n",
    "\n",
    "        # go into different generation modes\n",
    "        if is_greedy_gen_mode:\n",
    "            if num_return_sequences > 1:\n",
    "                raise ValueError(\n",
    "                    f\"num_return_sequences has to be 1, but is {num_return_sequences} when doing greedy search.\"\n",
    "                )\n",
    "\n",
    "            return self.greedy_search(\n",
    "                input_ids,\n",
    "                image_hidden_states,\n",
    "                max_length,\n",
    "                **model_kwargs\n",
    "            )\n",
    "        elif is_sample_gen_mode:\n",
    "            raise NotImplementedError(\"Multinomial sampling is not implemented.\")\n",
    "        elif is_beam_gen_mode:\n",
    "            if num_return_sequences > num_beams:\n",
    "                raise ValueError(\"'num_return_sequences' has to be smaller or equal to 'num_beams'.\")\n",
    "\n",
    "            if max_length is None:\n",
    "                raise ValueError(\"max_length has to be set for beam generation.\")\n",
    "\n",
    "            beam_scorer = BeamSearchScorer(\n",
    "                batch_size=batch_size,\n",
    "                num_beams=num_beams,\n",
    "                device=self.device,\n",
    "                length_penalty=1.0,  # length_penalty > 0.0 encourages the model to generate shorter sequences\n",
    "                do_early_stopping=early_stopping,\n",
    "                num_beam_hyps_to_keep=num_return_sequences,\n",
    "            )\n",
    "\n",
    "            # interleave input_ids with 'num_beams' additional sequences per batch\n",
    "            input_ids, model_kwargs = self._expand_inputs_for_generation(input_ids, expand_size=num_beams, **model_kwargs)\n",
    "\n",
    "            return self.beam_search(\n",
    "                input_ids,\n",
    "                image_hidden_states,\n",
    "                max_length,\n",
    "                beam_scorer,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "        elif is_beam_sample_gen_mode:\n",
    "            raise NotImplementedError(\"Beam-search multinomial sampling is not implemented.\")\n",
    "        elif is_group_beam_gen_mode:\n",
    "            raise NotImplementedError(\"Diverse beam-search decoding is not implemented.\")\n",
    "\n",
    "    def _expand_inputs_for_generation(self, input_ids, expand_size, attention_mask, **model_kwargs):\n",
    "        expanded_return_idx = (\n",
    "            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)\n",
    "        )\n",
    "        input_ids = input_ids.index_select(0, expanded_return_idx)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            model_kwargs[\"attention_mask\"] = attention_mask.index_select(0, expanded_return_idx)\n",
    "\n",
    "        return input_ids, model_kwargs\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        # only use last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def _update_model_kwargs_for_generation(self, presents, model_kwargs):\n",
    "        model_kwargs[\"past\"] = presents\n",
    "        attention_mask = model_kwargs[\"attention_mask\"]\n",
    "        model_kwargs[\"attention_mask\"] = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "\n",
    "        return model_kwargs\n",
    "\n",
    "    def beam_search(self,\n",
    "                    input_ids,\n",
    "                    image_hidden_states,\n",
    "                    max_length,\n",
    "                    beam_scorer,\n",
    "                    **model_kwargs):\n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "\n",
    "        if num_beams * batch_size != batch_beam_size:\n",
    "            raise ValueError(\n",
    "                f\"Batch dimension of 'input_ids' should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "            )\n",
    "\n",
    "        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while True:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            lm_logits, presents = self.forward(**model_inputs, image_hidden_states=image_hidden_states, return_loss=False)\n",
    "\n",
    "            next_token_logits = lm_logits[:, -1, :]\n",
    "\n",
    "            next_token_scores = nn.functional.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "            next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)\n",
    "\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "\n",
    "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=self.pad_token_id,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                # beam_indices=None,\n",
    "            )\n",
    "\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(presents, model_kwargs)\n",
    "\n",
    "            if model_kwargs[\"past\"] is not None:\n",
    "                model_kwargs[\"past\"] = self._reorder_cache(model_kwargs[\"past\"], beam_idx)\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len += 1\n",
    "\n",
    "            if beam_scorer.is_done or (max_length and cur_len >= max_length):\n",
    "                break\n",
    "\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            eos_token_id=self.eos_token_id,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        return sequence_outputs[\"sequences\"]\n",
    "\n",
    "    def greedy_search(self,\n",
    "                      input_ids,  # shape [batch_size x seq_len]\n",
    "                      image_hidden_states,  # shape [batch_size x image_hidden_dim]\n",
    "                      max_length,\n",
    "                      **model_kwargs\n",
    "                      ) -> torch.LongTensor:  # shape [batch_size x longest_generated_sequence_length]\n",
    "        batch_size = input_ids.size(0)\n",
    "        seq_len = input_ids.size(1)\n",
    "\n",
    "        # keep track of which sequences are already finished\n",
    "        # a 1 denotes that a sentence in a batch is unfinished, a 0 denotes that a sentence has finished\n",
    "        # finished sentences are padded until all sentences in the batch are finished\n",
    "        unfinished_sequences = torch.ones(size=(batch_size,), dtype=torch.int64, device=self.device)\n",
    "        cur_len = seq_len\n",
    "\n",
    "        while True:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "            lm_logits, presents = self.forward(**model_inputs, image_hidden_states=image_hidden_states, return_loss=False)\n",
    "\n",
    "            next_token_logits = lm_logits[:, -1, :]  # of shape [batch_size x vocab_size]\n",
    "\n",
    "            # no need to convert logits into probabilities first (via softmax), argmax can be directly applied to logits\n",
    "            next_tokens = torch.argmax(next_token_logits, dim=-1)  # of shape [batch_size]\n",
    "\n",
    "            # convert next token to padding token if given sentence has already finished (denoted by a 0 in unfinished_sequences)\n",
    "            # padding tokens are ignored when decoding, if skip_special_tokens=True is set\n",
    "            next_tokens = next_tokens * unfinished_sequences + self.pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "            # update variables for next step\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(presents, model_kwargs)\n",
    "            cur_len += 1\n",
    "\n",
    "            # if eos_token was found in one sentence, set sentence to finished (by converting 1 to 0 for that sentence)\n",
    "            binary_mask = (next_tokens != self.eos_token_id).long()\n",
    "            unfinished_sequences = unfinished_sequences.mul(binary_mask)\n",
    "\n",
    "            # stop when all sentences are finished (i.e. all sentences have value 0 in unfinished_sequences),\n",
    "            # or if we exceed the maximum length\n",
    "            if unfinished_sequences.max() == 0 or (max_length and cur_len >= max_length):\n",
    "                break\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "def print_model_summary(batch_size, seq_len, verbose):\n",
    "    \"\"\"\n",
    "    Choose between:\n",
    "        verbose = 0 (only model params)\n",
    "        verbose = 1 (model params and output shape of batch)\n",
    "        verbose = 2 (model params and output shape of batch, more detailed)\n",
    "    \"\"\"\n",
    "    inputs = {}\n",
    "    inputs[\"input_ids\"] = torch.randint(low=0, high=50257, size=(batch_size, seq_len))\n",
    "    inputs[\"attention_mask\"] = torch.randint(low=0, high=2, size=(batch_size, seq_len))\n",
    "    inputs[\"image_hidden_states\"] = torch.rand(batch_size, (2048 * 8 * 8))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LanguageModel()\n",
    "    model.to(device, non_blocking=True)\n",
    "\n",
    "    inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "    if verbose == 0:\n",
    "        summary(model)\n",
    "    else:\n",
    "        summary(model, input_data=dict(inputs), verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "21e32115-4fbe-48a3-b138-d3388e7ec75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ccadf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "10501636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seclab\\anaconda3\\envs\\rgrg_env\\lib\\site-packages\\transformers\\modeling_utils.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['language_model.gpt.wte.weight', 'language_model.gpt.wpe.weight', 'language_model.gpt.h.0.ln_1.weight', 'language_model.gpt.h.0.ln_1.bias', 'language_model.gpt.h.0.attn.causal_mask', 'language_model.gpt.h.0.attn.mask_out_value', 'language_model.gpt.h.0.attn.c_attn.weight', 'language_model.gpt.h.0.attn.c_attn.bias', 'language_model.gpt.h.0.attn.c_proj.weight', 'language_model.gpt.h.0.attn.c_proj.bias', 'language_model.gpt.h.0.attn.uk.weight', 'language_model.gpt.h.0.attn.uk.bias', 'language_model.gpt.h.0.attn.uv.weight', 'language_model.gpt.h.0.attn.uv.bias', 'language_model.gpt.h.0.ln_2.weight', 'language_model.gpt.h.0.ln_2.bias', 'language_model.gpt.h.0.mlp.c_fc.weight', 'language_model.gpt.h.0.mlp.c_fc.bias', 'language_model.gpt.h.0.mlp.c_proj.weight', 'language_model.gpt.h.0.mlp.c_proj.bias', 'language_model.gpt.h.1.ln_1.weight', 'language_model.gpt.h.1.ln_1.bias', 'language_model.gpt.h.1.attn.causal_mask', 'language_model.gpt.h.1.attn.mask_out_value', 'language_model.gpt.h.1.attn.c_attn.weight', 'language_model.gpt.h.1.attn.c_attn.bias', 'language_model.gpt.h.1.attn.c_proj.weight', 'language_model.gpt.h.1.attn.c_proj.bias', 'language_model.gpt.h.1.attn.uk.weight', 'language_model.gpt.h.1.attn.uk.bias', 'language_model.gpt.h.1.attn.uv.weight', 'language_model.gpt.h.1.attn.uv.bias', 'language_model.gpt.h.1.ln_2.weight', 'language_model.gpt.h.1.ln_2.bias', 'language_model.gpt.h.1.mlp.c_fc.weight', 'language_model.gpt.h.1.mlp.c_fc.bias', 'language_model.gpt.h.1.mlp.c_proj.weight', 'language_model.gpt.h.1.mlp.c_proj.bias', 'language_model.gpt.h.2.ln_1.weight', 'language_model.gpt.h.2.ln_1.bias', 'language_model.gpt.h.2.attn.causal_mask', 'language_model.gpt.h.2.attn.mask_out_value', 'language_model.gpt.h.2.attn.c_attn.weight', 'language_model.gpt.h.2.attn.c_attn.bias', 'language_model.gpt.h.2.attn.c_proj.weight', 'language_model.gpt.h.2.attn.c_proj.bias', 'language_model.gpt.h.2.attn.uk.weight', 'language_model.gpt.h.2.attn.uk.bias', 'language_model.gpt.h.2.attn.uv.weight', 'language_model.gpt.h.2.attn.uv.bias', 'language_model.gpt.h.2.ln_2.weight', 'language_model.gpt.h.2.ln_2.bias', 'language_model.gpt.h.2.mlp.c_fc.weight', 'language_model.gpt.h.2.mlp.c_fc.bias', 'language_model.gpt.h.2.mlp.c_proj.weight', 'language_model.gpt.h.2.mlp.c_proj.bias', 'language_model.gpt.h.3.ln_1.weight', 'language_model.gpt.h.3.ln_1.bias', 'language_model.gpt.h.3.attn.causal_mask', 'language_model.gpt.h.3.attn.mask_out_value', 'language_model.gpt.h.3.attn.c_attn.weight', 'language_model.gpt.h.3.attn.c_attn.bias', 'language_model.gpt.h.3.attn.c_proj.weight', 'language_model.gpt.h.3.attn.c_proj.bias', 'language_model.gpt.h.3.attn.uk.weight', 'language_model.gpt.h.3.attn.uk.bias', 'language_model.gpt.h.3.attn.uv.weight', 'language_model.gpt.h.3.attn.uv.bias', 'language_model.gpt.h.3.ln_2.weight', 'language_model.gpt.h.3.ln_2.bias', 'language_model.gpt.h.3.mlp.c_fc.weight', 'language_model.gpt.h.3.mlp.c_fc.bias', 'language_model.gpt.h.3.mlp.c_proj.weight', 'language_model.gpt.h.3.mlp.c_proj.bias', 'language_model.gpt.h.4.ln_1.weight', 'language_model.gpt.h.4.ln_1.bias', 'language_model.gpt.h.4.attn.causal_mask', 'language_model.gpt.h.4.attn.mask_out_value', 'language_model.gpt.h.4.attn.c_attn.weight', 'language_model.gpt.h.4.attn.c_attn.bias', 'language_model.gpt.h.4.attn.c_proj.weight', 'language_model.gpt.h.4.attn.c_proj.bias', 'language_model.gpt.h.4.attn.uk.weight', 'language_model.gpt.h.4.attn.uk.bias', 'language_model.gpt.h.4.attn.uv.weight', 'language_model.gpt.h.4.attn.uv.bias', 'language_model.gpt.h.4.ln_2.weight', 'language_model.gpt.h.4.ln_2.bias', 'language_model.gpt.h.4.mlp.c_fc.weight', 'language_model.gpt.h.4.mlp.c_fc.bias', 'language_model.gpt.h.4.mlp.c_proj.weight', 'language_model.gpt.h.4.mlp.c_proj.bias', 'language_model.gpt.h.5.ln_1.weight', 'language_model.gpt.h.5.ln_1.bias', 'language_model.gpt.h.5.attn.causal_mask', 'language_model.gpt.h.5.attn.mask_out_value', 'language_model.gpt.h.5.attn.c_attn.weight', 'language_model.gpt.h.5.attn.c_attn.bias', 'language_model.gpt.h.5.attn.c_proj.weight', 'language_model.gpt.h.5.attn.c_proj.bias', 'language_model.gpt.h.5.attn.uk.weight', 'language_model.gpt.h.5.attn.uk.bias', 'language_model.gpt.h.5.attn.uv.weight', 'language_model.gpt.h.5.attn.uv.bias', 'language_model.gpt.h.5.ln_2.weight', 'language_model.gpt.h.5.ln_2.bias', 'language_model.gpt.h.5.mlp.c_fc.weight', 'language_model.gpt.h.5.mlp.c_fc.bias', 'language_model.gpt.h.5.mlp.c_proj.weight', 'language_model.gpt.h.5.mlp.c_proj.bias', 'language_model.gpt.h.6.ln_1.weight', 'language_model.gpt.h.6.ln_1.bias', 'language_model.gpt.h.6.attn.causal_mask', 'language_model.gpt.h.6.attn.mask_out_value', 'language_model.gpt.h.6.attn.c_attn.weight', 'language_model.gpt.h.6.attn.c_attn.bias', 'language_model.gpt.h.6.attn.c_proj.weight', 'language_model.gpt.h.6.attn.c_proj.bias', 'language_model.gpt.h.6.attn.uk.weight', 'language_model.gpt.h.6.attn.uk.bias', 'language_model.gpt.h.6.attn.uv.weight', 'language_model.gpt.h.6.attn.uv.bias', 'language_model.gpt.h.6.ln_2.weight', 'language_model.gpt.h.6.ln_2.bias', 'language_model.gpt.h.6.mlp.c_fc.weight', 'language_model.gpt.h.6.mlp.c_fc.bias', 'language_model.gpt.h.6.mlp.c_proj.weight', 'language_model.gpt.h.6.mlp.c_proj.bias', 'language_model.gpt.h.7.ln_1.weight', 'language_model.gpt.h.7.ln_1.bias', 'language_model.gpt.h.7.attn.causal_mask', 'language_model.gpt.h.7.attn.mask_out_value', 'language_model.gpt.h.7.attn.c_attn.weight', 'language_model.gpt.h.7.attn.c_attn.bias', 'language_model.gpt.h.7.attn.c_proj.weight', 'language_model.gpt.h.7.attn.c_proj.bias', 'language_model.gpt.h.7.attn.uk.weight', 'language_model.gpt.h.7.attn.uk.bias', 'language_model.gpt.h.7.attn.uv.weight', 'language_model.gpt.h.7.attn.uv.bias', 'language_model.gpt.h.7.ln_2.weight', 'language_model.gpt.h.7.ln_2.bias', 'language_model.gpt.h.7.mlp.c_fc.weight', 'language_model.gpt.h.7.mlp.c_fc.bias', 'language_model.gpt.h.7.mlp.c_proj.weight', 'language_model.gpt.h.7.mlp.c_proj.bias', 'language_model.gpt.h.8.ln_1.weight', 'language_model.gpt.h.8.ln_1.bias', 'language_model.gpt.h.8.attn.causal_mask', 'language_model.gpt.h.8.attn.mask_out_value', 'language_model.gpt.h.8.attn.c_attn.weight', 'language_model.gpt.h.8.attn.c_attn.bias', 'language_model.gpt.h.8.attn.c_proj.weight', 'language_model.gpt.h.8.attn.c_proj.bias', 'language_model.gpt.h.8.attn.uk.weight', 'language_model.gpt.h.8.attn.uk.bias', 'language_model.gpt.h.8.attn.uv.weight', 'language_model.gpt.h.8.attn.uv.bias', 'language_model.gpt.h.8.ln_2.weight', 'language_model.gpt.h.8.ln_2.bias', 'language_model.gpt.h.8.mlp.c_fc.weight', 'language_model.gpt.h.8.mlp.c_fc.bias', 'language_model.gpt.h.8.mlp.c_proj.weight', 'language_model.gpt.h.8.mlp.c_proj.bias', 'language_model.gpt.h.9.ln_1.weight', 'language_model.gpt.h.9.ln_1.bias', 'language_model.gpt.h.9.attn.causal_mask', 'language_model.gpt.h.9.attn.mask_out_value', 'language_model.gpt.h.9.attn.c_attn.weight', 'language_model.gpt.h.9.attn.c_attn.bias', 'language_model.gpt.h.9.attn.c_proj.weight', 'language_model.gpt.h.9.attn.c_proj.bias', 'language_model.gpt.h.9.attn.uk.weight', 'language_model.gpt.h.9.attn.uk.bias', 'language_model.gpt.h.9.attn.uv.weight', 'language_model.gpt.h.9.attn.uv.bias', 'language_model.gpt.h.9.ln_2.weight', 'language_model.gpt.h.9.ln_2.bias', 'language_model.gpt.h.9.mlp.c_fc.weight', 'language_model.gpt.h.9.mlp.c_fc.bias', 'language_model.gpt.h.9.mlp.c_proj.weight', 'language_model.gpt.h.9.mlp.c_proj.bias', 'language_model.gpt.h.10.ln_1.weight', 'language_model.gpt.h.10.ln_1.bias', 'language_model.gpt.h.10.attn.causal_mask', 'language_model.gpt.h.10.attn.mask_out_value', 'language_model.gpt.h.10.attn.c_attn.weight', 'language_model.gpt.h.10.attn.c_attn.bias', 'language_model.gpt.h.10.attn.c_proj.weight', 'language_model.gpt.h.10.attn.c_proj.bias', 'language_model.gpt.h.10.attn.uk.weight', 'language_model.gpt.h.10.attn.uk.bias', 'language_model.gpt.h.10.attn.uv.weight', 'language_model.gpt.h.10.attn.uv.bias', 'language_model.gpt.h.10.ln_2.weight', 'language_model.gpt.h.10.ln_2.bias', 'language_model.gpt.h.10.mlp.c_fc.weight', 'language_model.gpt.h.10.mlp.c_fc.bias', 'language_model.gpt.h.10.mlp.c_proj.weight', 'language_model.gpt.h.10.mlp.c_proj.bias', 'language_model.gpt.h.11.ln_1.weight', 'language_model.gpt.h.11.ln_1.bias', 'language_model.gpt.h.11.attn.causal_mask', 'language_model.gpt.h.11.attn.mask_out_value', 'language_model.gpt.h.11.attn.c_attn.weight', 'language_model.gpt.h.11.attn.c_attn.bias', 'language_model.gpt.h.11.attn.c_proj.weight', 'language_model.gpt.h.11.attn.c_proj.bias', 'language_model.gpt.h.11.attn.uk.weight', 'language_model.gpt.h.11.attn.uk.bias', 'language_model.gpt.h.11.attn.uv.weight', 'language_model.gpt.h.11.attn.uv.bias', 'language_model.gpt.h.11.ln_2.weight', 'language_model.gpt.h.11.ln_2.bias', 'language_model.gpt.h.11.mlp.c_fc.weight', 'language_model.gpt.h.11.mlp.c_fc.bias', 'language_model.gpt.h.11.mlp.c_proj.weight', 'language_model.gpt.h.11.mlp.c_proj.bias', 'language_model.gpt.h.12.ln_1.weight', 'language_model.gpt.h.12.ln_1.bias', 'language_model.gpt.h.12.attn.causal_mask', 'language_model.gpt.h.12.attn.mask_out_value', 'language_model.gpt.h.12.attn.c_attn.weight', 'language_model.gpt.h.12.attn.c_attn.bias', 'language_model.gpt.h.12.attn.c_proj.weight', 'language_model.gpt.h.12.attn.c_proj.bias', 'language_model.gpt.h.12.attn.uk.weight', 'language_model.gpt.h.12.attn.uk.bias', 'language_model.gpt.h.12.attn.uv.weight', 'language_model.gpt.h.12.attn.uv.bias', 'language_model.gpt.h.12.ln_2.weight', 'language_model.gpt.h.12.ln_2.bias', 'language_model.gpt.h.12.mlp.c_fc.weight', 'language_model.gpt.h.12.mlp.c_fc.bias', 'language_model.gpt.h.12.mlp.c_proj.weight', 'language_model.gpt.h.12.mlp.c_proj.bias', 'language_model.gpt.h.13.ln_1.weight', 'language_model.gpt.h.13.ln_1.bias', 'language_model.gpt.h.13.attn.causal_mask', 'language_model.gpt.h.13.attn.mask_out_value', 'language_model.gpt.h.13.attn.c_attn.weight', 'language_model.gpt.h.13.attn.c_attn.bias', 'language_model.gpt.h.13.attn.c_proj.weight', 'language_model.gpt.h.13.attn.c_proj.bias', 'language_model.gpt.h.13.attn.uk.weight', 'language_model.gpt.h.13.attn.uk.bias', 'language_model.gpt.h.13.attn.uv.weight', 'language_model.gpt.h.13.attn.uv.bias', 'language_model.gpt.h.13.ln_2.weight', 'language_model.gpt.h.13.ln_2.bias', 'language_model.gpt.h.13.mlp.c_fc.weight', 'language_model.gpt.h.13.mlp.c_fc.bias', 'language_model.gpt.h.13.mlp.c_proj.weight', 'language_model.gpt.h.13.mlp.c_proj.bias', 'language_model.gpt.h.14.ln_1.weight', 'language_model.gpt.h.14.ln_1.bias', 'language_model.gpt.h.14.attn.causal_mask', 'language_model.gpt.h.14.attn.mask_out_value', 'language_model.gpt.h.14.attn.c_attn.weight', 'language_model.gpt.h.14.attn.c_attn.bias', 'language_model.gpt.h.14.attn.c_proj.weight', 'language_model.gpt.h.14.attn.c_proj.bias', 'language_model.gpt.h.14.attn.uk.weight', 'language_model.gpt.h.14.attn.uk.bias', 'language_model.gpt.h.14.attn.uv.weight', 'language_model.gpt.h.14.attn.uv.bias', 'language_model.gpt.h.14.ln_2.weight', 'language_model.gpt.h.14.ln_2.bias', 'language_model.gpt.h.14.mlp.c_fc.weight', 'language_model.gpt.h.14.mlp.c_fc.bias', 'language_model.gpt.h.14.mlp.c_proj.weight', 'language_model.gpt.h.14.mlp.c_proj.bias', 'language_model.gpt.h.15.ln_1.weight', 'language_model.gpt.h.15.ln_1.bias', 'language_model.gpt.h.15.attn.causal_mask', 'language_model.gpt.h.15.attn.mask_out_value', 'language_model.gpt.h.15.attn.c_attn.weight', 'language_model.gpt.h.15.attn.c_attn.bias', 'language_model.gpt.h.15.attn.c_proj.weight', 'language_model.gpt.h.15.attn.c_proj.bias', 'language_model.gpt.h.15.attn.uk.weight', 'language_model.gpt.h.15.attn.uk.bias', 'language_model.gpt.h.15.attn.uv.weight', 'language_model.gpt.h.15.attn.uv.bias', 'language_model.gpt.h.15.ln_2.weight', 'language_model.gpt.h.15.ln_2.bias', 'language_model.gpt.h.15.mlp.c_fc.weight', 'language_model.gpt.h.15.mlp.c_fc.bias', 'language_model.gpt.h.15.mlp.c_proj.weight', 'language_model.gpt.h.15.mlp.c_proj.bias', 'language_model.gpt.h.16.ln_1.weight', 'language_model.gpt.h.16.ln_1.bias', 'language_model.gpt.h.16.attn.causal_mask', 'language_model.gpt.h.16.attn.mask_out_value', 'language_model.gpt.h.16.attn.c_attn.weight', 'language_model.gpt.h.16.attn.c_attn.bias', 'language_model.gpt.h.16.attn.c_proj.weight', 'language_model.gpt.h.16.attn.c_proj.bias', 'language_model.gpt.h.16.attn.uk.weight', 'language_model.gpt.h.16.attn.uk.bias', 'language_model.gpt.h.16.attn.uv.weight', 'language_model.gpt.h.16.attn.uv.bias', 'language_model.gpt.h.16.ln_2.weight', 'language_model.gpt.h.16.ln_2.bias', 'language_model.gpt.h.16.mlp.c_fc.weight', 'language_model.gpt.h.16.mlp.c_fc.bias', 'language_model.gpt.h.16.mlp.c_proj.weight', 'language_model.gpt.h.16.mlp.c_proj.bias', 'language_model.gpt.h.17.ln_1.weight', 'language_model.gpt.h.17.ln_1.bias', 'language_model.gpt.h.17.attn.causal_mask', 'language_model.gpt.h.17.attn.mask_out_value', 'language_model.gpt.h.17.attn.c_attn.weight', 'language_model.gpt.h.17.attn.c_attn.bias', 'language_model.gpt.h.17.attn.c_proj.weight', 'language_model.gpt.h.17.attn.c_proj.bias', 'language_model.gpt.h.17.attn.uk.weight', 'language_model.gpt.h.17.attn.uk.bias', 'language_model.gpt.h.17.attn.uv.weight', 'language_model.gpt.h.17.attn.uv.bias', 'language_model.gpt.h.17.ln_2.weight', 'language_model.gpt.h.17.ln_2.bias', 'language_model.gpt.h.17.mlp.c_fc.weight', 'language_model.gpt.h.17.mlp.c_fc.bias', 'language_model.gpt.h.17.mlp.c_proj.weight', 'language_model.gpt.h.17.mlp.c_proj.bias', 'language_model.gpt.h.18.ln_1.weight', 'language_model.gpt.h.18.ln_1.bias', 'language_model.gpt.h.18.attn.causal_mask', 'language_model.gpt.h.18.attn.mask_out_value', 'language_model.gpt.h.18.attn.c_attn.weight', 'language_model.gpt.h.18.attn.c_attn.bias', 'language_model.gpt.h.18.attn.c_proj.weight', 'language_model.gpt.h.18.attn.c_proj.bias', 'language_model.gpt.h.18.attn.uk.weight', 'language_model.gpt.h.18.attn.uk.bias', 'language_model.gpt.h.18.attn.uv.weight', 'language_model.gpt.h.18.attn.uv.bias', 'language_model.gpt.h.18.ln_2.weight', 'language_model.gpt.h.18.ln_2.bias', 'language_model.gpt.h.18.mlp.c_fc.weight', 'language_model.gpt.h.18.mlp.c_fc.bias', 'language_model.gpt.h.18.mlp.c_proj.weight', 'language_model.gpt.h.18.mlp.c_proj.bias', 'language_model.gpt.h.19.ln_1.weight', 'language_model.gpt.h.19.ln_1.bias', 'language_model.gpt.h.19.attn.causal_mask', 'language_model.gpt.h.19.attn.mask_out_value', 'language_model.gpt.h.19.attn.c_attn.weight', 'language_model.gpt.h.19.attn.c_attn.bias', 'language_model.gpt.h.19.attn.c_proj.weight', 'language_model.gpt.h.19.attn.c_proj.bias', 'language_model.gpt.h.19.attn.uk.weight', 'language_model.gpt.h.19.attn.uk.bias', 'language_model.gpt.h.19.attn.uv.weight', 'language_model.gpt.h.19.attn.uv.bias', 'language_model.gpt.h.19.ln_2.weight', 'language_model.gpt.h.19.ln_2.bias', 'language_model.gpt.h.19.mlp.c_fc.weight', 'language_model.gpt.h.19.mlp.c_fc.bias', 'language_model.gpt.h.19.mlp.c_proj.weight', 'language_model.gpt.h.19.mlp.c_proj.bias', 'language_model.gpt.h.20.ln_1.weight', 'language_model.gpt.h.20.ln_1.bias', 'language_model.gpt.h.20.attn.causal_mask', 'language_model.gpt.h.20.attn.mask_out_value', 'language_model.gpt.h.20.attn.c_attn.weight', 'language_model.gpt.h.20.attn.c_attn.bias', 'language_model.gpt.h.20.attn.c_proj.weight', 'language_model.gpt.h.20.attn.c_proj.bias', 'language_model.gpt.h.20.attn.uk.weight', 'language_model.gpt.h.20.attn.uk.bias', 'language_model.gpt.h.20.attn.uv.weight', 'language_model.gpt.h.20.attn.uv.bias', 'language_model.gpt.h.20.ln_2.weight', 'language_model.gpt.h.20.ln_2.bias', 'language_model.gpt.h.20.mlp.c_fc.weight', 'language_model.gpt.h.20.mlp.c_fc.bias', 'language_model.gpt.h.20.mlp.c_proj.weight', 'language_model.gpt.h.20.mlp.c_proj.bias', 'language_model.gpt.h.21.ln_1.weight', 'language_model.gpt.h.21.ln_1.bias', 'language_model.gpt.h.21.attn.causal_mask', 'language_model.gpt.h.21.attn.mask_out_value', 'language_model.gpt.h.21.attn.c_attn.weight', 'language_model.gpt.h.21.attn.c_attn.bias', 'language_model.gpt.h.21.attn.c_proj.weight', 'language_model.gpt.h.21.attn.c_proj.bias', 'language_model.gpt.h.21.attn.uk.weight', 'language_model.gpt.h.21.attn.uk.bias', 'language_model.gpt.h.21.attn.uv.weight', 'language_model.gpt.h.21.attn.uv.bias', 'language_model.gpt.h.21.ln_2.weight', 'language_model.gpt.h.21.ln_2.bias', 'language_model.gpt.h.21.mlp.c_fc.weight', 'language_model.gpt.h.21.mlp.c_fc.bias', 'language_model.gpt.h.21.mlp.c_proj.weight', 'language_model.gpt.h.21.mlp.c_proj.bias', 'language_model.gpt.h.22.ln_1.weight', 'language_model.gpt.h.22.ln_1.bias', 'language_model.gpt.h.22.attn.causal_mask', 'language_model.gpt.h.22.attn.mask_out_value', 'language_model.gpt.h.22.attn.c_attn.weight', 'language_model.gpt.h.22.attn.c_attn.bias', 'language_model.gpt.h.22.attn.c_proj.weight', 'language_model.gpt.h.22.attn.c_proj.bias', 'language_model.gpt.h.22.attn.uk.weight', 'language_model.gpt.h.22.attn.uk.bias', 'language_model.gpt.h.22.attn.uv.weight', 'language_model.gpt.h.22.attn.uv.bias', 'language_model.gpt.h.22.ln_2.weight', 'language_model.gpt.h.22.ln_2.bias', 'language_model.gpt.h.22.mlp.c_fc.weight', 'language_model.gpt.h.22.mlp.c_fc.bias', 'language_model.gpt.h.22.mlp.c_proj.weight', 'language_model.gpt.h.22.mlp.c_proj.bias', 'language_model.gpt.h.23.ln_1.weight', 'language_model.gpt.h.23.ln_1.bias', 'language_model.gpt.h.23.attn.causal_mask', 'language_model.gpt.h.23.attn.mask_out_value', 'language_model.gpt.h.23.attn.c_attn.weight', 'language_model.gpt.h.23.attn.c_attn.bias', 'language_model.gpt.h.23.attn.c_proj.weight', 'language_model.gpt.h.23.attn.c_proj.bias', 'language_model.gpt.h.23.attn.uk.weight', 'language_model.gpt.h.23.attn.uk.bias', 'language_model.gpt.h.23.attn.uv.weight', 'language_model.gpt.h.23.attn.uv.bias', 'language_model.gpt.h.23.ln_2.weight', 'language_model.gpt.h.23.ln_2.bias', 'language_model.gpt.h.23.mlp.c_fc.weight', 'language_model.gpt.h.23.mlp.c_fc.bias', 'language_model.gpt.h.23.mlp.c_proj.weight', 'language_model.gpt.h.23.mlp.c_proj.bias', 'language_model.gpt.ln_f.weight', 'language_model.gpt.ln_f.bias', 'language_model.lm_head.weight', 'language_model.wte.weight', 'language_model.wpe.weight', 'language_model.gpt2_blocks.0.0.weight', 'language_model.gpt2_blocks.0.0.bias', 'language_model.gpt2_blocks.0.1.causal_mask', 'language_model.gpt2_blocks.0.1.mask_out_value', 'language_model.gpt2_blocks.0.1.c_attn.weight', 'language_model.gpt2_blocks.0.1.c_attn.bias', 'language_model.gpt2_blocks.0.1.c_proj.weight', 'language_model.gpt2_blocks.0.1.c_proj.bias', 'language_model.gpt2_blocks.0.1.uk.weight', 'language_model.gpt2_blocks.0.1.uk.bias', 'language_model.gpt2_blocks.0.1.uv.weight', 'language_model.gpt2_blocks.0.1.uv.bias', 'language_model.gpt2_blocks.0.2.weight', 'language_model.gpt2_blocks.0.2.bias', 'language_model.gpt2_blocks.0.3.c_fc.weight', 'language_model.gpt2_blocks.0.3.c_fc.bias', 'language_model.gpt2_blocks.0.3.c_proj.weight', 'language_model.gpt2_blocks.0.3.c_proj.bias', 'language_model.gpt2_blocks.1.0.weight', 'language_model.gpt2_blocks.1.0.bias', 'language_model.gpt2_blocks.1.1.causal_mask', 'language_model.gpt2_blocks.1.1.mask_out_value', 'language_model.gpt2_blocks.1.1.c_attn.weight', 'language_model.gpt2_blocks.1.1.c_attn.bias', 'language_model.gpt2_blocks.1.1.c_proj.weight', 'language_model.gpt2_blocks.1.1.c_proj.bias', 'language_model.gpt2_blocks.1.1.uk.weight', 'language_model.gpt2_blocks.1.1.uk.bias', 'language_model.gpt2_blocks.1.1.uv.weight', 'language_model.gpt2_blocks.1.1.uv.bias', 'language_model.gpt2_blocks.1.2.weight', 'language_model.gpt2_blocks.1.2.bias', 'language_model.gpt2_blocks.1.3.c_fc.weight', 'language_model.gpt2_blocks.1.3.c_fc.bias', 'language_model.gpt2_blocks.1.3.c_proj.weight', 'language_model.gpt2_blocks.1.3.c_proj.bias', 'language_model.gpt2_blocks.2.0.weight', 'language_model.gpt2_blocks.2.0.bias', 'language_model.gpt2_blocks.2.1.causal_mask', 'language_model.gpt2_blocks.2.1.mask_out_value', 'language_model.gpt2_blocks.2.1.c_attn.weight', 'language_model.gpt2_blocks.2.1.c_attn.bias', 'language_model.gpt2_blocks.2.1.c_proj.weight', 'language_model.gpt2_blocks.2.1.c_proj.bias', 'language_model.gpt2_blocks.2.1.uk.weight', 'language_model.gpt2_blocks.2.1.uk.bias', 'language_model.gpt2_blocks.2.1.uv.weight', 'language_model.gpt2_blocks.2.1.uv.bias', 'language_model.gpt2_blocks.2.2.weight', 'language_model.gpt2_blocks.2.2.bias', 'language_model.gpt2_blocks.2.3.c_fc.weight', 'language_model.gpt2_blocks.2.3.c_fc.bias', 'language_model.gpt2_blocks.2.3.c_proj.weight', 'language_model.gpt2_blocks.2.3.c_proj.bias', 'language_model.gpt2_blocks.3.0.weight', 'language_model.gpt2_blocks.3.0.bias', 'language_model.gpt2_blocks.3.1.causal_mask', 'language_model.gpt2_blocks.3.1.mask_out_value', 'language_model.gpt2_blocks.3.1.c_attn.weight', 'language_model.gpt2_blocks.3.1.c_attn.bias', 'language_model.gpt2_blocks.3.1.c_proj.weight', 'language_model.gpt2_blocks.3.1.c_proj.bias', 'language_model.gpt2_blocks.3.1.uk.weight', 'language_model.gpt2_blocks.3.1.uk.bias', 'language_model.gpt2_blocks.3.1.uv.weight', 'language_model.gpt2_blocks.3.1.uv.bias', 'language_model.gpt2_blocks.3.2.weight', 'language_model.gpt2_blocks.3.2.bias', 'language_model.gpt2_blocks.3.3.c_fc.weight', 'language_model.gpt2_blocks.3.3.c_fc.bias', 'language_model.gpt2_blocks.3.3.c_proj.weight', 'language_model.gpt2_blocks.3.3.c_proj.bias', 'language_model.gpt2_blocks.4.0.weight', 'language_model.gpt2_blocks.4.0.bias', 'language_model.gpt2_blocks.4.1.causal_mask', 'language_model.gpt2_blocks.4.1.mask_out_value', 'language_model.gpt2_blocks.4.1.c_attn.weight', 'language_model.gpt2_blocks.4.1.c_attn.bias', 'language_model.gpt2_blocks.4.1.c_proj.weight', 'language_model.gpt2_blocks.4.1.c_proj.bias', 'language_model.gpt2_blocks.4.1.uk.weight', 'language_model.gpt2_blocks.4.1.uk.bias', 'language_model.gpt2_blocks.4.1.uv.weight', 'language_model.gpt2_blocks.4.1.uv.bias', 'language_model.gpt2_blocks.4.2.weight', 'language_model.gpt2_blocks.4.2.bias', 'language_model.gpt2_blocks.4.3.c_fc.weight', 'language_model.gpt2_blocks.4.3.c_fc.bias', 'language_model.gpt2_blocks.4.3.c_proj.weight', 'language_model.gpt2_blocks.4.3.c_proj.bias', 'language_model.gpt2_blocks.5.0.weight', 'language_model.gpt2_blocks.5.0.bias', 'language_model.gpt2_blocks.5.1.causal_mask', 'language_model.gpt2_blocks.5.1.mask_out_value', 'language_model.gpt2_blocks.5.1.c_attn.weight', 'language_model.gpt2_blocks.5.1.c_attn.bias', 'language_model.gpt2_blocks.5.1.c_proj.weight', 'language_model.gpt2_blocks.5.1.c_proj.bias', 'language_model.gpt2_blocks.5.1.uk.weight', 'language_model.gpt2_blocks.5.1.uk.bias', 'language_model.gpt2_blocks.5.1.uv.weight', 'language_model.gpt2_blocks.5.1.uv.bias', 'language_model.gpt2_blocks.5.2.weight', 'language_model.gpt2_blocks.5.2.bias', 'language_model.gpt2_blocks.5.3.c_fc.weight', 'language_model.gpt2_blocks.5.3.c_fc.bias', 'language_model.gpt2_blocks.5.3.c_proj.weight', 'language_model.gpt2_blocks.5.3.c_proj.bias', 'language_model.gpt2_blocks.6.0.weight', 'language_model.gpt2_blocks.6.0.bias', 'language_model.gpt2_blocks.6.1.causal_mask', 'language_model.gpt2_blocks.6.1.mask_out_value', 'language_model.gpt2_blocks.6.1.c_attn.weight', 'language_model.gpt2_blocks.6.1.c_attn.bias', 'language_model.gpt2_blocks.6.1.c_proj.weight', 'language_model.gpt2_blocks.6.1.c_proj.bias', 'language_model.gpt2_blocks.6.1.uk.weight', 'language_model.gpt2_blocks.6.1.uk.bias', 'language_model.gpt2_blocks.6.1.uv.weight', 'language_model.gpt2_blocks.6.1.uv.bias', 'language_model.gpt2_blocks.6.2.weight', 'language_model.gpt2_blocks.6.2.bias', 'language_model.gpt2_blocks.6.3.c_fc.weight', 'language_model.gpt2_blocks.6.3.c_fc.bias', 'language_model.gpt2_blocks.6.3.c_proj.weight', 'language_model.gpt2_blocks.6.3.c_proj.bias', 'language_model.gpt2_blocks.7.0.weight', 'language_model.gpt2_blocks.7.0.bias', 'language_model.gpt2_blocks.7.1.causal_mask', 'language_model.gpt2_blocks.7.1.mask_out_value', 'language_model.gpt2_blocks.7.1.c_attn.weight', 'language_model.gpt2_blocks.7.1.c_attn.bias', 'language_model.gpt2_blocks.7.1.c_proj.weight', 'language_model.gpt2_blocks.7.1.c_proj.bias', 'language_model.gpt2_blocks.7.1.uk.weight', 'language_model.gpt2_blocks.7.1.uk.bias', 'language_model.gpt2_blocks.7.1.uv.weight', 'language_model.gpt2_blocks.7.1.uv.bias', 'language_model.gpt2_blocks.7.2.weight', 'language_model.gpt2_blocks.7.2.bias', 'language_model.gpt2_blocks.7.3.c_fc.weight', 'language_model.gpt2_blocks.7.3.c_fc.bias', 'language_model.gpt2_blocks.7.3.c_proj.weight', 'language_model.gpt2_blocks.7.3.c_proj.bias', 'language_model.gpt2_blocks.8.0.weight', 'language_model.gpt2_blocks.8.0.bias', 'language_model.gpt2_blocks.8.1.causal_mask', 'language_model.gpt2_blocks.8.1.mask_out_value', 'language_model.gpt2_blocks.8.1.c_attn.weight', 'language_model.gpt2_blocks.8.1.c_attn.bias', 'language_model.gpt2_blocks.8.1.c_proj.weight', 'language_model.gpt2_blocks.8.1.c_proj.bias', 'language_model.gpt2_blocks.8.1.uk.weight', 'language_model.gpt2_blocks.8.1.uk.bias', 'language_model.gpt2_blocks.8.1.uv.weight', 'language_model.gpt2_blocks.8.1.uv.bias', 'language_model.gpt2_blocks.8.2.weight', 'language_model.gpt2_blocks.8.2.bias', 'language_model.gpt2_blocks.8.3.c_fc.weight', 'language_model.gpt2_blocks.8.3.c_fc.bias', 'language_model.gpt2_blocks.8.3.c_proj.weight', 'language_model.gpt2_blocks.8.3.c_proj.bias', 'language_model.gpt2_blocks.9.0.weight', 'language_model.gpt2_blocks.9.0.bias', 'language_model.gpt2_blocks.9.1.causal_mask', 'language_model.gpt2_blocks.9.1.mask_out_value', 'language_model.gpt2_blocks.9.1.c_attn.weight', 'language_model.gpt2_blocks.9.1.c_attn.bias', 'language_model.gpt2_blocks.9.1.c_proj.weight', 'language_model.gpt2_blocks.9.1.c_proj.bias', 'language_model.gpt2_blocks.9.1.uk.weight', 'language_model.gpt2_blocks.9.1.uk.bias', 'language_model.gpt2_blocks.9.1.uv.weight', 'language_model.gpt2_blocks.9.1.uv.bias', 'language_model.gpt2_blocks.9.2.weight', 'language_model.gpt2_blocks.9.2.bias', 'language_model.gpt2_blocks.9.3.c_fc.weight', 'language_model.gpt2_blocks.9.3.c_fc.bias', 'language_model.gpt2_blocks.9.3.c_proj.weight', 'language_model.gpt2_blocks.9.3.c_proj.bias', 'language_model.gpt2_blocks.10.0.weight', 'language_model.gpt2_blocks.10.0.bias', 'language_model.gpt2_blocks.10.1.causal_mask', 'language_model.gpt2_blocks.10.1.mask_out_value', 'language_model.gpt2_blocks.10.1.c_attn.weight', 'language_model.gpt2_blocks.10.1.c_attn.bias', 'language_model.gpt2_blocks.10.1.c_proj.weight', 'language_model.gpt2_blocks.10.1.c_proj.bias', 'language_model.gpt2_blocks.10.1.uk.weight', 'language_model.gpt2_blocks.10.1.uk.bias', 'language_model.gpt2_blocks.10.1.uv.weight', 'language_model.gpt2_blocks.10.1.uv.bias', 'language_model.gpt2_blocks.10.2.weight', 'language_model.gpt2_blocks.10.2.bias', 'language_model.gpt2_blocks.10.3.c_fc.weight', 'language_model.gpt2_blocks.10.3.c_fc.bias', 'language_model.gpt2_blocks.10.3.c_proj.weight', 'language_model.gpt2_blocks.10.3.c_proj.bias', 'language_model.gpt2_blocks.11.0.weight', 'language_model.gpt2_blocks.11.0.bias', 'language_model.gpt2_blocks.11.1.causal_mask', 'language_model.gpt2_blocks.11.1.mask_out_value', 'language_model.gpt2_blocks.11.1.c_attn.weight', 'language_model.gpt2_blocks.11.1.c_attn.bias', 'language_model.gpt2_blocks.11.1.c_proj.weight', 'language_model.gpt2_blocks.11.1.c_proj.bias', 'language_model.gpt2_blocks.11.1.uk.weight', 'language_model.gpt2_blocks.11.1.uk.bias', 'language_model.gpt2_blocks.11.1.uv.weight', 'language_model.gpt2_blocks.11.1.uv.bias', 'language_model.gpt2_blocks.11.2.weight', 'language_model.gpt2_blocks.11.2.bias', 'language_model.gpt2_blocks.11.3.c_fc.weight', 'language_model.gpt2_blocks.11.3.c_fc.bias', 'language_model.gpt2_blocks.11.3.c_proj.weight', 'language_model.gpt2_blocks.11.3.c_proj.bias', 'language_model.gpt2_blocks.12.0.weight', 'language_model.gpt2_blocks.12.0.bias', 'language_model.gpt2_blocks.12.1.causal_mask', 'language_model.gpt2_blocks.12.1.mask_out_value', 'language_model.gpt2_blocks.12.1.c_attn.weight', 'language_model.gpt2_blocks.12.1.c_attn.bias', 'language_model.gpt2_blocks.12.1.c_proj.weight', 'language_model.gpt2_blocks.12.1.c_proj.bias', 'language_model.gpt2_blocks.12.1.uk.weight', 'language_model.gpt2_blocks.12.1.uk.bias', 'language_model.gpt2_blocks.12.1.uv.weight', 'language_model.gpt2_blocks.12.1.uv.bias', 'language_model.gpt2_blocks.12.2.weight', 'language_model.gpt2_blocks.12.2.bias', 'language_model.gpt2_blocks.12.3.c_fc.weight', 'language_model.gpt2_blocks.12.3.c_fc.bias', 'language_model.gpt2_blocks.12.3.c_proj.weight', 'language_model.gpt2_blocks.12.3.c_proj.bias', 'language_model.gpt2_blocks.13.0.weight', 'language_model.gpt2_blocks.13.0.bias', 'language_model.gpt2_blocks.13.1.causal_mask', 'language_model.gpt2_blocks.13.1.mask_out_value', 'language_model.gpt2_blocks.13.1.c_attn.weight', 'language_model.gpt2_blocks.13.1.c_attn.bias', 'language_model.gpt2_blocks.13.1.c_proj.weight', 'language_model.gpt2_blocks.13.1.c_proj.bias', 'language_model.gpt2_blocks.13.1.uk.weight', 'language_model.gpt2_blocks.13.1.uk.bias', 'language_model.gpt2_blocks.13.1.uv.weight', 'language_model.gpt2_blocks.13.1.uv.bias', 'language_model.gpt2_blocks.13.2.weight', 'language_model.gpt2_blocks.13.2.bias', 'language_model.gpt2_blocks.13.3.c_fc.weight', 'language_model.gpt2_blocks.13.3.c_fc.bias', 'language_model.gpt2_blocks.13.3.c_proj.weight', 'language_model.gpt2_blocks.13.3.c_proj.bias', 'language_model.gpt2_blocks.14.0.weight', 'language_model.gpt2_blocks.14.0.bias', 'language_model.gpt2_blocks.14.1.causal_mask', 'language_model.gpt2_blocks.14.1.mask_out_value', 'language_model.gpt2_blocks.14.1.c_attn.weight', 'language_model.gpt2_blocks.14.1.c_attn.bias', 'language_model.gpt2_blocks.14.1.c_proj.weight', 'language_model.gpt2_blocks.14.1.c_proj.bias', 'language_model.gpt2_blocks.14.1.uk.weight', 'language_model.gpt2_blocks.14.1.uk.bias', 'language_model.gpt2_blocks.14.1.uv.weight', 'language_model.gpt2_blocks.14.1.uv.bias', 'language_model.gpt2_blocks.14.2.weight', 'language_model.gpt2_blocks.14.2.bias', 'language_model.gpt2_blocks.14.3.c_fc.weight', 'language_model.gpt2_blocks.14.3.c_fc.bias', 'language_model.gpt2_blocks.14.3.c_proj.weight', 'language_model.gpt2_blocks.14.3.c_proj.bias', 'language_model.gpt2_blocks.15.0.weight', 'language_model.gpt2_blocks.15.0.bias', 'language_model.gpt2_blocks.15.1.causal_mask', 'language_model.gpt2_blocks.15.1.mask_out_value', 'language_model.gpt2_blocks.15.1.c_attn.weight', 'language_model.gpt2_blocks.15.1.c_attn.bias', 'language_model.gpt2_blocks.15.1.c_proj.weight', 'language_model.gpt2_blocks.15.1.c_proj.bias', 'language_model.gpt2_blocks.15.1.uk.weight', 'language_model.gpt2_blocks.15.1.uk.bias', 'language_model.gpt2_blocks.15.1.uv.weight', 'language_model.gpt2_blocks.15.1.uv.bias', 'language_model.gpt2_blocks.15.2.weight', 'language_model.gpt2_blocks.15.2.bias', 'language_model.gpt2_blocks.15.3.c_fc.weight', 'language_model.gpt2_blocks.15.3.c_fc.bias', 'language_model.gpt2_blocks.15.3.c_proj.weight', 'language_model.gpt2_blocks.15.3.c_proj.bias', 'language_model.gpt2_blocks.16.0.weight', 'language_model.gpt2_blocks.16.0.bias', 'language_model.gpt2_blocks.16.1.causal_mask', 'language_model.gpt2_blocks.16.1.mask_out_value', 'language_model.gpt2_blocks.16.1.c_attn.weight', 'language_model.gpt2_blocks.16.1.c_attn.bias', 'language_model.gpt2_blocks.16.1.c_proj.weight', 'language_model.gpt2_blocks.16.1.c_proj.bias', 'language_model.gpt2_blocks.16.1.uk.weight', 'language_model.gpt2_blocks.16.1.uk.bias', 'language_model.gpt2_blocks.16.1.uv.weight', 'language_model.gpt2_blocks.16.1.uv.bias', 'language_model.gpt2_blocks.16.2.weight', 'language_model.gpt2_blocks.16.2.bias', 'language_model.gpt2_blocks.16.3.c_fc.weight', 'language_model.gpt2_blocks.16.3.c_fc.bias', 'language_model.gpt2_blocks.16.3.c_proj.weight', 'language_model.gpt2_blocks.16.3.c_proj.bias', 'language_model.gpt2_blocks.17.0.weight', 'language_model.gpt2_blocks.17.0.bias', 'language_model.gpt2_blocks.17.1.causal_mask', 'language_model.gpt2_blocks.17.1.mask_out_value', 'language_model.gpt2_blocks.17.1.c_attn.weight', 'language_model.gpt2_blocks.17.1.c_attn.bias', 'language_model.gpt2_blocks.17.1.c_proj.weight', 'language_model.gpt2_blocks.17.1.c_proj.bias', 'language_model.gpt2_blocks.17.1.uk.weight', 'language_model.gpt2_blocks.17.1.uk.bias', 'language_model.gpt2_blocks.17.1.uv.weight', 'language_model.gpt2_blocks.17.1.uv.bias', 'language_model.gpt2_blocks.17.2.weight', 'language_model.gpt2_blocks.17.2.bias', 'language_model.gpt2_blocks.17.3.c_fc.weight', 'language_model.gpt2_blocks.17.3.c_fc.bias', 'language_model.gpt2_blocks.17.3.c_proj.weight', 'language_model.gpt2_blocks.17.3.c_proj.bias', 'language_model.gpt2_blocks.18.0.weight', 'language_model.gpt2_blocks.18.0.bias', 'language_model.gpt2_blocks.18.1.causal_mask', 'language_model.gpt2_blocks.18.1.mask_out_value', 'language_model.gpt2_blocks.18.1.c_attn.weight', 'language_model.gpt2_blocks.18.1.c_attn.bias', 'language_model.gpt2_blocks.18.1.c_proj.weight', 'language_model.gpt2_blocks.18.1.c_proj.bias', 'language_model.gpt2_blocks.18.1.uk.weight', 'language_model.gpt2_blocks.18.1.uk.bias', 'language_model.gpt2_blocks.18.1.uv.weight', 'language_model.gpt2_blocks.18.1.uv.bias', 'language_model.gpt2_blocks.18.2.weight', 'language_model.gpt2_blocks.18.2.bias', 'language_model.gpt2_blocks.18.3.c_fc.weight', 'language_model.gpt2_blocks.18.3.c_fc.bias', 'language_model.gpt2_blocks.18.3.c_proj.weight', 'language_model.gpt2_blocks.18.3.c_proj.bias', 'language_model.gpt2_blocks.19.0.weight', 'language_model.gpt2_blocks.19.0.bias', 'language_model.gpt2_blocks.19.1.causal_mask', 'language_model.gpt2_blocks.19.1.mask_out_value', 'language_model.gpt2_blocks.19.1.c_attn.weight', 'language_model.gpt2_blocks.19.1.c_attn.bias', 'language_model.gpt2_blocks.19.1.c_proj.weight', 'language_model.gpt2_blocks.19.1.c_proj.bias', 'language_model.gpt2_blocks.19.1.uk.weight', 'language_model.gpt2_blocks.19.1.uk.bias', 'language_model.gpt2_blocks.19.1.uv.weight', 'language_model.gpt2_blocks.19.1.uv.bias', 'language_model.gpt2_blocks.19.2.weight', 'language_model.gpt2_blocks.19.2.bias', 'language_model.gpt2_blocks.19.3.c_fc.weight', 'language_model.gpt2_blocks.19.3.c_fc.bias', 'language_model.gpt2_blocks.19.3.c_proj.weight', 'language_model.gpt2_blocks.19.3.c_proj.bias', 'language_model.gpt2_blocks.20.0.weight', 'language_model.gpt2_blocks.20.0.bias', 'language_model.gpt2_blocks.20.1.causal_mask', 'language_model.gpt2_blocks.20.1.mask_out_value', 'language_model.gpt2_blocks.20.1.c_attn.weight', 'language_model.gpt2_blocks.20.1.c_attn.bias', 'language_model.gpt2_blocks.20.1.c_proj.weight', 'language_model.gpt2_blocks.20.1.c_proj.bias', 'language_model.gpt2_blocks.20.1.uk.weight', 'language_model.gpt2_blocks.20.1.uk.bias', 'language_model.gpt2_blocks.20.1.uv.weight', 'language_model.gpt2_blocks.20.1.uv.bias', 'language_model.gpt2_blocks.20.2.weight', 'language_model.gpt2_blocks.20.2.bias', 'language_model.gpt2_blocks.20.3.c_fc.weight', 'language_model.gpt2_blocks.20.3.c_fc.bias', 'language_model.gpt2_blocks.20.3.c_proj.weight', 'language_model.gpt2_blocks.20.3.c_proj.bias', 'language_model.gpt2_blocks.21.0.weight', 'language_model.gpt2_blocks.21.0.bias', 'language_model.gpt2_blocks.21.1.causal_mask', 'language_model.gpt2_blocks.21.1.mask_out_value', 'language_model.gpt2_blocks.21.1.c_attn.weight', 'language_model.gpt2_blocks.21.1.c_attn.bias', 'language_model.gpt2_blocks.21.1.c_proj.weight', 'language_model.gpt2_blocks.21.1.c_proj.bias', 'language_model.gpt2_blocks.21.1.uk.weight', 'language_model.gpt2_blocks.21.1.uk.bias', 'language_model.gpt2_blocks.21.1.uv.weight', 'language_model.gpt2_blocks.21.1.uv.bias', 'language_model.gpt2_blocks.21.2.weight', 'language_model.gpt2_blocks.21.2.bias', 'language_model.gpt2_blocks.21.3.c_fc.weight', 'language_model.gpt2_blocks.21.3.c_fc.bias', 'language_model.gpt2_blocks.21.3.c_proj.weight', 'language_model.gpt2_blocks.21.3.c_proj.bias', 'language_model.gpt2_blocks.22.0.weight', 'language_model.gpt2_blocks.22.0.bias', 'language_model.gpt2_blocks.22.1.causal_mask', 'language_model.gpt2_blocks.22.1.mask_out_value', 'language_model.gpt2_blocks.22.1.c_attn.weight', 'language_model.gpt2_blocks.22.1.c_attn.bias', 'language_model.gpt2_blocks.22.1.c_proj.weight', 'language_model.gpt2_blocks.22.1.c_proj.bias', 'language_model.gpt2_blocks.22.1.uk.weight', 'language_model.gpt2_blocks.22.1.uk.bias', 'language_model.gpt2_blocks.22.1.uv.weight', 'language_model.gpt2_blocks.22.1.uv.bias', 'language_model.gpt2_blocks.22.2.weight', 'language_model.gpt2_blocks.22.2.bias', 'language_model.gpt2_blocks.22.3.c_fc.weight', 'language_model.gpt2_blocks.22.3.c_fc.bias', 'language_model.gpt2_blocks.22.3.c_proj.weight', 'language_model.gpt2_blocks.22.3.c_proj.bias', 'language_model.gpt2_blocks.23.0.weight', 'language_model.gpt2_blocks.23.0.bias', 'language_model.gpt2_blocks.23.1.causal_mask', 'language_model.gpt2_blocks.23.1.mask_out_value', 'language_model.gpt2_blocks.23.1.c_attn.weight', 'language_model.gpt2_blocks.23.1.c_attn.bias', 'language_model.gpt2_blocks.23.1.c_proj.weight', 'language_model.gpt2_blocks.23.1.c_proj.bias', 'language_model.gpt2_blocks.23.1.uk.weight', 'language_model.gpt2_blocks.23.1.uk.bias', 'language_model.gpt2_blocks.23.1.uv.weight', 'language_model.gpt2_blocks.23.1.uv.bias', 'language_model.gpt2_blocks.23.2.weight', 'language_model.gpt2_blocks.23.2.bias', 'language_model.gpt2_blocks.23.3.c_fc.weight', 'language_model.gpt2_blocks.23.3.c_fc.bias', 'language_model.gpt2_blocks.23.3.c_proj.weight', 'language_model.gpt2_blocks.23.3.c_proj.bias', 'language_model.final_layernorm.weight', 'language_model.final_layernorm.bias', 'language_model.feature_space_transformation_nn.0.weight', 'language_model.feature_space_transformation_nn.0.bias', 'language_model.feature_space_transformation_nn.2.weight', 'language_model.feature_space_transformation_nn.2.bias'])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_nlp = LanguageModel().to(device)\n",
    "\n",
    "# Filter and load model weights\n",
    "nlp_state_dict = {\n",
    "    key.replace(\"language_model.gpt_with_lm_head.\", \"\"): checkpoint[\"model\"][key]\n",
    "    for key in cp_nlp  # Only keys in cp_nlp\n",
    "}\n",
    "\n",
    "# Load the filtered state_dict into the model\n",
    "model_nlp.gpt_with_lm_head.load_state_dict(nlp_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "287b250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "# checkpoint에서 필요한 키만 필터링 및 이름 매핑\n",
    "nlp_state_dict = {}\n",
    "for key, value in checkpoint[\"model\"].items():\n",
    "    if key.startswith(\"language_model.gpt_with_lm_head.\"):\n",
    "        new_key = key.replace(\"language_model.gpt_with_lm_head.\", \"\")  # 키 이름 매핑\n",
    "        nlp_state_dict[new_key] = value\n",
    "\n",
    "# 필터링된 state_dict를 로드\n",
    "missing_keys, unexpected_keys = model_nlp.gpt_with_lm_head.load_state_dict(nlp_state_dict, strict=False)\n",
    "\n",
    "# 로드 결과 확인\n",
    "print(f\"Missing keys: {missing_keys}\")\n",
    "print(f\"Unexpected keys: {unexpected_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e1ae7986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (gpt_with_lm_head): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 1024)\n",
       "      (wpe): Embedding(1024, 1024)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x GPT2Block(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2PseudoAttention(\n",
       "            (c_attn): Conv1DWithTrainedWeights()\n",
       "            (c_proj): Conv1DWithTrainedWeights()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (uk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (uv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  )\n",
       "  (gpt): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2PseudoAttention(\n",
       "          (c_attn): Conv1DWithTrainedWeights()\n",
       "          (c_proj): Conv1DWithTrainedWeights()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (uk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (uv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  (wte): Embedding(50257, 1024)\n",
       "  (wpe): Embedding(1024, 1024)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (gpt2_blocks): ModuleList(\n",
       "    (0-23): 24 x ModuleList(\n",
       "      (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): GPT2PseudoAttention(\n",
       "        (c_attn): Conv1DWithTrainedWeights()\n",
       "        (c_proj): Conv1DWithTrainedWeights()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (uk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (uv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (feature_space_transformation_nn): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nlp.eval()\n",
    "# Tokenize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b09a5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_indices = torch.randint(0, 29, (region_features.size(0),))  # [batch_size]\n",
    "# region_features_reduced = region_features[torch.arange(region_features.size(0)), selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1a6ac37-3bb3-408d-b828-e4172c8d3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e23c36ca-a943-460b-a634-1183327117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer():\n",
    "    checkpoint = \"healx/gpt-2-pubmed-medium\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "70e9027d-5efb-4314-9ccb-8908a3ea3b4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfull_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_full_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenized_datasets\n\u001b[1;32m----> 2\u001b[0m tokenized_train_dataset, tokenized_val_dataset \u001b[38;5;241m=\u001b[39m get_tokenized_datasets(tokenizer, \u001b[43mraw_train_dataset\u001b[49m, raw_val_dataset)\n\u001b[0;32m      4\u001b[0m train_transforms \u001b[38;5;241m=\u001b[39m get_transforms(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m val_transforms \u001b[38;5;241m=\u001b[39m get_transforms(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from full_model.train_full_model import get_tokenized_datasets\n",
    "tokenized_train_dataset, tokenized_val_dataset = get_tokenized_datasets(tokenizer, raw_train_dataset, raw_val_dataset)\n",
    "\n",
    "train_transforms = get_transforms(\"train\")\n",
    "val_transforms = get_transforms(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "09197878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 54.52ex/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.97ex/s]\n"
     ]
    }
   ],
   "source": [
    "from full_model.train_full_model import get_tokenized_datasets\n",
    "tokenized_test_dataset, tokenized_test_dataset = get_tokenized_datasets(tokenizer, raw_test_dataset, raw_test_dataset)\n",
    "\n",
    "train_transforms = get_transforms(\"train\")\n",
    "val_transforms = get_transforms(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62557a-1b90-48e6-a13b-c3faf5bc563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.full_model.custom_dataset import CustomDataset\n",
    "\n",
    "train_dataset_complete = CustomDataset(\"train\", tokenized_train_dataset, train_transforms, log)\n",
    "val_dataset_complete = CustomDataset(\"val\", tokenized_val_dataset, val_transforms, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f6a50598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.full_model.custom_dataset import CustomDataset\n",
    "\n",
    "test_dataset_complete = CustomDataset(\"train\", tokenized_test_dataset, train_transforms, log)\n",
    "#val_dataset_complete = CustomDataset(\"val\", tokenized_test_dataset, val_transforms, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4e6395c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[144, 315, 512, 512], [315, 360, 512, 512], [433, 315, 512, 512], [180, 144, 512, 512]]'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset[0][\"bbox_coordinates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f69c7729-be40-4f06-896c-15ac03586f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.full_model.train_full_model import get_data_loaders\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(tokenizer, test_dataset_complete, test_dataset_complete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6e335ee2-65da-44a7-af17-5cb7df15899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[50256,    43,  2150,  ...,  1775,    13, 50256],\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [50256,  1858,   318,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 0,  ..., 0, 0, 0]]), 'images': tensor([[[[0.2064, 0.2194, 0.2324,  ..., 0.6869, 0.6219, 0.6998],\n",
      "          [0.2713, 0.2583, 0.2454,  ..., 0.6609, 0.6609, 0.8167],\n",
      "          [0.3492, 0.2843, 0.2194,  ..., 0.2973, 0.4272, 0.6869],\n",
      "          ...,\n",
      "          [0.2713, 0.2973, 0.3363,  ..., 0.7258, 0.7648, 0.7907],\n",
      "          [0.3103, 0.3363, 0.3752,  ..., 0.7518, 0.7907, 0.8167],\n",
      "          [0.3363, 0.3622, 0.4012,  ..., 0.7648, 0.8037, 0.8297]]]]), 'image_targets': [{'boxes': tensor([[144., 315., 512., 512.],\n",
      "        [315., 360., 512., 512.],\n",
      "        [433., 315., 512., 512.],\n",
      "        [180., 144., 512., 512.]]), 'labels': tensor([ 1,  2,  6, 19])}], 'region_has_sentence': tensor([[ True, False,  True,  True,  True, False,  True, False,  True, False,\n",
      "         False, False,  True, False,  True, False, False, False, False, False,\n",
      "         False, False, False, False,  True, False, False, False, False]]), 'region_is_abnormal': tensor([[ True, False,  True,  True,  True, False, False, False,  True, False,\n",
      "         False, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "for num_batch, batch in tqdm(enumerate(train_loader)):\n",
    "    print(batch)\n",
    "    images = batch[\"images\"]\n",
    "    \n",
    "    image_targets = batch[\"image_targets\"]\n",
    "\n",
    "    region_has_sentence = batch[\"region_has_sentence\"]\n",
    "    region_is_abnormal = batch[\"region_is_abnormal\"]\n",
    "\n",
    "    batch_size = images.size(0)\n",
    "\n",
    "    images = images.to(device, non_blocking=True)\n",
    "    image_targets = [{k: v.to(device, non_blocking=True) for k, v in t.items()} for t in image_targets]\n",
    "    region_has_sentence = region_has_sentence.to(device, non_blocking=True)\n",
    "    region_is_abnormal = region_is_abnormal.to(device, non_blocking=True)\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    input_ids = input_ids.to(device, non_blocking=True)\n",
    "    attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "    #obj_detector_loss_dict, top_region_features, class_detected = self.object_detector(images, image_targets)\n",
    "    class_detected = []\n",
    "   \n",
    "    for target in batch[\"image_targets\"]:\n",
    "        indices = target[\"labels\"]  # Get indices\n",
    "        # Create a one-hot vector for the current indices\n",
    "        one_hot = torch.zeros((len(indices),29), dtype=torch.bool)\n",
    "        one_hot.scatter_(1, indices.unsqueeze(1), True)\n",
    "        class_detected.append(one_hot)\n",
    "    \n",
    "    # Combine all one-hot vectors for the batch\n",
    "    class_detected = torch.cat(class_detected, dim=0)\n",
    "    top_region_features = object_test_f\n",
    "    break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "150df9dc-dcdc-4cb7-80b8-ba90fc92e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_decoder_input_for_training(\n",
    "        class_detected,  # shape [batch_size x 29]\n",
    "        region_has_sentence,  # shape [batch_size x 29]\n",
    "        input_ids,  # shape [(batch_size * 29) x seq_len]\n",
    "        attention_mask,  # shape [(batch_size * 29) x seq_len]\n",
    "        region_features,  # shape [batch_size x 29 x 1024]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We want to train the decoder only on region features (and corresponding input_ids/attention_mask) whose corresponding sentences are non-empty and\n",
    "        that were detected by the object detector.\n",
    "        \"\"\"\n",
    "        # valid is of shape [batch_size x 29]\n",
    "        device = class_detected.device#iadd\n",
    "        region_has_sentence = region_has_sentence.to(device)#i add\n",
    "    \n",
    "        #valid = torch.logical_and(class_detected, region_has_sentence) # youna have to change\n",
    "        valid =  region_has_sentence\n",
    "        # reshape to [(batch_size * 29)], such that we can apply the mask to input_ids and attention_mask\n",
    "        valid_reshaped = valid.reshape(-1)\n",
    "        print(valid.shape)\n",
    "        print(valid_reshaped.shape)\n",
    "        valid_input_ids = input_ids[valid_reshaped]  # of shape [num_detected_regions_with_non_empty_gt_phrase_in_batch x seq_len]\n",
    "        valid_attention_mask = attention_mask[valid_reshaped]  # of shape [num_detected_regions_with_non_empty_gt_phrase_in_batch x seq_len]\n",
    "        valid_region_features = region_features[valid]  # of shape [num_detected_regions_with_non_empty_gt_phrase_in_batch x 1024]\n",
    "\n",
    "        return valid_input_ids, valid_attention_mask, valid_region_features\n",
    "\n",
    "def get_valid_decoder_input_for_evaluation(\n",
    "    selected_regions,  # shape [batch_size x 29]\n",
    "    input_ids,  # shape [(batch_size * 29) x seq_len]\n",
    "    attention_mask  # shape [(batch_size * 29) x seq_len]\n",
    "):\n",
    "    \"\"\"\n",
    "    For evaluation, we want to evaluate the decoder on the top_region_features selected by the classifier to get a sentence generated.\n",
    "    We also have to get the corresponding input_ids and attention_mask accordingly.\n",
    "    \"\"\"\n",
    "    # reshape to [(batch_size * 29)]\n",
    "    selected_regions = selected_regions.reshape(-1)\n",
    "\n",
    "    valid_input_ids = input_ids[selected_regions]  # of shape [num_regions_selected_in_batch x seq_len]\n",
    "    valid_attention_mask = attention_mask[selected_regions]  # of shape [num_regions_selected_in_batch x seq_len]\n",
    "\n",
    "    return valid_input_ids, valid_attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52c47923-e400-4835-8312-9a24775d84a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74, 1024])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_region_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8e2de6e-9044-4a6e-9e5f-247334e02b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 29, 1024)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_region_features[:8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62d956a3-c2e8-4f31-a594-5b184cae10af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 29, 1024)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_region_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c1d41e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(region_has_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2b0e03a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True,  True,  True, False,  True, False,  True, False,\n",
       "         False, False,  True, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False,  True, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_has_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "967a0785-adb2-43c9-a714-3016c1887f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29])\n",
      "torch.Size([29])\n"
     ]
    }
   ],
   "source": [
    "valid_input_ids, valid_attention_mask, valid_region_features = get_valid_decoder_input_for_training(\n",
    "                 class_detected, region_has_sentence, input_ids, attention_mask, top_region_features[0]#top_region_features[:8]\n",
    "             )\n",
    "\n",
    "#valid_input_ids, valid_attention_mask = get_valid_decoder_input_for_evaluation(selected_regions, input_ids, attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "458b45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.zeros((batch_size, 1024), dtype=torch.int64).to(device)\n",
    "\n",
    "# # 모든 토큰에 대해 attention 활성화\n",
    "# attention_mask = torch.ones((batch_size, 1024), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output, attentions = model_nlp(\n",
    "#         input_ids=input_ids,#input_ids\n",
    "#         attention_mask=attention_mask,#attention_mask\n",
    "#         image_hidden_states=region_features[0].unsqueeze(0), # regions_feature\n",
    "#         output_attentions=True, # i add # return_loss , pask_key_values, position_ids, use_cache\n",
    "#     )\n",
    "\n",
    "model_dtype = next(model_nlp.parameters()).dtype  # Get model's parameter dtype\n",
    "valid_region_features = torch.from_numpy(valid_region_features).to(dtype=model_dtype, device=device)\n",
    "\n",
    "language_model_loss, pre,attn_output = model_nlp(\n",
    "            valid_input_ids,\n",
    "            valid_attention_mask,\n",
    "            valid_region_features,\n",
    "            False,#return_loss,\n",
    "            None,#past_key_values=\n",
    "            None,#position_ids\n",
    "            True)##use_cache)\n",
    "# 중요도 계산\n",
    "#aggregated_attention = torch.stack(attentions).mean(dim=0)  # 평균화\n",
    "#print(\"Aggregated Attention:\", aggregated_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "80c546f0-762f-4597-839a-63575fd90e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.zeros((batch_size, 1024), dtype=torch.int64).to(device)\n",
    "\n",
    "# # 모든 토큰에 대해 attention 활성화\n",
    "# attention_mask = torch.ones((batch_size, 1024), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output, attentions = model_nlp(\n",
    "#         input_ids=input_ids,#input_ids\n",
    "#         attention_mask=attention_mask,#attention_mask\n",
    "#         image_hidden_states=region_features[0].unsqueeze(0), # regions_feature\n",
    "#         output_attentions=True, # i add # return_loss , pask_key_values, position_ids, use_cache\n",
    "#     )\n",
    "\n",
    "#model_dtype = next(model_nlp.parameters()).dtype  # Get model's parameter dtype\n",
    "#valid_region_features = torch.from_numpy(valid_region_features).to(dtype=model_dtype, device=device)\n",
    "\n",
    "language_model_loss, pre,attentions = model_nlp(\n",
    "            valid_input_ids,\n",
    "            valid_attention_mask,\n",
    "            valid_region_features,\n",
    "            False,#return_loss,\n",
    "            None,#past_key_values=\n",
    "            None,#position_ids\n",
    "            True)##use_cache)\n",
    "# 중요도 계산\n",
    "#aggregated_attention = torch.stack(attentions).mean(dim=0)  # 평균화\n",
    "#print(\"Aggregated Attention:\", aggregated_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "10ce02e8-029f-4e06-8797-dc3b7b087c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 이미지 Attention Weight 추출\n",
    "image_attention_weights = attentions[-1][:, :, :, 0]  # [Batch, Heads, Query=67]\n",
    "\n",
    "# 2. Heads 방향으로 평균화\n",
    "avg_image_attention_weights = image_attention_weights.mean(dim=1)  # [Batch=74, Query=67]\n",
    "\n",
    "# 3. 이미지 Region Feature 추출\n",
    "image_features = attn_output[-1][:, -1, :]  # [Batch=74, Hidden_Dim=1024]\n",
    "\n",
    "# 4. Query별 중요도를 Hidden_Dim에 적용\n",
    "# 각 Query(67)의 중요도를 Hidden_Dim(1024)에 곱하도록 차원 변경\n",
    "query_importance = (avg_image_attention_weights.unsqueeze(-1) * image_features.unsqueeze(1)).sum(dim=1)  # [Batch=74, Hidden_Dim=1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ad147d2-a1b0-4d08-904a-b284fb1a4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 16, 80])\n",
      "torch.Size([79, 80])\n",
      "torch.Size([79, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(image_attention_weights.shape)\n",
    "print(avg_image_attention_weights.shape)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "670d642d-e85a-44ce-8463-044079cd90cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1024])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_importance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c97297d-4f2e-4c2d-ae7a-838e5607daaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0560,  0.0327,  0.0052,  ...,  0.0751, -0.0744,  0.0991],\n",
       "        [ 0.0210,  0.0150,  0.0003,  ...,  0.0009, -0.0168,  0.0035],\n",
       "        [ 0.0071, -0.0010,  0.0064,  ...,  0.0022, -0.0070,  0.0050],\n",
       "        ...,\n",
       "        [ 0.0010, -0.0033,  0.0254,  ...,  0.0216, -0.0167,  0.0082],\n",
       "        [-0.0037, -0.0028,  0.0280,  ...,  0.0236, -0.0186,  0.0078],\n",
       "        [ 0.0171, -0.0184,  0.0216,  ...,  0.0117, -0.0287,  0.0254]],\n",
       "       device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f316664a-b028-4bce-8985-47f76528c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor를 NumPy 배열로 변환\n",
    "query_importance_numpy = query_importance.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# NumPy 배열을 저장\n",
    "np.save(\"query_importance_test.npy\", query_importance_numpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5322ae-e3c3-4ed5-bc05-2e491db5d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fe16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output,attentnions = model_nlp.generate(\n",
    "        image_hidden_states=region_features_reduced,\n",
    "        max_length=300,  # Maximum length of the generated sequence\n",
    "        num_beams=4,\n",
    "        num_beam_groups=1,# Beam search for better results\n",
    "        do_sample=False, # Enable sampling\n",
    "        num_return_sequences=1,  # Return one sequence\n",
    "        output_attentions=True,\n",
    "    ) \n",
    "# beam_search_output, selected_regions, _, _ = output\n",
    "# selected_regions = selected_regions.detach().cpu().numpy()\n",
    "\n",
    "#                 # generated_sentences_for_selected_regions is a List[str] of length \"num_regions_selected_in_batch\"\n",
    "# generated_sents_for_selected_regions = tokenizer.batch_decode(\n",
    "#     beam_search_output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c791a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Pretrained GPT-2 모델 및 토크나이저 불러오기\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tuning 가능하도록 모델 학습 모드로 전환\n",
    "model.train()\n",
    "\n",
    "# 입력 데이터 준비\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tuning 루프 (여기서는 간단한 예로 사용)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fine-tuning 단계\n",
    "for epoch in range(1):  # 예시로 1 epoch만\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"], output_attentions=True)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 모델 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# Attention Map 추출\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Attention Map은 outputs.attentions에 저장\n",
    "attention_maps = outputs.attentions  # List of tensors, one for each layer\n",
    "\n",
    "# 예시: 첫 번째 레이어, 첫 번째 헤드의 Attention Map\n",
    "first_layer_first_head = attention_maps[0][0]  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"Attention Map Shape:\", first_layer_first_head.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb79468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.binary_classifier.binary_classifier_region_abnormal import BinaryClassifierRegionAbnormal\n",
    "from src.binary_classifier.binary_classifier_region_selection import BinaryClassifierRegionSelection\n",
    "from src.object_detector.object_detector import ObjectDetector\n",
    "from src.language_model.language_model import LanguageModel\n",
    "\n",
    "\n",
    "class ReportGenerationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model consisting of:\n",
    "        - object detector encoder\n",
    "        - binary classifier for selecting regions for sentence genneration\n",
    "        - binary classifier for detecting if a region is abnormal or normal (to encode this information in the region feature vectors)\n",
    "        - language model decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrain_without_lm_model=False):\n",
    "        super().__init__()\n",
    "        self.pretrain_without_lm_model = pretrain_without_lm_model\n",
    "\n",
    "        self.object_detector = ObjectDetector(return_feature_vectors=True)\n",
    "        # Load the best object detector from the 1st training stage here when starting the 2nd training stage\n",
    "        # path_to_best_object_detector_weights = \"/u/home/tanida/runs/object_detector/run_10/weights/val_loss_13.482_epoch_6.pth\"\n",
    "        # self.object_detector.load_state_dict(torch.load(path_to_best_object_detector_weights))\n",
    "\n",
    "        self.binary_classifier_region_selection = BinaryClassifierRegionSelection()\n",
    "        self.binary_classifier_region_abnormal = BinaryClassifierRegionAbnormal()\n",
    "\n",
    "        self.language_model = LanguageModel()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: torch.FloatTensor,  # images is of shape [batch_size x 1 x 512 x 512] (whole gray-scale images of size 512 x 512)\n",
    "        image_targets: List[Dict],  # contains a dict for every image with keys \"boxes\" and \"labels\"\n",
    "        input_ids: torch.LongTensor,  # shape [(batch_size * 29) x seq_len], 1 sentence for every region for every image (sentence can be empty, i.e. \"\")\n",
    "        attention_mask: torch.FloatTensor,  # shape [(batch_size * 29) x seq_len]\n",
    "        region_has_sentence: torch.BoolTensor,  # shape [batch_size x 29], ground truth boolean mask that indicates if a region has a sentence or not\n",
    "        region_is_abnormal: torch.BoolTensor,  # shape [batch_size x 29], ground truth boolean mask that indicates if a region has is abnormal or not\n",
    "        return_loss: bool = True,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward method is used for training and evaluation of model.\n",
    "        Generate method is used for inference.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # top_region_features of shape [batch_size x 29 x 1024] (i.e. 1 feature vector for every region for every image in batch)\n",
    "            # class_detected is a boolean tensor of shape [batch_size x 29]. Its value is True for a class if the object detector detected the class/region in the image\n",
    "            obj_detector_loss_dict, top_region_features, class_detected = self.object_detector(images, image_targets)\n",
    "\n",
    "            # delete tensors that we don't need anymore to free up GPU resources\n",
    "            del images\n",
    "            del image_targets\n",
    "\n",
    "            # during training, only get the two losses for the two binary classifiers\n",
    "\n",
    "            classifier_loss_region_selection = self.binary_classifier_region_selection(\n",
    "                top_region_features, class_detected, return_loss=True, region_has_sentence=region_has_sentence\n",
    "            )\n",
    "\n",
    "            classifier_loss_region_abnormal = self.binary_classifier_region_abnormal(\n",
    "                top_region_features, class_detected, region_is_abnormal\n",
    "            )\n",
    "\n",
    "            if self.pretrain_without_lm_model:\n",
    "                return obj_detector_loss_dict, classifier_loss_region_selection, classifier_loss_region_abnormal\n",
    "\n",
    "            # to train the decoder, we want to use only the top region features (and corresponding input_ids, attention_mask)\n",
    "            # of regions that were both detected by the object detector and have a sentence as the ground truth\n",
    "            # this is done under the assumption that at inference time, the binary classifier for region selection will do an adequate job\n",
    "            # at selecting those regions that need a sentence to be generated by itself\n",
    "            valid_input_ids, valid_attention_mask, valid_region_features = self.get_valid_decoder_input_for_training(\n",
    "                class_detected, region_has_sentence, input_ids, attention_mask, top_region_features\n",
    "            )\n",
    "\n",
    "            del top_region_features\n",
    "            del region_has_sentence\n",
    "            del region_is_abnormal\n",
    "            del class_detected\n",
    "            del input_ids\n",
    "            del attention_mask\n",
    "\n",
    "        else:\n",
    "            # during evaluation, also return detections (i.e. detected bboxes)\n",
    "            obj_detector_loss_dict, detections, top_region_features, class_detected = self.object_detector(images, image_targets)\n",
    "\n",
    "            del images\n",
    "            del image_targets\n",
    "\n",
    "            # during evaluation, for the binary classifier for region selection, get the loss, the regions that were selected by the classifier\n",
    "            # (and that were also detected) and the corresponding region features (selected_region_features)\n",
    "            # this is done to evaluate the decoder under \"real-word\" conditions, i.e. the binary classifier decides which regions get a sentence\n",
    "            classifier_loss_region_selection, selected_regions, selected_region_features = self.binary_classifier_region_selection(\n",
    "                top_region_features, class_detected, return_loss=True, region_has_sentence=region_has_sentence\n",
    "            )\n",
    "\n",
    "            # for the binary classifier for abnormal/normal detection, get the loss and the predicted abnormal regions\n",
    "            classifier_loss_region_abnormal, predicted_abnormal_regions = self.binary_classifier_region_abnormal(\n",
    "                top_region_features, class_detected, region_is_abnormal\n",
    "            )\n",
    "\n",
    "            if self.pretrain_without_lm_model:\n",
    "                return obj_detector_loss_dict, classifier_loss_region_selection, classifier_loss_region_abnormal, detections, class_detected, selected_regions, predicted_abnormal_regions\n",
    "\n",
    "            del top_region_features\n",
    "            del region_has_sentence\n",
    "            del region_is_abnormal\n",
    "\n",
    "            # use the selected_regions mask to filter the inputs_ids and attention_mask to those that correspond to regions that were selected\n",
    "            valid_input_ids, valid_attention_mask = self.get_valid_decoder_input_for_evaluation(selected_regions, input_ids, attention_mask)\n",
    "            valid_region_features = selected_region_features\n",
    "\n",
    "            del input_ids\n",
    "            del attention_mask\n",
    "\n",
    "        # valid_input_ids can be empty if during:\n",
    "        # training:\n",
    "        #   - the regions that have a gt sentence (specified by region_has_sentence) were all not detected (specified by class_detected).\n",
    "        #   This can happend if e.g. a lateral chest x-ray was erroneously included in the dataset (and hence the object detector not detecting\n",
    "        #   any regions, since it was trained on frontal chest x-rays)\n",
    "        # evaluation:\n",
    "        #   - no regions were selected by the binary classifier (specified by selected_regions)\n",
    "        #   - the regions that were selected by the binary classifier for region selection were all not detected (also specified by selected_regions,\n",
    "        #   since class_detected is encoded in selected_regions). Again, the reason might be a bad input image\n",
    "        #\n",
    "        # empty valid_input_ids (and thus empty valid_attention_mask, valid_region_features) will throw an exception in the language model,\n",
    "        # which is why we have to return early\n",
    "        if valid_input_ids.shape[0] == 0:\n",
    "            return -1\n",
    "\n",
    "        language_model_loss = self.language_model(\n",
    "            valid_input_ids,\n",
    "            valid_attention_mask,\n",
    "            valid_region_features,\n",
    "            return_loss,\n",
    "            past_key_values,\n",
    "            position_ids,\n",
    "            use_cache,\n",
    "        )\n",
    "\n",
    "        del valid_input_ids\n",
    "        del valid_attention_mask\n",
    "        del valid_region_features\n",
    "\n",
    "        if self.training:\n",
    "            return obj_detector_loss_dict, classifier_loss_region_selection, classifier_loss_region_abnormal, language_model_loss\n",
    "        else:\n",
    "            # class_detected needed to evaluate how good the object detector is at detecting the different regions during evaluation\n",
    "            # detections and class_detected needed to compute IoU of object detector during evaluation\n",
    "            # selected_regions needed to evaluate binary classifier for region selection during evaluation and\n",
    "            # to map each generated sentence to its corresponding region (for example for plotting)\n",
    "            # predicted_abnormal_regions needed to evalute the binary classifier for normal/abnormal detection\n",
    "            return (\n",
    "                obj_detector_loss_dict,\n",
    "                classifier_loss_region_selection,\n",
    "                classifier_loss_region_abnormal,\n",
    "                language_model_loss,\n",
    "                detections,\n",
    "                class_detected,\n",
    "                selected_regions,\n",
    "                predicted_abnormal_regions\n",
    "            )\n",
    "\n",
    "    def get_valid_decoder_input_for_training(\n",
    "        self,\n",
    "        class_detected,  # shape [batch_size x 29]\n",
    "        region_has_sentence,  # shape [batch_size x 29]\n",
    "        input_ids,  # shape [(batch_size * 29) x seq_len]\n",
    "        attention_mask,  # shape [(batch_size * 29) x seq_len]\n",
    "        region_features,  # shape [batch_size x 29 x 1024]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We want to train the decoder only on region features (and corresponding input_ids/attention_mask) whose corresponding sentences are non-empty and\n",
    "        that were detected by the object detector.\n",
    "        \"\"\"\n",
    "        # valid is of shape [batch_size x 29]\n",
    "        valid = torch.logical_and(class_detected, region_has_sentence)\n",
    "\n",
    "        # reshape to [(batch_size * 29)], such that we can apply the mask to input_ids and attention_mask\n",
    "        valid_reshaped = valid.reshape(-1)\n",
    "\n",
    "        valid_input_ids = input_ids[valid_reshaped]  # of shape [num_detected_regions_with_non_empty_gt_phrase_in_batch x seq_len]\n",
    "        valid_attention_mask = attention_mask[valid_reshaped]  # of shape [num_detected_regions_with_non_empty_gt_phrase_in_batch x seq_len]\n",
    "        valid_region_features = region_features[valid]  # of shape [num_detected_regions_with_non_empty_gt_phrase_in_batch x 1024]\n",
    "\n",
    "        return valid_input_ids, valid_attention_mask, valid_region_features\n",
    "\n",
    "    def get_valid_decoder_input_for_evaluation(\n",
    "        self,\n",
    "        selected_regions,  # shape [batch_size x 29]\n",
    "        input_ids,  # shape [(batch_size * 29) x seq_len]\n",
    "        attention_mask  # shape [(batch_size * 29) x seq_len]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        For evaluation, we want to evaluate the decoder on the top_region_features selected by the classifier to get a sentence generated.\n",
    "        We also have to get the corresponding input_ids and attention_mask accordingly.\n",
    "        \"\"\"\n",
    "        # reshape to [(batch_size * 29)]\n",
    "        selected_regions = selected_regions.reshape(-1)\n",
    "\n",
    "        valid_input_ids = input_ids[selected_regions]  # of shape [num_regions_selected_in_batch x seq_len]\n",
    "        valid_attention_mask = attention_mask[selected_regions]  # of shape [num_regions_selected_in_batch x seq_len]\n",
    "\n",
    "        return valid_input_ids, valid_attention_mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        images: torch.FloatTensor,  # images is of shape [batch_size x 1 x 512 x 512] (whole gray-scale images of size 512 x 512)\n",
    "        max_length: int = None,\n",
    "        num_beams: int = 1,\n",
    "        num_beam_groups: int = 1,\n",
    "        do_sample: bool = False,\n",
    "        num_return_sequences: int = 1,\n",
    "        early_stopping: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        In inference mode, we usually input 1 image (with 29 regions) at a time.\n",
    "\n",
    "        The object detector first finds the region features for all 29 regions.\n",
    "\n",
    "        The binary classifier takes the region_features of shape [batch_size=1, 29, 1024] and returns:\n",
    "            - selected_region_features: shape [num_regions_selected_in_batch, 1024],\n",
    "            all region_features which were selected by the classifier to get a sentence generated (and which were also detected by the object detector)\n",
    "\n",
    "            - selected_regions: shape [batch_size x 29], boolean matrix that indicates which regions were selected to get a sentences generated\n",
    "            (these regions must also have been detected by the object detector).\n",
    "            This is needed in case we want to find the corresponding reference sentences to compute scores for metrics such as BertScore or BLEU.\n",
    "\n",
    "        The decoder then takes the selected_region_features and generates output ids for the batch.\n",
    "        These output ids can then be decoded by the tokenizer to get the generated sentences.\n",
    "\n",
    "        We also return selected_regions, such that we can map each generated sentence to a selected region.\n",
    "        We also return detections, such that we can map each generated sentence to a bounding box.\n",
    "        We also return class_detected to know which regions were not detected by the object detector (can be plotted).\n",
    "        \"\"\"\n",
    "        # top_region_features of shape [batch_size x 29 x 1024]\n",
    "        _, detections, top_region_features, class_detected = self.object_detector(images)\n",
    "\n",
    "        del images\n",
    "\n",
    "        # selected_region_features is of shape [num_regions_selected_in_batch x 1024]\n",
    "        # selected_regions is of shape [batch_size x 29] and is True for regions that should get a sentence\n",
    "        # (it has exactly num_regions_selected_in_batch True values)\n",
    "        selected_regions, selected_region_features = self.binary_classifier_region_selection(\n",
    "            top_region_features, class_detected, return_loss=False\n",
    "        )\n",
    "\n",
    "        del top_region_features\n",
    "\n",
    "        # selected_region_features can be empty if no region was both detected by the object detector and selected\n",
    "        # by the binary classifier to get a sentence generated. This can happen especially early on in training\n",
    "        # Since this would throw an exception in the language model, we return early\n",
    "        if selected_region_features.shape[0] == 0:\n",
    "            return -1\n",
    "\n",
    "        # output_ids of shape (num_regions_selected_in_batch x longest_generated_sequence_length)\n",
    "        output_ids = self.language_model.generate(\n",
    "            selected_region_features,\n",
    "            max_length,\n",
    "            num_beams,\n",
    "            num_beam_groups,\n",
    "            do_sample,\n",
    "            num_return_sequences,\n",
    "            early_stopping,\n",
    "        )\n",
    "\n",
    "        del selected_region_features\n",
    "\n",
    "        return output_ids, selected_regions, detections, class_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff0fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Pretrained GPT-2 모델 및 토크나이저 불러오기\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# 입력 텍스트\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델 평가 모드 및 Attention Map 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Attention Map 가져오기 (첫 번째 레이어의 첫 번째 헤드)\n",
    "attention_maps = outputs.attentions  # List of attention maps, one per layer\n",
    "layer_idx = 0  # 첫 번째 레이어\n",
    "head_idx = 0   # 첫 번째 헤드\n",
    "\n",
    "# Attention Map 크기: (batch_size, num_heads, seq_len, seq_len)\n",
    "attention_map = attention_maps[layer_idx][0, head_idx].numpy()  # (seq_len, seq_len)\n",
    "\n",
    "# 히트맵 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_map, cmap=\"viridis\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(text.split())), text.split(), rotation=90)  # X축: 입력 토큰\n",
    "plt.yticks(range(len(text.split())), text.split())              # Y축: 입력 토큰\n",
    "plt.title(f\"Attention Map (Layer {layer_idx + 1}, Head {head_idx + 1})\")\n",
    "plt.xlabel(\"Key Tokens\")\n",
    "plt.ylabel(\"Query Tokens\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e7412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rgrg_env",
   "language": "python",
   "name": "rgrg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
